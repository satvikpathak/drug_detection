{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d1840c",
   "metadata": {},
   "source": [
    "1. Setup Environment\n",
    "\n",
    "Install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35a46b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers==4.20.1 datasets==2.10.0 pandas==1.4.2 numpy==1.22.4 scikit-learn==1.1.1 torch==1.11.0 nltk==3.7 imbalanced-learn==0.9.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cada9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85a04ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\satvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bf111fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854e8c5",
   "metadata": {},
   "source": [
    "2. Create and Preprocess drug_use_data.csv\n",
    "\n",
    "Load SetFit/ade_corpus_v2_classification train split, create CSV, and preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0464c215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as drug_use_data.csv\n",
      "Training samples: 14109, Test samples: 3528\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import urllib.request\n",
    "\n",
    "# Define splits\n",
    "splits = {'train': 'train.jsonl', 'test': 'test.jsonl'}\n",
    "\n",
    "# Load via hf:// protocol\n",
    "try:\n",
    "    df = pd.read_json(\"hf://datasets/SetFit/ade_corpus_v2_classification/\" + splits[\"train\"], lines=True)\n",
    "except Exception as e:\n",
    "    print(f\"hf:// loading failed: {e}\")\n",
    "    print(\"Falling back to direct URL...\")\n",
    "    url = \"https://huggingface.co/datasets/SetFit/ade_corpus_v2_classification/resolve/main/train.jsonl\"\n",
    "    urllib.request.urlretrieve(url, \"train.jsonl\")\n",
    "    df = pd.read_json(\"train.jsonl\", lines=True)\n",
    "\n",
    "# Simulate substance and symptom labels\n",
    "substance_map = {\n",
    "    'morphine': 'opioid', 'oxycodone': 'opioid', 'fentanyl': 'opioid',\n",
    "    'cocaine': 'stimulant', 'methamphetamine': 'stimulant',\n",
    "    'placebo': 'none'\n",
    "}\n",
    "symptom_list = ['nausea', 'confusion', 'drowsiness', 'overdose']\n",
    "\n",
    "def assign_labels(text):\n",
    "    substance = 'none'\n",
    "    symptoms = []\n",
    "    text_lower = str(text).lower()\n",
    "    for drug, subst in substance_map.items():\n",
    "        if drug in text_lower:\n",
    "            substance = subst\n",
    "            break\n",
    "    for symp in symptom_list:\n",
    "        if symp in text_lower:\n",
    "            symptoms.append(symp)\n",
    "    return substance, symptoms if symptoms else ['none']\n",
    "\n",
    "# Apply labels\n",
    "df['substance_label'], df['symptom_labels'] = zip(*df['text'].apply(assign_labels))\n",
    "\n",
    "# Save to CSV\n",
    "df[['text', 'substance_label', 'symptom_labels']].to_csv('drug_use_data.csv', index=False)\n",
    "print('Dataset saved as drug_use_data.csv')\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Encode labels\n",
    "substance_classes = df['substance_label'].unique()\n",
    "substance2id = {label: idx for idx, label in enumerate(substance_classes)}\n",
    "df['substance_label'] = df['substance_label'].map(substance2id)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "symptom_encoded = mlb.fit_transform(df['symptom_labels'])\n",
    "symptom_df = pd.DataFrame(symptom_encoded, columns=mlb.classes_)\n",
    "\n",
    "df = pd.concat([df[['text', 'substance_label']], symptom_df], axis=1)\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['substance_label'])\n",
    "\n",
    "print(f'Training samples: {len(train_df)}, Test samples: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab3f6f0",
   "metadata": {},
   "source": [
    "3. Tokenize Data\n",
    "\n",
    "Tokenize using BioBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81dee1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/config.json from cache at C:\\Users\\satvi/.cache\\huggingface\\transformers\\ece5e89bab3b63a40e413c7f599e6081663cad06eb394e48d5023930733d15a3.ad895c9bc4687ffedea1a4cc498ac3f67ebd2083732981c2a06f548cde7d6582\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/vocab.txt from cache at C:\\Users\\satvi/.cache\\huggingface\\transformers\\e1dbe55c1d83f79c61148b28fbe20bfbb6aeb2f7163225830b3063fda58425e5.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/tokenizer_config.json from cache at None\n",
      "loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/config.json from cache at C:\\Users\\satvi/.cache\\huggingface\\transformers\\ece5e89bab3b63a40e413c7f599e6081663cad06eb394e48d5023930733d15a3.ad895c9bc4687ffedea1a4cc498ac3f67ebd2083732981c2a06f548cde7d6582\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/dmis-lab/biobert-base-cased-v1.2/resolve/main/config.json from cache at C:\\Users\\satvi/.cache\\huggingface\\transformers\\ece5e89bab3b63a40e413c7f599e6081663cad06eb394e48d5023930733d15a3.ad895c9bc4687ffedea1a4cc498ac3f67ebd2083732981c2a06f548cde7d6582\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"dmis-lab/biobert-base-cased-v1.2\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.20.1\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2', use_fast=True, do_lower_case=False)\n",
    "\n",
    "def tokenize_data(texts, max_length=64): \n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_data(train_df['text'])\n",
    "test_encodings = tokenize_data(test_df['text'])\n",
    "\n",
    "class DrugUseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, substance_labels, symptom_labels):\n",
    "        self.encodings = encodings\n",
    "        self.substance_labels = substance_labels\n",
    "        self.symptom_labels = symptom_labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['substance_labels'] = torch.tensor(self.substance_labels[idx], dtype=torch.long)\n",
    "        item['symptom_labels'] = torch.tensor(self.symptom_labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.substance_labels)\n",
    "\n",
    "symptom_columns = mlb.classes_\n",
    "train_dataset = DrugUseDataset(\n",
    "    train_encodings,\n",
    "    train_df['substance_label'].values,\n",
    "    train_df[symptom_columns].values\n",
    ")\n",
    "test_dataset = DrugUseDataset(\n",
    "    test_encodings,\n",
    "    test_df['substance_label'].values,\n",
    "    test_df[symptom_columns].values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd25d65",
   "metadata": {},
   "source": [
    "4. Define Custom Model\n",
    "\n",
    "BioBERT for multi-task classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "856710d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF features...\n",
      "TF-IDF feature shape: (14109, 5000)\n",
      "Model created with 1314184 parameters\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Simple feedforward network instead of transformer\n",
    "class SimpleMultiTaskModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_substance_classes, num_symptom_labels):\n",
    "        super(SimpleMultiTaskModel, self).__init__()\n",
    "        self.hidden1 = torch.nn.Linear(input_size, 256)\n",
    "        self.hidden2 = torch.nn.Linear(256, 128)\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.substance_classifier = torch.nn.Linear(128, num_substance_classes)\n",
    "        self.symptom_classifier = torch.nn.Linear(128, num_symptom_labels)\n",
    "        self.num_substance_classes = num_substance_classes\n",
    "        \n",
    "    def forward(self, x, substance_labels=None, symptom_labels=None):\n",
    "        # Forward pass through hidden layers\n",
    "        hidden = torch.relu(self.hidden1(x))\n",
    "        hidden = self.dropout(hidden)\n",
    "        hidden = torch.relu(self.hidden2(hidden))\n",
    "        hidden = self.dropout(hidden)\n",
    "        \n",
    "        # Classification heads\n",
    "        substance_logits = self.substance_classifier(hidden)\n",
    "        symptom_logits = self.symptom_classifier(hidden)\n",
    "        \n",
    "        loss = None\n",
    "        if substance_labels is not None and symptom_labels is not None:\n",
    "            # Compute class weights for substance labels\n",
    "            class_counts = np.bincount(substance_labels.cpu().numpy(), minlength=self.num_substance_classes)\n",
    "            class_weights = torch.tensor(1.0 / (class_counts + 1e-6), dtype=torch.float).to(substance_labels.device)\n",
    "            substance_loss = torch.nn.CrossEntropyLoss(weight=class_weights)(substance_logits, substance_labels)\n",
    "            symptom_loss = torch.nn.BCEWithLogitsLoss()(symptom_logits, symptom_labels)\n",
    "            loss = substance_loss + symptom_loss\n",
    "            \n",
    "        return {'loss': loss, 'substance_logits': substance_logits, 'symptom_logits': symptom_logits}\n",
    "\n",
    "# Create TF-IDF features from text data\n",
    "print(\"Creating TF-IDF features...\")\n",
    "vectorizer = TfidfVectorizer(max_features=5000, stop_words='english', ngram_range=(1, 2))\n",
    "X_train_tfidf = vectorizer.fit_transform(train_df['text']).toarray()\n",
    "X_test_tfidf = vectorizer.transform(test_df['text']).toarray()\n",
    "\n",
    "print(f\"TF-IDF feature shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "# Create custom dataset for TF-IDF features\n",
    "class TFIDFDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, substance_labels, symptom_labels):\n",
    "        self.features = torch.tensor(features, dtype=torch.float32)\n",
    "        self.substance_labels = torch.tensor(substance_labels, dtype=torch.long)\n",
    "        self.symptom_labels = torch.tensor(symptom_labels, dtype=torch.float32)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'x': self.features[idx],\n",
    "            'substance_labels': self.substance_labels[idx],\n",
    "            'symptom_labels': self.symptom_labels[idx]\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "# Create datasets\n",
    "train_dataset = TFIDFDataset(\n",
    "    X_train_tfidf,\n",
    "    train_df['substance_label'].values,\n",
    "    train_df[symptom_columns].values\n",
    ")\n",
    "\n",
    "test_dataset = TFIDFDataset(\n",
    "    X_test_tfidf,\n",
    "    test_df['substance_label'].values,\n",
    "    test_df[symptom_columns].values\n",
    ")\n",
    "\n",
    "# Initialize model\n",
    "model = SimpleMultiTaskModel(\n",
    "    input_size=5000, \n",
    "    num_substance_classes=len(substance_classes), \n",
    "    num_symptom_labels=len(symptom_columns)\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "print(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe4d1a",
   "metadata": {},
   "source": [
    "5. Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "36396a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "  0%|          | 0/1323 [13:57<?, ?it/s]\n",
      "c:\\Users\\satvi\\OneDrive\\Desktop\\final_res\\.venv\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 14109\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 32\n",
      "  Gradient Accumulation steps = 4\n",
      "  Total optimization steps = 1323\n",
      "  4%|▍         | 54/1323 [00:01<00:42, 30.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8132, 'learning_rate': 5e-06, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 104/1323 [00:03<00:41, 29.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8114, 'learning_rate': 1e-05, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 153/1323 [00:05<00:39, 29.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.8051, 'learning_rate': 1.5000000000000002e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 205/1323 [00:06<00:35, 31.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7953, 'learning_rate': 2e-05, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 253/1323 [00:08<00:36, 29.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7817, 'learning_rate': 1.910952804986643e-05, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 304/1323 [00:10<00:35, 28.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7644, 'learning_rate': 1.821905609973286e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 355/1323 [00:11<00:30, 32.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7446, 'learning_rate': 1.732858414959929e-05, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 403/1323 [00:13<00:29, 30.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.7207, 'learning_rate': 1.643811219946572e-05, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 439/1323 [00:14<00:26, 33.86it/s]***** Running Evaluation *****\n",
      "  Num examples = 3528\n",
      "  Batch size = 8\n",
      "                                                  \n",
      " 33%|███▎      | 441/1323 [00:17<00:26, 33.86it/s]Saving model checkpoint to ./results\\checkpoint-441\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      " 33%|███▎      | 443/1323 [00:17<03:28,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.6849604845046997, 'eval_substance_accuracy': 0.9943310657596371, 'eval_symptom_f1': 0.0, 'eval_runtime': 2.7014, 'eval_samples_per_second': 1305.991, 'eval_steps_per_second': 163.249, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 455/1323 [00:17<01:28,  9.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.695, 'learning_rate': 1.5547640249332147e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 503/1323 [00:18<00:25, 32.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6685, 'learning_rate': 1.4657168299198576e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 555/1323 [00:20<00:22, 33.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6362, 'learning_rate': 1.3766696349065006e-05, 'epoch': 1.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 46%|████▌     | 603/1323 [00:21<00:21, 33.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6038, 'learning_rate': 1.2876224398931433e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|████▉     | 655/1323 [00:23<00:21, 31.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5779, 'learning_rate': 1.1985752448797864e-05, 'epoch': 1.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 703/1323 [00:24<00:19, 32.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5389, 'learning_rate': 1.1095280498664294e-05, 'epoch': 1.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 755/1323 [00:26<00:17, 32.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5067, 'learning_rate': 1.0204808548530723e-05, 'epoch': 1.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 803/1323 [00:28<00:17, 30.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4721, 'learning_rate': 9.31433659839715e-06, 'epoch': 1.81}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 855/1323 [00:29<00:14, 32.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4446, 'learning_rate': 8.42386464826358e-06, 'epoch': 1.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▋   | 879/1323 [00:30<00:13, 33.13it/s]***** Running Evaluation *****\n",
      "  Num examples = 3528\n",
      "  Batch size = 8\n",
      "                                                  \n",
      " 67%|██████▋   | 882/1323 [00:33<00:13, 33.13it/s]Saving model checkpoint to ./results\\checkpoint-882\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      " 67%|██████▋   | 883/1323 [00:33<01:46,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4124690294265747, 'eval_substance_accuracy': 0.9943310657596371, 'eval_symptom_f1': 0.0, 'eval_runtime': 2.7691, 'eval_samples_per_second': 1274.066, 'eval_steps_per_second': 159.258, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 903/1323 [00:33<00:27, 15.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4124, 'learning_rate': 7.5333926981300095e-06, 'epoch': 2.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 955/1323 [00:35<00:11, 31.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3923, 'learning_rate': 6.642920747996439e-06, 'epoch': 2.15}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▌  | 1003/1323 [00:37<00:09, 33.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3694, 'learning_rate': 5.7524487978628674e-06, 'epoch': 2.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 1055/1323 [00:38<00:08, 32.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3481, 'learning_rate': 4.861976847729297e-06, 'epoch': 2.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 1103/1323 [00:40<00:06, 32.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3341, 'learning_rate': 3.971504897595726e-06, 'epoch': 2.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 1155/1323 [00:41<00:05, 33.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3143, 'learning_rate': 3.081032947462155e-06, 'epoch': 2.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 1203/1323 [00:43<00:03, 32.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3056, 'learning_rate': 2.1905609973285845e-06, 'epoch': 2.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▍| 1255/1323 [00:44<00:02, 31.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3004, 'learning_rate': 1.3000890471950135e-06, 'epoch': 2.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 1303/1323 [00:46<00:00, 30.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2897, 'learning_rate': 4.0961709706144254e-07, 'epoch': 2.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1323/1323 [00:46<00:00, 32.64it/s]***** Running Evaluation *****\n",
      "  Num examples = 3528\n",
      "  Batch size = 8\n",
      "                                                   \n",
      "100%|██████████| 1323/1323 [00:49<00:00, 32.64it/s]Saving model checkpoint to ./results\\checkpoint-1323\n",
      "Trainer.model is not a `PreTrainedModel`, only saving its state dict.\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results\\checkpoint-441 (score: 0.9943310657596371).\n",
      "100%|██████████| 1323/1323 [00:49<00:00, 26.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2903836965560913, 'eval_substance_accuracy': 0.9943310657596371, 'eval_symptom_f1': 0.0, 'eval_runtime': 2.7717, 'eval_samples_per_second': 1272.858, 'eval_steps_per_second': 159.107, 'epoch': 3.0}\n",
      "{'train_runtime': 49.7833, 'train_samples_per_second': 850.225, 'train_steps_per_second': 26.575, 'train_loss': 1.5510746054097908, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1323, training_loss=1.5510746054097908, metrics={'train_runtime': 49.7833, 'train_samples_per_second': 850.225, 'train_steps_per_second': 26.575, 'train_loss': 1.5510746054097908, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_metrics(pred):\n",
    "    substance_preds = np.argmax(pred.predictions[0], axis=1)\n",
    "    substance_labels = pred.label_ids[0]\n",
    "    symptom_preds = (pred.predictions[1] > 0.5).astype(int)\n",
    "    symptom_labels = pred.label_ids[1]\n",
    "\n",
    "    substance_accuracy = accuracy_score(substance_labels, substance_preds)\n",
    "    symptom_f1 = f1_score(symptom_labels, symptom_preds, average='micro')\n",
    "\n",
    "    return {\n",
    "        'substance_accuracy': substance_accuracy,\n",
    "        'symptom_f1': symptom_f1\n",
    "    }\n",
    "\n",
    "# Reduce batch size to prevent OOM\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,  # Reduced from 32\n",
    "    per_device_eval_batch_size=8,   # Reduced from 32\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=200,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='substance_accuracy',\n",
    "    dataloader_pin_memory=False,    # Disable pin memory to save RAM\n",
    "    gradient_accumulation_steps=4   # Maintain effective batch size\n",
    ")\n",
    "\n",
    "import gc\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "gc.collect()\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5bdb3c",
   "metadata": {},
   "source": [
    "6. Evaluate Model\n",
    "\n",
    "Evaluate and print results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33040f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 3528\n",
      "  Batch size = 8\n",
      "100%|██████████| 441/441 [00:02<00:00, 167.00it/s]\n",
      "***** Running Prediction *****\n",
      "  Num examples = 3528\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 1.6849604845046997, 'eval_substance_accuracy': 0.9943310657596371, 'eval_symptom_f1': 0.0, 'eval_runtime': 2.6544, 'eval_samples_per_second': 1329.113, 'eval_steps_per_second': 166.139, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 427/441 [00:02<00:00, 163.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Substance Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        none       0.99      1.00      1.00      3508\n",
      "      opioid       0.00      0.00      0.00        18\n",
      "   stimulant       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.99      3528\n",
      "   macro avg       0.33      0.33      0.33      3528\n",
      "weighted avg       0.99      0.99      0.99      3528\n",
      "\n",
      "Symptom Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   confusion       0.00      0.00      0.00         3\n",
      "  drowsiness       0.00      0.00      0.00         3\n",
      "      nausea       0.00      0.00      0.00        19\n",
      "        none       0.00      0.00      0.00      3485\n",
      "    overdose       0.00      0.00      0.00        19\n",
      "\n",
      "   micro avg       0.00      0.00      0.00      3529\n",
      "   macro avg       0.00      0.00      0.00      3529\n",
      "weighted avg       0.00      0.00      0.00      3529\n",
      " samples avg       0.00      0.00      0.00      3529\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\satvi\\OneDrive\\Desktop\\final_res\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\satvi\\OneDrive\\Desktop\\final_res\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\satvi\\OneDrive\\Desktop\\final_res\\.venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1327: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f'Evaluation Results: {eval_results}')\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "substance_preds = np.argmax(predictions.predictions[0], axis=1)\n",
    "symptom_preds = (predictions.predictions[1] > 0.5).astype(int)\n",
    "\n",
    "print('Substance Classification Report:')\n",
    "print(classification_report(test_df['substance_label'], substance_preds, target_names=substance_classes))\n",
    "\n",
    "print('Symptom Classification Report:')\n",
    "print(classification_report(test_df[symptom_columns], symptom_preds, target_names=symptom_columns, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6afcaa5",
   "metadata": {},
   "source": [
    "7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ecf079df",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SimpleMultiTaskModel' object has no attribute 'save_pretrained'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave_pretrained\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./biobert_drug_use_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39msave_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./biobert_drug_use_model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel and tokenizer saved!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\satvi\\OneDrive\\Desktop\\final_res\\.venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1185\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1183\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1184\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1185\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m   1186\u001b[0m     \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'SimpleMultiTaskModel' object has no attribute 'save_pretrained'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 441/441 [00:20<00:00, 163.35it/s]"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('./biobert_drug_use_model')\n",
    "tokenizer.save_pretrained('./biobert_drug_use_model')\n",
    "print('Model and tokenizer saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
