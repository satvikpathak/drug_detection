{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d1840c",
   "metadata": {},
   "source": [
    "1. Setup Environment\n",
    "\n",
    "Install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a46b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers==4.20.1 datasets==2.10.0 pandas==1.4.2 numpy==1.22.4 scikit-learn==1.1.1 torch==1.11.0 nltk==3.7 imbalanced-learn==0.9.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cada9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a04ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf111fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854e8c5",
   "metadata": {},
   "source": [
    "2. Create and Preprocess drug_use_data.csv\n",
    "\n",
    "Load SetFit/ade_corpus_v2_classification train split, create CSV, and preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0464c215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as drug_use_data.csv\n",
      "Original distribution: Counter({0: 17510, 1: 102, 2: 25})\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "\n",
    "# Define splits\n",
    "splits = {'train': 'train.jsonl', 'test': 'test.jsonl'}\n",
    "\n",
    "# Load via hf:// protocol\n",
    "try:\n",
    "    df = pd.read_json(\"hf://datasets/SetFit/ade_corpus_v2_classification/\" + splits[\"train\"], lines=True)\n",
    "except Exception as e:\n",
    "    print(f\"hf:// loading failed: {e}\")\n",
    "    print(\"Falling back to direct URL...\")\n",
    "    url = \"https://huggingface.co/datasets/SetFit/ade_corpus_v2_classification/resolve/main/train.jsonl\"\n",
    "    urllib.request.urlretrieve(url, \"train.jsonl\")\n",
    "    df = pd.read_json(\"train.jsonl\", lines=True)\n",
    "\n",
    "# Expanded substance and symptom lists\n",
    "substance_map = {\n",
    "    'morphine': 'opioid', 'oxycodone': 'opioid', 'fentanyl': 'opioid', 'hydrocodone': 'opioid',\n",
    "    'heroin': 'opioid', 'codeine': 'opioid', 'tramadol': 'opioid',\n",
    "    'cocaine': 'stimulant', 'methamphetamine': 'stimulant', 'amphetamine': 'stimulant',\n",
    "    'placebo': 'none', 'heparin': 'none'\n",
    "}\n",
    "symptom_list = ['nausea', 'confusion', 'drowsiness', 'overdose', 'dizziness', 'vomiting',\n",
    "                'fatigue', 'headache', 'anxiety', 'seizure', 'hematoma', 'rash', 'pain',\n",
    "                'constipation', 'dyspnea', 'pruritus']\n",
    "\n",
    "def assign_labels(text, original_label=None):\n",
    "    substance = 'none'\n",
    "    symptoms = []\n",
    "    text_lower = str(text).lower()\n",
    "    \n",
    "    # Check for substances\n",
    "    for drug, subst in substance_map.items():\n",
    "        if drug in text_lower:\n",
    "            substance = subst\n",
    "            break\n",
    "    \n",
    "    # Check for symptoms\n",
    "    for symp in symptom_list:\n",
    "        if symp in text_lower:\n",
    "            symptoms.append(symp)\n",
    "    \n",
    "    # Use original ADE label if available and no symptoms found\n",
    "    if original_label == 1 and not symptoms:\n",
    "        symptoms = ['adverse_event']\n",
    "    \n",
    "    return substance, symptoms if symptoms else ['none']\n",
    "\n",
    "# Apply labels with original label information\n",
    "if 'label' in df.columns:\n",
    "    df['substance_label'], df['symptom_labels'] = zip(*[\n",
    "        assign_labels(text, label) for text, label in zip(df['text'], df['label'])\n",
    "    ])\n",
    "else:\n",
    "    df['substance_label'], df['symptom_labels'] = zip(*df['text'].apply(lambda x: assign_labels(x)))\n",
    "\n",
    "# Save to CSV BEFORE any processing that might duplicate data\n",
    "df[['text', 'substance_label', 'symptom_labels']].to_csv('drug_use_data.csv', index=False)\n",
    "print('Dataset saved as drug_use_data.csv')\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Encode labels\n",
    "substance_classes = df['substance_label'].unique()\n",
    "substance2id = {label: idx for idx, label in enumerate(substance_classes)}\n",
    "df['substance_label'] = df['substance_label'].map(substance2id)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "symptom_encoded = mlb.fit_transform(df['symptom_labels'])\n",
    "symptom_df = pd.DataFrame(symptom_encoded, columns=mlb.classes_)\n",
    "symptom_columns = mlb.classes_\n",
    "\n",
    "# Combine dataframes\n",
    "df = pd.concat([df[['text', 'substance_label']], symptom_df], axis=1)\n",
    "\n",
    "# Apply SMOTE for balanced training data\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "\n",
    "print(\"Original distribution:\", Counter(df['substance_label']))\n",
    "\n",
    "# Use TF-IDF features for SMOTE\n",
    "temp_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_tfidf_temp = temp_vectorizer.fit_transform(df['text']).toarray()\n",
    "\n",
    "try:\n",
    "    smote = SMOTE(random_state=42, k_neighbors=min(3, Counter(df['substance_label']).most_common()[-1][1] - 1))\n",
    "    X_balanced, y_balanced = smote.fit_resample(X_tfidf_temp, df['substance_label'])\n",
    "    \n",
    "    # Create balanced dataframe by finding closest matches\n",
    "    balanced_indices = []\n",
    "    for x_sample in X_balanced:\n",
    "        similarities = np.dot(X_tfidf_temp, x_sample)\n",
    "        closest_idx = np.argmax(similarities)\n",
    "        balanced_indices.append(closest_idx)\n",
    "    \n",
    "    df_balanced = df.iloc[balanced_indices].copy()\n",
    "    df_balanced['substance_label'] = y_balanced\n",
    "    df = df_balanced\n",
    "    \n",
    "    print(\"Balanced distribution:\", Counter(df['substance_label']))\n",
    "except ValueError as e:\n",
    "    print(f\"SMOTE failed: {e}, using original data with manual balancing\")\n",
    "    # Fallback: simple oversampling for minority classes\n",
    "    minority_threshold = len(df) * 0.1  # 10% threshold\n",
    "    minority_data = []\n",
    "    for label in df['substance_label'].unique():\n",
    "        label_data = df[df['substance_label'] == label]\n",
    "        if len(label_data) < minority_threshold:\n",
    "            # Duplicate minority class samples\n",
    "            multiplier = int(minority_threshold / len(label_data)) + 1\n",
    "            minority_data.append(pd.concat([label_data] * multiplier, ignore_index=True))\n",
    "    \n",
    "    if minority_data:\n",
    "        df = pd.concat([df] + minority_data, ignore_index=True)\n",
    "        print(\"Manual balancing applied\")\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['substance_label'])\n",
    "\n",
    "print(f'Training samples: {len(train_df)}, Test samples: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab3f6f0",
   "metadata": {},
   "source": [
    "3. Create TF-IDF Features and Datasets\n",
    "\n",
    "Use TF-IDF features and create custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dee1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# Create TF-IDF features with reduced memory footprint\n",
    "print(\"Creating TF-IDF features...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=2000,  # Reduced from 5000 to save memory\n",
    "    stop_words='english', \n",
    "    ngram_range=(1, 2),  # Reduced from (1,3) to save memory\n",
    "    dtype=np.float32,\n",
    "    min_df=3,  # Increased to reduce vocabulary size\n",
    "    max_df=0.90  # More aggressive filtering\n",
    ")\n",
    "\n",
    "# Keep matrices in sparse format - DON'T convert to dense arrays\n",
    "X_train_tfidf_sparse = vectorizer.fit_transform(train_df['text'])\n",
    "X_test_tfidf_sparse = vectorizer.transform(test_df['text'])\n",
    "\n",
    "print(f\"TF-IDF sparse matrix shape: {X_train_tfidf_sparse.shape}\")\n",
    "print(f\"Memory usage (sparse): ~{X_train_tfidf_sparse.data.nbytes / 1024**2:.1f} MB\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "\n",
    "# Convert sparse matrices to dense in smaller batches to avoid memory issues\n",
    "def sparse_to_dense_batched(sparse_matrix, batch_size=1000):\n",
    "    \"\"\"Convert sparse matrix to dense in batches to manage memory\"\"\"\n",
    "    n_samples = sparse_matrix.shape[0]\n",
    "    n_features = sparse_matrix.shape[1]\n",
    "    \n",
    "    # Pre-allocate dense array\n",
    "    dense_array = np.zeros((n_samples, n_features), dtype=np.float32)\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        end_idx = min(i + batch_size, n_samples)\n",
    "        batch_sparse = sparse_matrix[i:end_idx]\n",
    "        dense_array[i:end_idx] = batch_sparse.toarray()\n",
    "        \n",
    "        if i % (batch_size * 10) == 0:  # Progress update every 10 batches\n",
    "            print(f\"Processed {i}/{n_samples} samples...\")\n",
    "    \n",
    "    return dense_array\n",
    "\n",
    "print(\"Converting sparse matrices to dense (this may take a moment)...\")\n",
    "try:\n",
    "    X_train_tfidf = sparse_to_dense_batched(X_train_tfidf_sparse, batch_size=500)\n",
    "    X_test_tfidf = sparse_to_dense_batched(X_test_tfidf_sparse, batch_size=500)\n",
    "    print(\"Conversion completed successfully!\")\n",
    "except MemoryError:\n",
    "    print(\"Still not enough memory. Using even smaller batch size...\")\n",
    "    try:\n",
    "        X_train_tfidf = sparse_to_dense_batched(X_train_tfidf_sparse, batch_size=100)\n",
    "        X_test_tfidf = sparse_to_dense_batched(X_test_tfidf_sparse, batch_size=100)\n",
    "        print(\"Conversion completed with smaller batches!\")\n",
    "    except MemoryError:\n",
    "        print(\"Memory still insufficient. Switching to sparse-compatible approach...\")\n",
    "        # Alternative: Work directly with sparse matrices (requires model modification)\n",
    "        raise MemoryError(\"Consider using a machine with more RAM or further reducing max_features\")\n",
    "\n",
    "print(f\"Final TF-IDF feature shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "# Memory-efficient dataset class\n",
    "class TFIDFDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, substance_labels, symptom_labels):\n",
    "        # Store as numpy arrays to save memory compared to tensors\n",
    "        self.features = features.astype(np.float32)\n",
    "        self.substance_labels = substance_labels.astype(np.int64)\n",
    "        self.symptom_labels = symptom_labels.astype(np.float32)\n",
    "        \n",
    "        print(f\"Dataset created with {len(self.features)} samples\")\n",
    "        print(f\"Feature shape: {self.features.shape}\")\n",
    "        print(f\"Memory usage: ~{self.features.nbytes / 1024**2:.1f} MB\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert to tensors only when needed (lazy loading)\n",
    "        return {\n",
    "            'x': torch.from_numpy(self.features[idx]).float(),\n",
    "            'substance_labels': torch.from_numpy(np.array(self.substance_labels[idx])).long(),\n",
    "            'symptom_labels': torch.from_numpy(self.symptom_labels[idx]).float()\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "# Ensure symptom columns exist in both dataframes\n",
    "missing_train_cols = [col for col in symptom_columns if col not in train_df.columns]\n",
    "missing_test_cols = [col for col in symptom_columns if col not in test_df.columns]\n",
    "\n",
    "if missing_train_cols:\n",
    "    print(f\"Adding missing columns to train_df: {missing_train_cols}\")\n",
    "    for col in missing_train_cols:\n",
    "        train_df[col] = 0\n",
    "\n",
    "if missing_test_cols:\n",
    "    print(f\"Adding missing columns to test_df: {missing_test_cols}\")\n",
    "    for col in missing_test_cols:\n",
    "        test_df[col] = 0\n",
    "\n",
    "# Get symptom data\n",
    "train_symptom_data = train_df[symptom_columns].values\n",
    "test_symptom_data = test_df[symptom_columns].values\n",
    "\n",
    "print(f\"Train symptom data shape: {train_symptom_data.shape}\")\n",
    "print(f\"Test symptom data shape: {test_symptom_data.shape}\")\n",
    "\n",
    "# Create datasets with memory management\n",
    "import gc\n",
    "\n",
    "# Clear any unnecessary variables\n",
    "if 'X_train_tfidf_sparse' in locals():\n",
    "    del X_train_tfidf_sparse\n",
    "if 'X_test_tfidf_sparse' in locals():\n",
    "    del X_test_tfidf_sparse\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    print(\"Creating training dataset...\")\n",
    "    train_dataset = TFIDFDataset(\n",
    "        X_train_tfidf,\n",
    "        train_df['substance_label'].values,\n",
    "        train_symptom_data\n",
    "    )\n",
    "    \n",
    "    print(\"Creating test dataset...\")\n",
    "    test_dataset = TFIDFDataset(\n",
    "        X_test_tfidf,\n",
    "        test_df['substance_label'].values,\n",
    "        test_symptom_data\n",
    "    )\n",
    "    \n",
    "    print(\"Datasets created successfully!\")\n",
    "    \n",
    "    # Verify dataset integrity\n",
    "    sample = train_dataset[0]\n",
    "    print(f\"Sample data shapes - Features: {sample['x'].shape}, \"\n",
    "          f\"Substance: {sample['substance_labels'].shape}, \"\n",
    "          f\"Symptoms: {sample['symptom_labels'].shape}\")\n",
    "    \n",
    "    # Clean up large arrays to free memory\n",
    "    del X_train_tfidf, X_test_tfidf\n",
    "    gc.collect()\n",
    "    print(\"Memory cleanup completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating datasets: {e}\")\n",
    "    print(\"Debugging information:\")\n",
    "    print(f\"Available memory info:\")\n",
    "    import psutil\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Total RAM: {memory.total / 1024**3:.1f} GB\")\n",
    "    print(f\"Available RAM: {memory.available / 1024**3:.1f} GB\")\n",
    "    print(f\"Used RAM: {memory.percent}%\")\n",
    "    raise\n",
    "\n",
    "print(\"TF-IDF processing completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd25d65",
   "metadata": {},
   "source": [
    "4. Define Custom Model\n",
    "\n",
    "BioBERT for multi-task classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856710d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedMultiTaskModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_substance_classes, num_symptom_labels):\n",
    "        super(EnhancedMultiTaskModel, self).__init__()\n",
    "        \n",
    "        # Input normalization\n",
    "        self.input_norm = torch.nn.BatchNorm1d(input_size)\n",
    "        \n",
    "        # Enhanced architecture with residual connections\n",
    "        self.hidden1 = torch.nn.Linear(input_size, 512)\n",
    "        self.norm1 = torch.nn.BatchNorm1d(512)\n",
    "        self.hidden2 = torch.nn.Linear(512, 256)\n",
    "        self.norm2 = torch.nn.BatchNorm1d(256)\n",
    "        self.hidden3 = torch.nn.Linear(256, 128)\n",
    "        self.norm3 = torch.nn.BatchNorm1d(128)\n",
    "        \n",
    "        # Residual connection layer\n",
    "        self.residual = torch.nn.Linear(input_size, 128)\n",
    "        \n",
    "        # Dropout with different rates\n",
    "        self.dropout1 = torch.nn.Dropout(0.2)\n",
    "        self.dropout2 = torch.nn.Dropout(0.3)\n",
    "        self.dropout3 = torch.nn.Dropout(0.2)\n",
    "        \n",
    "        # Task-specific layers with attention\n",
    "        self.substance_attention = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 128),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.symptom_attention = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 128),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.substance_classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(64, num_substance_classes)\n",
    "        )\n",
    "        \n",
    "        self.symptom_classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(64, num_symptom_labels)\n",
    "        )\n",
    "        \n",
    "        self.num_substance_classes = num_substance_classes\n",
    "        self.num_symptom_labels = num_symptom_labels\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, torch.nn.BatchNorm1d):\n",
    "                torch.nn.init.constant_(m.weight, 1)\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x, substance_labels=None, symptom_labels=None):\n",
    "        # Input normalization\n",
    "        x_norm = self.input_norm(x)\n",
    "        \n",
    "        # Forward pass through hidden layers\n",
    "        hidden = torch.relu(self.hidden1(x_norm))\n",
    "        hidden = self.norm1(hidden)\n",
    "        hidden = self.dropout1(hidden)\n",
    "        \n",
    "        hidden = torch.relu(self.hidden2(hidden))\n",
    "        hidden = self.norm2(hidden)\n",
    "        hidden = self.dropout2(hidden)\n",
    "        \n",
    "        hidden = torch.relu(self.hidden3(hidden))\n",
    "        hidden = self.norm3(hidden)\n",
    "        \n",
    "        # Residual connection\n",
    "        residual = torch.relu(self.residual(x_norm))\n",
    "        hidden = hidden + residual  # Add residual connection\n",
    "        hidden = self.dropout3(hidden)\n",
    "        \n",
    "        # Task-specific attention\n",
    "        substance_att = self.substance_attention(hidden)\n",
    "        symptom_att = self.symptom_attention(hidden)\n",
    "        \n",
    "        # Apply attention\n",
    "        substance_features = hidden * substance_att\n",
    "        symptom_features = hidden * symptom_att\n",
    "        \n",
    "        # Generate logits\n",
    "        substance_logits = self.substance_classifier(substance_features)\n",
    "        symptom_logits = self.symptom_classifier(symptom_features)\n",
    "        \n",
    "        loss = None\n",
    "        if substance_labels is not None and symptom_labels is not None:\n",
    "            # Improved loss calculation\n",
    "            \n",
    "            # Focal loss for substance classification (better for imbalanced classes)\n",
    "            alpha = 0.25\n",
    "            gamma = 2.0\n",
    "            \n",
    "            # Standard cross entropy\n",
    "            ce_loss = torch.nn.functional.cross_entropy(substance_logits, substance_labels, reduction='none')\n",
    "            pt = torch.exp(-ce_loss)\n",
    "            focal_loss = alpha * (1 - pt) ** gamma * ce_loss\n",
    "            substance_loss = focal_loss.mean()\n",
    "            \n",
    "            # Class-balanced BCE loss for symptoms\n",
    "            pos_counts = substance_labels.bincount(minlength=self.num_substance_classes).float()\n",
    "            total_count = len(substance_labels)\n",
    "            pos_weights = total_count / (2.0 * pos_counts + 1e-6)\n",
    "            \n",
    "            # For symptoms, use adaptive positive weights\n",
    "            symptom_pos_counts = symptom_labels.sum(dim=0) + 1e-6\n",
    "            symptom_neg_counts = (1 - symptom_labels).sum(dim=0) + 1e-6\n",
    "            symptom_pos_weights = symptom_neg_counts / symptom_pos_counts\n",
    "            symptom_pos_weights = torch.clamp(symptom_pos_weights, min=0.1, max=10.0)\n",
    "            \n",
    "            symptom_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "                symptom_logits, \n",
    "                symptom_labels, \n",
    "                pos_weight=symptom_pos_weights\n",
    "            )\n",
    "            \n",
    "            # Combine losses with adaptive weighting\n",
    "            substance_weight = 0.7  # Higher weight for substance classification\n",
    "            symptom_weight = 0.3\n",
    "            \n",
    "            loss = substance_weight * substance_loss + symptom_weight * symptom_loss\n",
    "        \n",
    "        return {\n",
    "            'loss': loss, \n",
    "            'substance_logits': substance_logits, \n",
    "            'symptom_logits': symptom_logits,\n",
    "            'substance_probs': torch.softmax(substance_logits, dim=-1),\n",
    "            'symptom_probs': torch.sigmoid(symptom_logits)\n",
    "        }\n",
    "\n",
    "# Create model with proper input size\n",
    "actual_input_size = X_train_tfidf.shape[1]  # Use actual TF-IDF feature size\n",
    "print(f\"Using input size: {actual_input_size}\")\n",
    "\n",
    "model = EnhancedMultiTaskModel(\n",
    "    input_size=actual_input_size,\n",
    "    num_substance_classes=len(substance_classes),\n",
    "    num_symptom_labels=len(symptom_columns)\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model created with {total_params:,} total parameters\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(f\"Input size: {actual_input_size}\")\n",
    "print(f\"Substance classes: {len(substance_classes)}\")\n",
    "print(f\"Symptom labels: {len(symptom_columns)}\")\n",
    "print(f\"Hidden layers: 512 -> 256 -> 128\")\n",
    "print(\"Features: Batch normalization, residual connections, attention mechanisms, focal loss\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe4d1a",
   "metadata": {},
   "source": [
    "5. Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36396a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "import gc\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Custom training function since Transformers Trainer expects specific model structure\n",
    "def train_model(model, train_dataset, test_dataset, num_epochs=10, batch_size=16, learning_rate=5e-4):\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=False)\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'substance_acc': [], 'symptom_f1': [],\n",
    "        'symptom_precision': [], 'symptom_recall': []\n",
    "    }\n",
    "    \n",
    "    best_substance_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    print(f\"Total epochs: {num_epochs}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(f\"Total training batches: {len(train_loader)}\")\n",
    "    print(f\"Total validation batches: {len(test_loader)}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Move data to device\n",
    "            x = batch['x'].to(device)\n",
    "            substance_labels = batch['substance_labels'].to(device)\n",
    "            symptom_labels = batch['symptom_labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(x, substance_labels=substance_labels, symptom_labels=symptom_labels)\n",
    "            loss = outputs['loss']\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            \n",
    "            # Log progress every 50 batches\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, \"\n",
    "                      f\"Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        all_substance_preds = []\n",
    "        all_substance_labels = []\n",
    "        all_symptom_preds = []\n",
    "        all_symptom_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                x = batch['x'].to(device)\n",
    "                substance_labels = batch['substance_labels'].to(device)\n",
    "                symptom_labels = batch['symptom_labels'].to(device)\n",
    "                \n",
    "                outputs = model(x, substance_labels=substance_labels, symptom_labels=symptom_labels)\n",
    "                loss = outputs['loss']\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "                \n",
    "                # Collect predictions\n",
    "                substance_preds = torch.argmax(outputs['substance_logits'], dim=1)\n",
    "                symptom_preds = (torch.sigmoid(outputs['symptom_logits']) > 0.5).float()\n",
    "                \n",
    "                all_substance_preds.extend(substance_preds.cpu().numpy())\n",
    "                all_substance_labels.extend(substance_labels.cpu().numpy())\n",
    "                all_symptom_preds.extend(symptom_preds.cpu().numpy())\n",
    "                all_symptom_labels.extend(symptom_labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        \n",
    "        # Calculate metrics\n",
    "        all_substance_preds = np.array(all_substance_preds)\n",
    "        all_substance_labels = np.array(all_substance_labels)\n",
    "        all_symptom_preds = np.array(all_symptom_preds)\n",
    "        all_symptom_labels = np.array(all_symptom_labels)\n",
    "        \n",
    "        substance_accuracy = accuracy_score(all_substance_labels, all_substance_preds)\n",
    "        symptom_f1 = f1_score(all_symptom_labels, all_symptom_preds, average='micro', zero_division=0)\n",
    "        symptom_precision = precision_score(all_symptom_labels, all_symptom_preds, average='micro', zero_division=0)\n",
    "        symptom_recall = recall_score(all_symptom_labels, all_symptom_preds, average='micro', zero_division=0)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save best model\n",
    "        if substance_accuracy > best_substance_acc:\n",
    "            best_substance_acc = substance_accuracy\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"New best model saved! Substance accuracy: {best_substance_acc:.4f}\")\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['substance_acc'].append(substance_accuracy)\n",
    "        history['symptom_f1'].append(symptom_f1)\n",
    "        history['symptom_precision'].append(symptom_precision)\n",
    "        history['symptom_recall'].append(symptom_recall)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} Results:\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Substance Accuracy: {substance_accuracy:.4f}\")\n",
    "        print(f\"Symptom F1: {symptom_f1:.4f}, Precision: {symptom_precision:.4f}, Recall: {symptom_recall:.4f}\")\n",
    "        print(f\"Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Clear cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"\\nLoaded best model with substance accuracy: {best_substance_acc:.4f}\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# Enhanced evaluation function\n",
    "def evaluate_model(model, test_dataset, batch_size=16):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    all_substance_preds = []\n",
    "    all_substance_labels = []\n",
    "    all_symptom_preds = []\n",
    "    all_symptom_labels = []\n",
    "    all_substance_probs = []\n",
    "    all_symptom_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x = batch['x'].to(device)\n",
    "            substance_labels = batch['substance_labels'].to(device)\n",
    "            symptom_labels = batch['symptom_labels'].to(device)\n",
    "            \n",
    "            outputs = model(x)\n",
    "            \n",
    "            substance_preds = torch.argmax(outputs['substance_logits'], dim=1)\n",
    "            symptom_preds = (torch.sigmoid(outputs['symptom_logits']) > 0.5).float()\n",
    "            \n",
    "            all_substance_preds.extend(substance_preds.cpu().numpy())\n",
    "            all_substance_labels.extend(substance_labels.cpu().numpy())\n",
    "            all_symptom_preds.extend(symptom_preds.cpu().numpy())\n",
    "            all_symptom_labels.extend(symptom_labels.cpu().numpy())\n",
    "            all_substance_probs.extend(torch.softmax(outputs['substance_logits'], dim=1).cpu().numpy())\n",
    "            all_symptom_probs.extend(torch.sigmoid(outputs['symptom_logits']).cpu().numpy())\n",
    "    \n",
    "    return {\n",
    "        'substance_preds': np.array(all_substance_preds),\n",
    "        'substance_labels': np.array(all_substance_labels),\n",
    "        'symptom_preds': np.array(all_symptom_preds),\n",
    "        'symptom_labels': np.array(all_symptom_labels),\n",
    "        'substance_probs': np.array(all_substance_probs),\n",
    "        'symptom_probs': np.array(all_symptom_probs)\n",
    "    }\n",
    "\n",
    "# Clear memory before training\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "gc.collect()\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "trained_model, training_history = train_model(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    num_epochs=10,  # Increased epochs for better convergence\n",
    "    batch_size=16,  # Larger batch size for stability\n",
    "    learning_rate=5e-4  # Optimized learning rate\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5bdb3c",
   "metadata": {},
   "source": [
    "6. Evaluate Model\n",
    "\n",
    "Evaluate and print results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33040f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the trainer evaluation code with this:\n",
    "\n",
    "# Evaluate the trained model\n",
    "print(\"\\nEvaluating trained model...\")\n",
    "eval_results = evaluate_model(trained_model, test_dataset, batch_size=16)\n",
    "\n",
    "# Print evaluation metrics\n",
    "substance_accuracy = accuracy_score(eval_results['substance_labels'], eval_results['substance_preds'])\n",
    "symptom_f1 = f1_score(eval_results['symptom_labels'], eval_results['symptom_preds'], average='micro', zero_division=0)\n",
    "symptom_precision = precision_score(eval_results['symptom_labels'], eval_results['symptom_preds'], average='micro', zero_division=0)\n",
    "symptom_recall = recall_score(eval_results['symptom_labels'], eval_results['symptom_preds'], average='micro', zero_division=0)\n",
    "\n",
    "print(f'Final Evaluation Results:')\n",
    "print(f'Substance Accuracy: {substance_accuracy:.4f}')\n",
    "print(f'Symptom F1 Score: {symptom_f1:.4f}')\n",
    "print(f'Symptom Precision: {symptom_precision:.4f}')\n",
    "print(f'Symptom Recall: {symptom_recall:.4f}')\n",
    "\n",
    "# Get predictions for classification reports\n",
    "substance_preds = eval_results['substance_preds']\n",
    "symptom_preds = eval_results['symptom_preds']\n",
    "\n",
    "# Make sure you have these variables defined (they should be from your data preprocessing)\n",
    "# If not, you'll need to extract them from your datasets\n",
    "print('\\nSubstance Classification Report:')\n",
    "print(classification_report(eval_results['substance_labels'], substance_preds, \n",
    "                          target_names=substance_classes, zero_division=0))\n",
    "\n",
    "print('\\nSymptom Classification Report:')\n",
    "print(classification_report(eval_results['symptom_labels'], symptom_preds, \n",
    "                          target_names=symptom_columns, zero_division=0))\n",
    "\n",
    "# Optional: Plot training history\n",
    "def plot_training_history(history):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss plots\n",
    "    axes[0, 0].plot(history['train_loss'], label='Training Loss')\n",
    "    axes[0, 0].plot(history['val_loss'], label='Validation Loss')\n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Substance accuracy\n",
    "    axes[0, 1].plot(history['substance_acc'], label='Substance Accuracy', color='green')\n",
    "    axes[0, 1].set_title('Substance Classification Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Symptom F1 score\n",
    "    axes[1, 0].plot(history['symptom_f1'], label='Symptom F1', color='orange')\n",
    "    axes[1, 0].set_title('Symptom Classification F1 Score')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('F1 Score')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Symptom precision and recall\n",
    "    axes[1, 1].plot(history['symptom_precision'], label='Precision', color='red')\n",
    "    axes[1, 1].plot(history['symptom_recall'], label='Recall', color='blue')\n",
    "    axes[1, 1].set_title('Symptom Precision and Recall')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Score')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history\n",
    "plot_training_history(training_history)\n",
    "\n",
    "# Optional: Save the trained model\n",
    "# torch.save(trained_model.state_dict(), 'best_model.pth')\n",
    "# print(\"Model saved as 'best_model.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6afcaa5",
   "metadata": {},
   "source": [
    "7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf079df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "\n",
    "# Save the trained model (use the trained_model from your custom training loop)\n",
    "torch.save(trained_model.state_dict(), './tfidf_drug_use_model.pt')\n",
    "print('Model state dict saved to ./tfidf_drug_use_model.pt')\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "with open('./tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "print('TF-IDF vectorizer saved to ./tfidf_vectorizer.pkl')\n",
    "\n",
    "# Optional: Save additional model information for easier loading later\n",
    "model_info = {\n",
    "    'model_state_dict': trained_model.state_dict(),\n",
    "    'model_config': {\n",
    "        'tfidf_dim': trained_model.tfidf_dim,\n",
    "        'hidden_dim': trained_model.hidden_dim,\n",
    "        'num_substances': trained_model.num_substances,\n",
    "        'num_symptoms': trained_model.num_symptoms,\n",
    "        'dropout_rate': trained_model.dropout_rate\n",
    "    },\n",
    "    'substance_classes': substance_classes,  # Make sure this variable exists\n",
    "    'symptom_columns': symptom_columns,     # Make sure this variable exists\n",
    "    'training_history': training_history\n",
    "}\n",
    "\n",
    "torch.save(model_info, './complete_model_info.pt')\n",
    "print('Complete model information saved to ./complete_model_info.pt')\n",
    "\n",
    "print('All files saved successfully!')\n",
    "\n",
    "# Example of how to load the model later:\n",
    "def load_trained_model(model_path, vectorizer_path, device='cpu'):\n",
    "    \"\"\"\n",
    "    Function to load the saved model and vectorizer\n",
    "    \"\"\"\n",
    "    # Load vectorizer\n",
    "    with open(vectorizer_path, 'rb') as f:\n",
    "        loaded_vectorizer = pickle.load(f)\n",
    "    \n",
    "    # Load complete model info\n",
    "    model_info = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Recreate model (you'll need to import your DrugUseClassifier class)\n",
    "    # loaded_model = DrugUseClassifier(\n",
    "    #     tfidf_dim=model_info['model_config']['tfidf_dim'],\n",
    "    #     hidden_dim=model_info['model_config']['hidden_dim'],\n",
    "    #     num_substances=model_info['model_config']['num_substances'],\n",
    "    #     num_symptoms=model_info['model_config']['num_symptoms'],\n",
    "    #     dropout_rate=model_info['model_config']['dropout_rate']\n",
    "    # )\n",
    "    \n",
    "    # Load the trained weights\n",
    "    # loaded_model.load_state_dict(model_info['model_state_dict'])\n",
    "    # loaded_model.to(device)\n",
    "    # loaded_model.eval()\n",
    "    \n",
    "    return loaded_vectorizer, model_info\n",
    "\n",
    "# Uncomment and use this to test loading:\n",
    "# loaded_vectorizer, loaded_model_info = load_trained_model('./complete_model_info.pt', './tfidf_vectorizer.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
