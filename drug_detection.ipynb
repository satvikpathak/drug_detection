{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d1840c",
   "metadata": {},
   "source": [
    "1. Setup Environment\n",
    "\n",
    "Install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35a46b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "contourpy 1.3.2 requires numpy>=1.23, but you have numpy 1.22.4 which is incompatible.\n",
      "matplotlib 3.10.3 requires numpy>=1.23, but you have numpy 1.22.4 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers==4.20.1 datasets==2.10.0 pandas==1.4.2 numpy==1.22.4 scikit-learn==1.1.1 torch==1.11.0 nltk==3.7 imbalanced-learn==0.9.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cada9edb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\satvi\\OneDrive\\Desktop\\final_res\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85a04ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\satvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf111fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854e8c5",
   "metadata": {},
   "source": [
    "2. Create and Preprocess drug_use_data.csv\n",
    "\n",
    "Load SetFit/ade_corpus_v2_classification train split, create CSV, and preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0464c215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as drug_use_data.csv\n",
      "Original distribution: Counter({0: 17510, 1: 102, 2: 25})\n",
      "Balanced distribution: Counter({0: 18379, 1: 17512, 2: 16639})\n",
      "Training samples: 42024, Test samples: 10506\n",
      "\n",
      "Final dataset info:\n",
      "Substance classes: ['none', 'opioid', 'stimulant']\n",
      "Symptom classes: ['adverse_event', 'anxiety', 'confusion', 'constipation', 'dizziness', 'drowsiness', 'dyspnea', 'fatigue', 'headache', 'hematoma', 'nausea', 'none', 'overdose', 'pain', 'pruritus', 'rash', 'seizure', 'vomiting']\n",
      "Total features: text + 3 substance classes + 18 symptom classes\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Define splits\n",
    "splits = {'train': 'train.jsonl', 'test': 'test.jsonl'}\n",
    "\n",
    "# Load via hf:// protocol\n",
    "try:\n",
    "    df = pd.read_json(\"hf://datasets/SetFit/ade_corpus_v2_classification/\" + splits[\"train\"], lines=True)\n",
    "except Exception as e:\n",
    "    print(f\"hf:// loading failed: {e}\")\n",
    "    print(\"Falling back to direct URL...\")\n",
    "    url = \"https://huggingface.co/datasets/SetFit/ade_corpus_v2_classification/resolve/main/train.jsonl\"\n",
    "    urllib.request.urlretrieve(url, \"train.jsonl\")\n",
    "    df = pd.read_json(\"train.jsonl\", lines=True)\n",
    "\n",
    "# Expanded substance and symptom lists\n",
    "substance_map = {\n",
    "    'morphine': 'opioid', 'oxycodone': 'opioid', 'fentanyl': 'opioid', 'hydrocodone': 'opioid',\n",
    "    'heroin': 'opioid', 'codeine': 'opioid', 'tramadol': 'opioid',\n",
    "    'cocaine': 'stimulant', 'methamphetamine': 'stimulant', 'amphetamine': 'stimulant',\n",
    "    'placebo': 'none', 'heparin': 'none'\n",
    "}\n",
    "symptom_list = ['nausea', 'confusion', 'drowsiness', 'overdose', 'dizziness', 'vomiting',\n",
    "                'fatigue', 'headache', 'anxiety', 'seizure', 'hematoma', 'rash', 'pain',\n",
    "                'constipation', 'dyspnea', 'pruritus']\n",
    "\n",
    "def assign_labels(text, original_label=None):\n",
    "    substance = 'none'\n",
    "    symptoms = []\n",
    "    text_lower = str(text).lower()\n",
    "    \n",
    "    # Check for substances\n",
    "    for drug, subst in substance_map.items():\n",
    "        if drug in text_lower:\n",
    "            substance = subst\n",
    "            break\n",
    "    \n",
    "    # Check for symptoms\n",
    "    for symp in symptom_list:\n",
    "        if symp in text_lower:\n",
    "            symptoms.append(symp)\n",
    "    \n",
    "    # Use original ADE label if available and no symptoms found\n",
    "    if original_label == 1 and not symptoms:\n",
    "        symptoms = ['adverse_event']\n",
    "    \n",
    "    return substance, symptoms if symptoms else ['none']\n",
    "\n",
    "# Apply labels with original label information\n",
    "if 'label' in df.columns:\n",
    "    df['substance_label'], df['symptom_labels'] = zip(*[\n",
    "        assign_labels(text, label) for text, label in zip(df['text'], df['label'])\n",
    "    ])\n",
    "else:\n",
    "    df['substance_label'], df['symptom_labels'] = zip(*df['text'].apply(lambda x: assign_labels(x)))\n",
    "\n",
    "# Save to CSV BEFORE any processing that might duplicate data\n",
    "df[['text', 'substance_label', 'symptom_labels']].to_csv('drug_use_data.csv', index=False)\n",
    "print('Dataset saved as drug_use_data.csv')\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Encode labels\n",
    "substance_classes = df['substance_label'].unique()\n",
    "substance2id = {label: idx for idx, label in enumerate(substance_classes)}\n",
    "df['substance_label'] = df['substance_label'].map(substance2id)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "symptom_encoded = mlb.fit_transform(df['symptom_labels'])\n",
    "symptom_df = pd.DataFrame(symptom_encoded, columns=mlb.classes_)\n",
    "symptom_columns = mlb.classes_\n",
    "\n",
    "# Combine dataframes\n",
    "df = pd.concat([df[['text', 'substance_label']], symptom_df], axis=1)\n",
    "\n",
    "# Apply SMOTE for balanced training data\n",
    "print(\"Original distribution:\", Counter(df['substance_label']))\n",
    "\n",
    "# Use TF-IDF features for SMOTE\n",
    "temp_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_tfidf_temp = temp_vectorizer.fit_transform(df['text']).toarray()\n",
    "\n",
    "try:\n",
    "    smote = SMOTE(random_state=42, k_neighbors=min(3, Counter(df['substance_label']).most_common()[-1][1] - 1))\n",
    "    X_balanced, y_balanced = smote.fit_resample(X_tfidf_temp, df['substance_label'])\n",
    "    \n",
    "    # Create balanced dataframe by finding closest matches\n",
    "    balanced_indices = []\n",
    "    for x_sample in X_balanced:\n",
    "        similarities = np.dot(X_tfidf_temp, x_sample)\n",
    "        closest_idx = np.argmax(similarities)\n",
    "        balanced_indices.append(closest_idx)\n",
    "    \n",
    "    df_balanced = df.iloc[balanced_indices].copy()\n",
    "    df_balanced['substance_label'] = y_balanced\n",
    "    df = df_balanced\n",
    "    \n",
    "    print(\"Balanced distribution:\", Counter(df['substance_label']))\n",
    "except ValueError as e:\n",
    "    print(f\"SMOTE failed: {e}, using original data with manual balancing\")\n",
    "    # Fallback: simple oversampling for minority classes\n",
    "    minority_threshold = len(df) * 0.1  # 10% threshold\n",
    "    minority_data = []\n",
    "    for label in df['substance_label'].unique():\n",
    "        label_data = df[df['substance_label'] == label]\n",
    "        if len(label_data) < minority_threshold:\n",
    "            # Duplicate minority class samples\n",
    "            multiplier = int(minority_threshold / len(label_data)) + 1\n",
    "            minority_data.append(pd.concat([label_data] * multiplier, ignore_index=True))\n",
    "    \n",
    "    if minority_data:\n",
    "        df = pd.concat([df] + minority_data, ignore_index=True)\n",
    "        print(\"Manual balancing applied\")\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['substance_label'])\n",
    "\n",
    "print(f'Training samples: {len(train_df)}, Test samples: {len(test_df)}')\n",
    "\n",
    "# Print final data info\n",
    "print(\"\\nFinal dataset info:\")\n",
    "print(f\"Substance classes: {list(substance2id.keys())}\")\n",
    "print(f\"Symptom classes: {list(symptom_columns)}\")\n",
    "print(f\"Total features: text + {len(substance2id)} substance classes + {len(symptom_columns)} symptom classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab3f6f0",
   "metadata": {},
   "source": [
    "3. Create TF-IDF Features and Datasets\n",
    "\n",
    "Use TF-IDF features and create custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81dee1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF features...\n",
      "TF-IDF sparse matrix shape: (42024, 2000)\n",
      "Memory usage (sparse): ~3.4 MB\n",
      "Vocabulary size: 2000\n",
      "Converting sparse matrices to dense (this may take a moment)...\n",
      "Processed 0/42024 samples...\n",
      "Processed 5000/42024 samples...\n",
      "Processed 10000/42024 samples...\n",
      "Processed 15000/42024 samples...\n",
      "Processed 20000/42024 samples...\n",
      "Processed 25000/42024 samples...\n",
      "Processed 30000/42024 samples...\n",
      "Processed 35000/42024 samples...\n",
      "Processed 40000/42024 samples...\n",
      "Processed 0/10506 samples...\n",
      "Processed 5000/10506 samples...\n",
      "Processed 10000/10506 samples...\n",
      "Conversion completed successfully!\n",
      "Final TF-IDF feature shape: (42024, 2000)\n",
      "Train symptom data shape: (42024, 18)\n",
      "Test symptom data shape: (10506, 18)\n",
      "Creating training dataset...\n",
      "Dataset created with 42024 samples\n",
      "Feature shape: (42024, 2000)\n",
      "Memory usage: ~320.6 MB\n",
      "Creating test dataset...\n",
      "Dataset created with 10506 samples\n",
      "Feature shape: (10506, 2000)\n",
      "Memory usage: ~80.2 MB\n",
      "Datasets created successfully!\n",
      "Sample data shapes - Features: torch.Size([2000]), Substance: torch.Size([]), Symptoms: torch.Size([18])\n",
      "Memory cleanup completed!\n",
      "TF-IDF processing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# Create TF-IDF features with reduced memory footprint\n",
    "print(\"Creating TF-IDF features...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=2000,  # Reduced from 5000 to save memory\n",
    "    stop_words='english', \n",
    "    ngram_range=(1, 2),  # Reduced from (1,3) to save memory\n",
    "    dtype=np.float32,\n",
    "    min_df=3,  # Increased to reduce vocabulary size\n",
    "    max_df=0.90  # More aggressive filtering\n",
    ")\n",
    "\n",
    "# Keep matrices in sparse format - DON'T convert to dense arrays\n",
    "X_train_tfidf_sparse = vectorizer.fit_transform(train_df['text'])\n",
    "X_test_tfidf_sparse = vectorizer.transform(test_df['text'])\n",
    "\n",
    "print(f\"TF-IDF sparse matrix shape: {X_train_tfidf_sparse.shape}\")\n",
    "print(f\"Memory usage (sparse): ~{X_train_tfidf_sparse.data.nbytes / 1024**2:.1f} MB\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "\n",
    "# Convert sparse matrices to dense in smaller batches to avoid memory issues\n",
    "def sparse_to_dense_batched(sparse_matrix, batch_size=1000):\n",
    "    \"\"\"Convert sparse matrix to dense in batches to manage memory\"\"\"\n",
    "    n_samples = sparse_matrix.shape[0]\n",
    "    n_features = sparse_matrix.shape[1]\n",
    "    \n",
    "    # Pre-allocate dense array\n",
    "    dense_array = np.zeros((n_samples, n_features), dtype=np.float32)\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        end_idx = min(i + batch_size, n_samples)\n",
    "        batch_sparse = sparse_matrix[i:end_idx]\n",
    "        dense_array[i:end_idx] = batch_sparse.toarray()\n",
    "        \n",
    "        if i % (batch_size * 10) == 0:  # Progress update every 10 batches\n",
    "            print(f\"Processed {i}/{n_samples} samples...\")\n",
    "    \n",
    "    return dense_array\n",
    "\n",
    "print(\"Converting sparse matrices to dense (this may take a moment)...\")\n",
    "try:\n",
    "    X_train_tfidf = sparse_to_dense_batched(X_train_tfidf_sparse, batch_size=500)\n",
    "    X_test_tfidf = sparse_to_dense_batched(X_test_tfidf_sparse, batch_size=500)\n",
    "    print(\"Conversion completed successfully!\")\n",
    "except MemoryError:\n",
    "    print(\"Still not enough memory. Using even smaller batch size...\")\n",
    "    try:\n",
    "        X_train_tfidf = sparse_to_dense_batched(X_train_tfidf_sparse, batch_size=100)\n",
    "        X_test_tfidf = sparse_to_dense_batched(X_test_tfidf_sparse, batch_size=100)\n",
    "        print(\"Conversion completed with smaller batches!\")\n",
    "    except MemoryError:\n",
    "        print(\"Memory still insufficient. Switching to sparse-compatible approach...\")\n",
    "        # Alternative: Work directly with sparse matrices (requires model modification)\n",
    "        raise MemoryError(\"Consider using a machine with more RAM or further reducing max_features\")\n",
    "\n",
    "print(f\"Final TF-IDF feature shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "# Memory-efficient dataset class\n",
    "class TFIDFDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, substance_labels, symptom_labels):\n",
    "        # Store as numpy arrays to save memory compared to tensors\n",
    "        self.features = features.astype(np.float32)\n",
    "        self.substance_labels = substance_labels.astype(np.int64)\n",
    "        self.symptom_labels = symptom_labels.astype(np.float32)\n",
    "        \n",
    "        print(f\"Dataset created with {len(self.features)} samples\")\n",
    "        print(f\"Feature shape: {self.features.shape}\")\n",
    "        print(f\"Memory usage: ~{self.features.nbytes / 1024**2:.1f} MB\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert to tensors only when needed (lazy loading)\n",
    "        return {\n",
    "            'x': torch.from_numpy(self.features[idx]).float(),\n",
    "            'substance_labels': torch.from_numpy(np.array(self.substance_labels[idx])).long(),\n",
    "            'symptom_labels': torch.from_numpy(self.symptom_labels[idx]).float()\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "# Ensure symptom columns exist in both dataframes\n",
    "missing_train_cols = [col for col in symptom_columns if col not in train_df.columns]\n",
    "missing_test_cols = [col for col in symptom_columns if col not in test_df.columns]\n",
    "\n",
    "if missing_train_cols:\n",
    "    print(f\"Adding missing columns to train_df: {missing_train_cols}\")\n",
    "    for col in missing_train_cols:\n",
    "        train_df[col] = 0\n",
    "\n",
    "if missing_test_cols:\n",
    "    print(f\"Adding missing columns to test_df: {missing_test_cols}\")\n",
    "    for col in missing_test_cols:\n",
    "        test_df[col] = 0\n",
    "\n",
    "# Get symptom data\n",
    "train_symptom_data = train_df[symptom_columns].values\n",
    "test_symptom_data = test_df[symptom_columns].values\n",
    "\n",
    "print(f\"Train symptom data shape: {train_symptom_data.shape}\")\n",
    "print(f\"Test symptom data shape: {test_symptom_data.shape}\")\n",
    "\n",
    "# Create datasets with memory management\n",
    "import gc\n",
    "\n",
    "# Clear any unnecessary variables\n",
    "if 'X_train_tfidf_sparse' in locals():\n",
    "    del X_train_tfidf_sparse\n",
    "if 'X_test_tfidf_sparse' in locals():\n",
    "    del X_test_tfidf_sparse\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    print(\"Creating training dataset...\")\n",
    "    train_dataset = TFIDFDataset(\n",
    "        X_train_tfidf,\n",
    "        train_df['substance_label'].values,\n",
    "        train_symptom_data\n",
    "    )\n",
    "    \n",
    "    print(\"Creating test dataset...\")\n",
    "    test_dataset = TFIDFDataset(\n",
    "        X_test_tfidf,\n",
    "        test_df['substance_label'].values,\n",
    "        test_symptom_data\n",
    "    )\n",
    "    \n",
    "    print(\"Datasets created successfully!\")\n",
    "    \n",
    "    # Verify dataset integrity\n",
    "    sample = train_dataset[0]\n",
    "    print(f\"Sample data shapes - Features: {sample['x'].shape}, \"\n",
    "          f\"Substance: {sample['substance_labels'].shape}, \"\n",
    "          f\"Symptoms: {sample['symptom_labels'].shape}\")\n",
    "    \n",
    "    # Clean up large arrays to free memory\n",
    "    del X_train_tfidf, X_test_tfidf\n",
    "    gc.collect()\n",
    "    print(\"Memory cleanup completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating datasets: {e}\")\n",
    "    print(\"Debugging information:\")\n",
    "    print(f\"Available memory info:\")\n",
    "    import psutil\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Total RAM: {memory.total / 1024**3:.1f} GB\")\n",
    "    print(f\"Available RAM: {memory.available / 1024**3:.1f} GB\")\n",
    "    print(f\"Used RAM: {memory.percent}%\")\n",
    "    raise\n",
    "\n",
    "print(\"TF-IDF processing completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd25d65",
   "metadata": {},
   "source": [
    "4. Define Custom Model\n",
    "\n",
    "BioBERT for multi-task classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "856710d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining input size...\n",
      "Input size from vectorizer vocabulary: 2000\n",
      "Final input size: 2000\n",
      "Substance classes: 3\n",
      "Symptom labels: 18\n",
      "\n",
      "Model created successfully!\n",
      "Total parameters: 1,501,685\n",
      "Trainable parameters: 1,501,685\n",
      "Model device: cpu\n",
      "\n",
      "Model Architecture:\n",
      "Input size: 2000\n",
      "Substance classes: 3\n",
      "Symptom labels: 18\n",
      "Hidden layers: 512 -> 256 -> 128\n",
      "Features: Batch normalization, residual connections, attention mechanisms, focal loss\n",
      "\n",
      "Testing model with sample batch...\n",
      "✓ Model test successful!\n",
      "  Loss: 0.2741\n",
      "  Substance logits shape: torch.Size([2, 3])\n",
      "  Symptom logits shape: torch.Size([2, 18])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EnhancedMultiTaskModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_substance_classes, num_symptom_labels):\n",
    "        super(EnhancedMultiTaskModel, self).__init__()\n",
    "        \n",
    "        # Input normalization\n",
    "        self.input_norm = torch.nn.BatchNorm1d(input_size)\n",
    "        \n",
    "        # Enhanced architecture with residual connections\n",
    "        self.hidden1 = torch.nn.Linear(input_size, 512)\n",
    "        self.norm1 = torch.nn.BatchNorm1d(512)\n",
    "        self.hidden2 = torch.nn.Linear(512, 256)\n",
    "        self.norm2 = torch.nn.BatchNorm1d(256)\n",
    "        self.hidden3 = torch.nn.Linear(256, 128)\n",
    "        self.norm3 = torch.nn.BatchNorm1d(128)\n",
    "        \n",
    "        # Residual connection layer\n",
    "        self.residual = torch.nn.Linear(input_size, 128)\n",
    "        \n",
    "        # Dropout with different rates\n",
    "        self.dropout1 = torch.nn.Dropout(0.2)\n",
    "        self.dropout2 = torch.nn.Dropout(0.3)\n",
    "        self.dropout3 = torch.nn.Dropout(0.2)\n",
    "        \n",
    "        # Task-specific layers with attention\n",
    "        self.substance_attention = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 128),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.symptom_attention = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 128),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.substance_classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(64, num_substance_classes)\n",
    "        )\n",
    "        \n",
    "        self.symptom_classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(64, num_symptom_labels)\n",
    "        )\n",
    "        \n",
    "        self.num_substance_classes = num_substance_classes\n",
    "        self.num_symptom_labels = num_symptom_labels\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, torch.nn.BatchNorm1d):\n",
    "                torch.nn.init.constant_(m.weight, 1)\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x, substance_labels=None, symptom_labels=None):\n",
    "        # Input normalization\n",
    "        x_norm = self.input_norm(x)\n",
    "        \n",
    "        # Forward pass through hidden layers\n",
    "        hidden = torch.relu(self.hidden1(x_norm))\n",
    "        hidden = self.norm1(hidden)\n",
    "        hidden = self.dropout1(hidden)\n",
    "        \n",
    "        hidden = torch.relu(self.hidden2(hidden))\n",
    "        hidden = self.norm2(hidden)\n",
    "        hidden = self.dropout2(hidden)\n",
    "        \n",
    "        hidden = torch.relu(self.hidden3(hidden))\n",
    "        hidden = self.norm3(hidden)\n",
    "        \n",
    "        # Residual connection\n",
    "        residual = torch.relu(self.residual(x_norm))\n",
    "        hidden = hidden + residual  # Add residual connection\n",
    "        hidden = self.dropout3(hidden)\n",
    "        \n",
    "        # Task-specific attention\n",
    "        substance_att = self.substance_attention(hidden)\n",
    "        symptom_att = self.symptom_attention(hidden)\n",
    "        \n",
    "        # Apply attention\n",
    "        substance_features = hidden * substance_att\n",
    "        symptom_features = hidden * symptom_att\n",
    "        \n",
    "        # Generate logits\n",
    "        substance_logits = self.substance_classifier(substance_features)\n",
    "        symptom_logits = self.symptom_classifier(symptom_features)\n",
    "        \n",
    "        loss = None\n",
    "        if substance_labels is not None and symptom_labels is not None:\n",
    "            # Improved loss calculation\n",
    "            \n",
    "            # Focal loss for substance classification (better for imbalanced classes)\n",
    "            alpha = 0.25\n",
    "            gamma = 2.0\n",
    "            \n",
    "            # Standard cross entropy\n",
    "            ce_loss = torch.nn.functional.cross_entropy(substance_logits, substance_labels, reduction='none')\n",
    "            pt = torch.exp(-ce_loss)\n",
    "            focal_loss = alpha * (1 - pt) ** gamma * ce_loss\n",
    "            substance_loss = focal_loss.mean()\n",
    "            \n",
    "            # Class-balanced BCE loss for symptoms\n",
    "            pos_counts = substance_labels.bincount(minlength=self.num_substance_classes).float()\n",
    "            total_count = len(substance_labels)\n",
    "            pos_weights = total_count / (2.0 * pos_counts + 1e-6)\n",
    "            \n",
    "            # For symptoms, use adaptive positive weights\n",
    "            symptom_pos_counts = symptom_labels.sum(dim=0) + 1e-6\n",
    "            symptom_neg_counts = (1 - symptom_labels).sum(dim=0) + 1e-6\n",
    "            symptom_pos_weights = symptom_neg_counts / symptom_pos_counts\n",
    "            symptom_pos_weights = torch.clamp(symptom_pos_weights, min=0.1, max=10.0)\n",
    "            \n",
    "            symptom_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "                symptom_logits, \n",
    "                symptom_labels, \n",
    "                pos_weight=symptom_pos_weights\n",
    "            )\n",
    "            \n",
    "            # Combine losses with adaptive weighting\n",
    "            substance_weight = 0.7  # Higher weight for substance classification\n",
    "            symptom_weight = 0.3\n",
    "            \n",
    "            loss = substance_weight * substance_loss + symptom_weight * symptom_loss\n",
    "        \n",
    "        return {\n",
    "            'loss': loss, \n",
    "            'substance_logits': substance_logits, \n",
    "            'symptom_logits': symptom_logits,\n",
    "            'substance_probs': torch.softmax(substance_logits, dim=-1),\n",
    "            'symptom_probs': torch.sigmoid(symptom_logits)\n",
    "        }\n",
    "\n",
    "# Get input size from the vectorizer or dataset (multiple methods)\n",
    "print(\"Determining input size...\")\n",
    "\n",
    "# Method 1: From vectorizer (most reliable)\n",
    "if 'vectorizer' in locals() and hasattr(vectorizer, 'vocabulary_'):\n",
    "    actual_input_size = len(vectorizer.vocabulary_)\n",
    "    print(f\"Input size from vectorizer vocabulary: {actual_input_size}\")\n",
    "elif 'vectorizer' in locals() and hasattr(vectorizer, 'max_features'):\n",
    "    actual_input_size = vectorizer.max_features\n",
    "    print(f\"Input size from vectorizer max_features: {actual_input_size}\")\n",
    "# Method 2: From dataset\n",
    "elif 'train_dataset' in locals():\n",
    "    sample = train_dataset[0]\n",
    "    actual_input_size = sample['x'].shape[0]\n",
    "    print(f\"Input size from dataset sample: {actual_input_size}\")\n",
    "# Method 3: Check what we set in vectorizer creation\n",
    "else:\n",
    "    # Fallback to the value we used in vectorizer creation\n",
    "    actual_input_size = 2000  # This was the max_features we set\n",
    "    print(f\"Using fallback input size: {actual_input_size}\")\n",
    "    print(\"Warning: Using fallback size. Ensure this matches your vectorizer configuration.\")\n",
    "\n",
    "# Verify the input size is correct\n",
    "if 'train_dataset' in locals():\n",
    "    sample = train_dataset[0]\n",
    "    sample_input_size = sample['x'].shape[0]\n",
    "    if sample_input_size != actual_input_size:\n",
    "        print(f\"WARNING: Mismatch detected!\")\n",
    "        print(f\"Calculated input size: {actual_input_size}\")\n",
    "        print(f\"Actual dataset input size: {sample_input_size}\")\n",
    "        actual_input_size = sample_input_size\n",
    "        print(f\"Using dataset input size: {actual_input_size}\")\n",
    "\n",
    "print(f\"Final input size: {actual_input_size}\")\n",
    "print(f\"Substance classes: {len(substance_classes)}\")\n",
    "print(f\"Symptom labels: {len(symptom_columns)}\")\n",
    "\n",
    "# Create the model\n",
    "model = EnhancedMultiTaskModel(\n",
    "    input_size=actual_input_size,\n",
    "    num_substance_classes=len(substance_classes),\n",
    "    num_symptom_labels=len(symptom_columns)\n",
    ")\n",
    "\n",
    "# Set device (make sure device is defined)\n",
    "if 'device' not in locals():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device set to: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel created successfully!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(f\"Input size: {actual_input_size}\")\n",
    "print(f\"Substance classes: {len(substance_classes)}\")\n",
    "print(f\"Symptom labels: {len(symptom_columns)}\")\n",
    "print(f\"Hidden layers: 512 -> 256 -> 128\")\n",
    "print(\"Features: Batch normalization, residual connections, attention mechanisms, focal loss\")\n",
    "\n",
    "# Test model with a sample batch to ensure everything works\n",
    "if 'train_dataset' in locals():\n",
    "    print(\"\\nTesting model with sample batch...\")\n",
    "    try:\n",
    "        sample_batch = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=False)\n",
    "        batch = next(iter(sample_batch))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = batch['x'].to(device)\n",
    "            substance_labels = batch['substance_labels'].to(device)\n",
    "            symptom_labels = batch['symptom_labels'].to(device)\n",
    "            \n",
    "            outputs = model(x, substance_labels=substance_labels, symptom_labels=symptom_labels)\n",
    "            \n",
    "            print(f\"✓ Model test successful!\")\n",
    "            print(f\"  Loss: {outputs['loss'].item():.4f}\")\n",
    "            print(f\"  Substance logits shape: {outputs['substance_logits'].shape}\")\n",
    "            print(f\"  Symptom logits shape: {outputs['symptom_logits'].shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Model test failed: {e}\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"Warning: train_dataset not available for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe4d1a",
   "metadata": {},
   "source": [
    "5. Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36396a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "Starting training...\n",
      "Total epochs: 10\n",
      "Batch size: 16\n",
      "Learning rate: 0.0005\n",
      "Total training batches: 2627\n",
      "Total validation batches: 657\n",
      "Epoch 1/10, Batch 0/2627, Loss: 0.3197\n",
      "Epoch 1/10, Batch 50/2627, Loss: 0.1440\n",
      "Epoch 1/10, Batch 100/2627, Loss: 0.0479\n",
      "Epoch 1/10, Batch 150/2627, Loss: 0.0440\n",
      "Epoch 1/10, Batch 200/2627, Loss: 0.0236\n",
      "Epoch 1/10, Batch 250/2627, Loss: 0.0890\n",
      "Epoch 1/10, Batch 300/2627, Loss: 0.0234\n",
      "Epoch 1/10, Batch 350/2627, Loss: 0.0570\n",
      "Epoch 1/10, Batch 400/2627, Loss: 0.0367\n",
      "Epoch 1/10, Batch 450/2627, Loss: 0.0349\n",
      "Epoch 1/10, Batch 500/2627, Loss: 0.0183\n",
      "Epoch 1/10, Batch 550/2627, Loss: 0.0188\n",
      "Epoch 1/10, Batch 600/2627, Loss: 0.0073\n",
      "Epoch 1/10, Batch 650/2627, Loss: 0.0090\n",
      "Epoch 1/10, Batch 700/2627, Loss: 0.0071\n",
      "Epoch 1/10, Batch 750/2627, Loss: 0.0799\n",
      "Epoch 1/10, Batch 800/2627, Loss: 0.0134\n",
      "Epoch 1/10, Batch 850/2627, Loss: 0.0099\n",
      "Epoch 1/10, Batch 900/2627, Loss: 0.0083\n",
      "Epoch 1/10, Batch 950/2627, Loss: 0.0182\n",
      "Epoch 1/10, Batch 1000/2627, Loss: 0.0153\n",
      "Epoch 1/10, Batch 1050/2627, Loss: 0.0120\n",
      "Epoch 1/10, Batch 1100/2627, Loss: 0.0103\n",
      "Epoch 1/10, Batch 1150/2627, Loss: 0.0037\n",
      "Epoch 1/10, Batch 1200/2627, Loss: 0.0066\n",
      "Epoch 1/10, Batch 1250/2627, Loss: 0.0187\n",
      "Epoch 1/10, Batch 1300/2627, Loss: 0.0076\n",
      "Epoch 1/10, Batch 1350/2627, Loss: 0.0115\n",
      "Epoch 1/10, Batch 1400/2627, Loss: 0.0101\n",
      "Epoch 1/10, Batch 1450/2627, Loss: 0.0063\n",
      "Epoch 1/10, Batch 1500/2627, Loss: 0.0052\n",
      "Epoch 1/10, Batch 1550/2627, Loss: 0.0513\n",
      "Epoch 1/10, Batch 1600/2627, Loss: 0.0059\n",
      "Epoch 1/10, Batch 1650/2627, Loss: 0.0079\n",
      "Epoch 1/10, Batch 1700/2627, Loss: 0.0028\n",
      "Epoch 1/10, Batch 1750/2627, Loss: 0.1107\n",
      "Epoch 1/10, Batch 1800/2627, Loss: 0.0076\n",
      "Epoch 1/10, Batch 1850/2627, Loss: 0.0114\n",
      "Epoch 1/10, Batch 1900/2627, Loss: 0.0144\n",
      "Epoch 1/10, Batch 1950/2627, Loss: 0.0021\n",
      "Epoch 1/10, Batch 2000/2627, Loss: 0.0072\n",
      "Epoch 1/10, Batch 2050/2627, Loss: 0.0062\n",
      "Epoch 1/10, Batch 2100/2627, Loss: 0.0098\n",
      "Epoch 1/10, Batch 2150/2627, Loss: 0.0366\n",
      "Epoch 1/10, Batch 2200/2627, Loss: 0.0093\n",
      "Epoch 1/10, Batch 2250/2627, Loss: 0.0100\n",
      "Epoch 1/10, Batch 2300/2627, Loss: 0.0005\n",
      "Epoch 1/10, Batch 2350/2627, Loss: 0.0775\n",
      "Epoch 1/10, Batch 2400/2627, Loss: 0.0126\n",
      "Epoch 1/10, Batch 2450/2627, Loss: 0.0062\n",
      "Epoch 1/10, Batch 2500/2627, Loss: 0.0164\n",
      "Epoch 1/10, Batch 2550/2627, Loss: 0.0090\n",
      "Epoch 1/10, Batch 2600/2627, Loss: 0.0108\n",
      "New best model saved! Substance accuracy: 0.7570\n",
      "\n",
      "Epoch 1/10 Results:\n",
      "Train Loss: 0.0236, Val Loss: 1.1389\n",
      "Substance Accuracy: 0.7570\n",
      "Symptom F1: 0.8483, Precision: 0.8637, Recall: 0.8335\n",
      "Learning Rate: 0.000488\n",
      "------------------------------------------------------------\n",
      "Epoch 2/10, Batch 0/2627, Loss: 0.0077\n",
      "Epoch 2/10, Batch 50/2627, Loss: 0.0046\n",
      "Epoch 2/10, Batch 100/2627, Loss: 0.0040\n",
      "Epoch 2/10, Batch 150/2627, Loss: 0.0038\n",
      "Epoch 2/10, Batch 200/2627, Loss: 0.0029\n",
      "Epoch 2/10, Batch 250/2627, Loss: 0.0054\n",
      "Epoch 2/10, Batch 300/2627, Loss: 0.0084\n",
      "Epoch 2/10, Batch 350/2627, Loss: 0.0035\n",
      "Epoch 2/10, Batch 400/2627, Loss: 0.0067\n",
      "Epoch 2/10, Batch 450/2627, Loss: 0.0033\n",
      "Epoch 2/10, Batch 500/2627, Loss: 0.0664\n",
      "Epoch 2/10, Batch 550/2627, Loss: 0.0099\n",
      "Epoch 2/10, Batch 600/2627, Loss: 0.0091\n",
      "Epoch 2/10, Batch 650/2627, Loss: 0.0042\n",
      "Epoch 2/10, Batch 700/2627, Loss: 0.0076\n",
      "Epoch 2/10, Batch 750/2627, Loss: 0.0067\n",
      "Epoch 2/10, Batch 800/2627, Loss: 0.0065\n",
      "Epoch 2/10, Batch 850/2627, Loss: 0.0074\n",
      "Epoch 2/10, Batch 900/2627, Loss: 0.0046\n",
      "Epoch 2/10, Batch 950/2627, Loss: 0.0073\n",
      "Epoch 2/10, Batch 1000/2627, Loss: 0.0096\n",
      "Epoch 2/10, Batch 1050/2627, Loss: 0.0114\n",
      "Epoch 2/10, Batch 1100/2627, Loss: 0.0427\n",
      "Epoch 2/10, Batch 1150/2627, Loss: 0.0067\n",
      "Epoch 2/10, Batch 1200/2627, Loss: 0.0429\n",
      "Epoch 2/10, Batch 1250/2627, Loss: 0.0045\n",
      "Epoch 2/10, Batch 1300/2627, Loss: 0.0078\n",
      "Epoch 2/10, Batch 1350/2627, Loss: 0.0048\n",
      "Epoch 2/10, Batch 1400/2627, Loss: 0.0072\n",
      "Epoch 2/10, Batch 1450/2627, Loss: 0.0073\n",
      "Epoch 2/10, Batch 1500/2627, Loss: 0.0058\n",
      "Epoch 2/10, Batch 1550/2627, Loss: 0.0052\n",
      "Epoch 2/10, Batch 1600/2627, Loss: 0.0066\n",
      "Epoch 2/10, Batch 1650/2627, Loss: 0.0073\n",
      "Epoch 2/10, Batch 1700/2627, Loss: 0.0036\n",
      "Epoch 2/10, Batch 1750/2627, Loss: 0.0008\n",
      "Epoch 2/10, Batch 1800/2627, Loss: 0.0185\n",
      "Epoch 2/10, Batch 1850/2627, Loss: 0.0032\n",
      "Epoch 2/10, Batch 1900/2627, Loss: 0.0028\n",
      "Epoch 2/10, Batch 1950/2627, Loss: 0.0047\n",
      "Epoch 2/10, Batch 2000/2627, Loss: 0.0307\n",
      "Epoch 2/10, Batch 2050/2627, Loss: 0.0118\n",
      "Epoch 2/10, Batch 2100/2627, Loss: 0.0059\n",
      "Epoch 2/10, Batch 2150/2627, Loss: 0.0053\n",
      "Epoch 2/10, Batch 2200/2627, Loss: 0.0092\n",
      "Epoch 2/10, Batch 2250/2627, Loss: 0.0023\n",
      "Epoch 2/10, Batch 2300/2627, Loss: 0.0090\n",
      "Epoch 2/10, Batch 2350/2627, Loss: 0.0085\n",
      "Epoch 2/10, Batch 2400/2627, Loss: 0.0136\n",
      "Epoch 2/10, Batch 2450/2627, Loss: 0.0063\n",
      "Epoch 2/10, Batch 2500/2627, Loss: 0.0055\n",
      "Epoch 2/10, Batch 2550/2627, Loss: 0.0041\n",
      "Epoch 2/10, Batch 2600/2627, Loss: 0.0053\n",
      "New best model saved! Substance accuracy: 0.7820\n",
      "\n",
      "Epoch 2/10 Results:\n",
      "Train Loss: 0.0102, Val Loss: 0.9491\n",
      "Substance Accuracy: 0.7820\n",
      "Symptom F1: 0.8789, Precision: 0.8898, Recall: 0.8683\n",
      "Learning Rate: 0.000452\n",
      "------------------------------------------------------------\n",
      "Epoch 3/10, Batch 0/2627, Loss: 0.0026\n",
      "Epoch 3/10, Batch 50/2627, Loss: 0.0050\n",
      "Epoch 3/10, Batch 100/2627, Loss: 0.0066\n",
      "Epoch 3/10, Batch 150/2627, Loss: 0.0063\n",
      "Epoch 3/10, Batch 200/2627, Loss: 0.0123\n",
      "Epoch 3/10, Batch 250/2627, Loss: 0.0063\n",
      "Epoch 3/10, Batch 300/2627, Loss: 0.0092\n",
      "Epoch 3/10, Batch 350/2627, Loss: 0.0088\n",
      "Epoch 3/10, Batch 400/2627, Loss: 0.0049\n",
      "Epoch 3/10, Batch 450/2627, Loss: 0.0037\n",
      "Epoch 3/10, Batch 500/2627, Loss: 0.0003\n",
      "Epoch 3/10, Batch 550/2627, Loss: 0.0068\n",
      "Epoch 3/10, Batch 600/2627, Loss: 0.0054\n",
      "Epoch 3/10, Batch 650/2627, Loss: 0.0062\n",
      "Epoch 3/10, Batch 700/2627, Loss: 0.0223\n",
      "Epoch 3/10, Batch 750/2627, Loss: 0.0054\n",
      "Epoch 3/10, Batch 800/2627, Loss: 0.0142\n",
      "Epoch 3/10, Batch 850/2627, Loss: 0.0022\n",
      "Epoch 3/10, Batch 900/2627, Loss: 0.0060\n",
      "Epoch 3/10, Batch 950/2627, Loss: 0.0078\n",
      "Epoch 3/10, Batch 1000/2627, Loss: 0.0035\n",
      "Epoch 3/10, Batch 1050/2627, Loss: 0.0025\n",
      "Epoch 3/10, Batch 1100/2627, Loss: 0.0037\n",
      "Epoch 3/10, Batch 1150/2627, Loss: 0.0076\n",
      "Epoch 3/10, Batch 1200/2627, Loss: 0.0012\n",
      "Epoch 3/10, Batch 1250/2627, Loss: 0.0047\n",
      "Epoch 3/10, Batch 1300/2627, Loss: 0.0082\n",
      "Epoch 3/10, Batch 1350/2627, Loss: 0.0151\n",
      "Epoch 3/10, Batch 1400/2627, Loss: 0.0025\n",
      "Epoch 3/10, Batch 1450/2627, Loss: 0.0004\n",
      "Epoch 3/10, Batch 1500/2627, Loss: 0.0085\n",
      "Epoch 3/10, Batch 1550/2627, Loss: 0.0110\n",
      "Epoch 3/10, Batch 1600/2627, Loss: 0.0033\n",
      "Epoch 3/10, Batch 1650/2627, Loss: 0.0026\n",
      "Epoch 3/10, Batch 1700/2627, Loss: 0.0096\n",
      "Epoch 3/10, Batch 1750/2627, Loss: 0.0016\n",
      "Epoch 3/10, Batch 1800/2627, Loss: 0.0042\n",
      "Epoch 3/10, Batch 1850/2627, Loss: 0.0068\n",
      "Epoch 3/10, Batch 1900/2627, Loss: 0.0014\n",
      "Epoch 3/10, Batch 1950/2627, Loss: 0.0058\n",
      "Epoch 3/10, Batch 2000/2627, Loss: 0.0069\n",
      "Epoch 3/10, Batch 2050/2627, Loss: 0.0075\n",
      "Epoch 3/10, Batch 2100/2627, Loss: 0.0061\n",
      "Epoch 3/10, Batch 2150/2627, Loss: 0.0041\n",
      "Epoch 3/10, Batch 2200/2627, Loss: 0.0083\n",
      "Epoch 3/10, Batch 2250/2627, Loss: 0.0010\n",
      "Epoch 3/10, Batch 2300/2627, Loss: 0.0033\n",
      "Epoch 3/10, Batch 2350/2627, Loss: 0.0006\n",
      "Epoch 3/10, Batch 2400/2627, Loss: 0.0065\n",
      "Epoch 3/10, Batch 2450/2627, Loss: 0.0002\n",
      "Epoch 3/10, Batch 2500/2627, Loss: 0.0089\n",
      "Epoch 3/10, Batch 2550/2627, Loss: 0.0258\n",
      "Epoch 3/10, Batch 2600/2627, Loss: 0.0108\n",
      "\n",
      "Epoch 3/10 Results:\n",
      "Train Loss: 0.0086, Val Loss: 1.0665\n",
      "Substance Accuracy: 0.7577\n",
      "Symptom F1: 0.8887, Precision: 0.8980, Recall: 0.8796\n",
      "Learning Rate: 0.000397\n",
      "------------------------------------------------------------\n",
      "Epoch 4/10, Batch 0/2627, Loss: 0.0057\n",
      "Epoch 4/10, Batch 50/2627, Loss: 0.0073\n",
      "Epoch 4/10, Batch 100/2627, Loss: 0.0025\n",
      "Epoch 4/10, Batch 150/2627, Loss: 0.0017\n",
      "Epoch 4/10, Batch 200/2627, Loss: 0.0133\n",
      "Epoch 4/10, Batch 250/2627, Loss: 0.0010\n",
      "Epoch 4/10, Batch 300/2627, Loss: 0.0044\n",
      "Epoch 4/10, Batch 350/2627, Loss: 0.0093\n",
      "Epoch 4/10, Batch 400/2627, Loss: 0.0031\n",
      "Epoch 4/10, Batch 450/2627, Loss: 0.0053\n",
      "Epoch 4/10, Batch 500/2627, Loss: 0.0026\n",
      "Epoch 4/10, Batch 550/2627, Loss: 0.0055\n",
      "Epoch 4/10, Batch 600/2627, Loss: 0.0026\n",
      "Epoch 4/10, Batch 650/2627, Loss: 0.0021\n",
      "Epoch 4/10, Batch 700/2627, Loss: 0.0038\n",
      "Epoch 4/10, Batch 750/2627, Loss: 0.0063\n",
      "Epoch 4/10, Batch 800/2627, Loss: 0.0052\n",
      "Epoch 4/10, Batch 850/2627, Loss: 0.0053\n",
      "Epoch 4/10, Batch 900/2627, Loss: 0.0102\n",
      "Epoch 4/10, Batch 950/2627, Loss: 0.0082\n",
      "Epoch 4/10, Batch 1000/2627, Loss: 0.0022\n",
      "Epoch 4/10, Batch 1050/2627, Loss: 0.0035\n",
      "Epoch 4/10, Batch 1100/2627, Loss: 0.0065\n",
      "Epoch 4/10, Batch 1150/2627, Loss: 0.0170\n",
      "Epoch 4/10, Batch 1200/2627, Loss: 0.0030\n",
      "Epoch 4/10, Batch 1250/2627, Loss: 0.0006\n",
      "Epoch 4/10, Batch 1300/2627, Loss: 0.0023\n",
      "Epoch 4/10, Batch 1350/2627, Loss: 0.0014\n",
      "Epoch 4/10, Batch 1400/2627, Loss: 0.0036\n",
      "Epoch 4/10, Batch 1450/2627, Loss: 0.0064\n",
      "Epoch 4/10, Batch 1500/2627, Loss: 0.0097\n",
      "Epoch 4/10, Batch 1550/2627, Loss: 0.0017\n",
      "Epoch 4/10, Batch 1600/2627, Loss: 0.0100\n",
      "Epoch 4/10, Batch 1650/2627, Loss: 0.0038\n",
      "Epoch 4/10, Batch 1700/2627, Loss: 0.0286\n",
      "Epoch 4/10, Batch 1750/2627, Loss: 0.0024\n",
      "Epoch 4/10, Batch 1800/2627, Loss: 0.0093\n",
      "Epoch 4/10, Batch 1850/2627, Loss: 0.0094\n",
      "Epoch 4/10, Batch 1900/2627, Loss: 0.0064\n",
      "Epoch 4/10, Batch 1950/2627, Loss: 0.0010\n",
      "Epoch 4/10, Batch 2000/2627, Loss: 0.0051\n",
      "Epoch 4/10, Batch 2050/2627, Loss: 0.0385\n",
      "Epoch 4/10, Batch 2100/2627, Loss: 0.0081\n",
      "Epoch 4/10, Batch 2150/2627, Loss: 0.0033\n",
      "Epoch 4/10, Batch 2200/2627, Loss: 0.0026\n",
      "Epoch 4/10, Batch 2250/2627, Loss: 0.0137\n",
      "Epoch 4/10, Batch 2300/2627, Loss: 0.0064\n",
      "Epoch 4/10, Batch 2350/2627, Loss: 0.0121\n",
      "Epoch 4/10, Batch 2400/2627, Loss: 0.0022\n",
      "Epoch 4/10, Batch 2450/2627, Loss: 0.0206\n",
      "Epoch 4/10, Batch 2500/2627, Loss: 0.0021\n",
      "Epoch 4/10, Batch 2550/2627, Loss: 0.0067\n",
      "Epoch 4/10, Batch 2600/2627, Loss: 0.0038\n",
      "New best model saved! Substance accuracy: 0.7913\n",
      "\n",
      "Epoch 4/10 Results:\n",
      "Train Loss: 0.0077, Val Loss: 0.9356\n",
      "Substance Accuracy: 0.7913\n",
      "Symptom F1: 0.8845, Precision: 0.8962, Recall: 0.8730\n",
      "Learning Rate: 0.000328\n",
      "------------------------------------------------------------\n",
      "Epoch 5/10, Batch 0/2627, Loss: 0.0054\n",
      "Epoch 5/10, Batch 50/2627, Loss: 0.0027\n",
      "Epoch 5/10, Batch 100/2627, Loss: 0.0105\n",
      "Epoch 5/10, Batch 150/2627, Loss: 0.0058\n",
      "Epoch 5/10, Batch 200/2627, Loss: 0.0133\n",
      "Epoch 5/10, Batch 250/2627, Loss: 0.0024\n",
      "Epoch 5/10, Batch 300/2627, Loss: 0.0019\n",
      "Epoch 5/10, Batch 350/2627, Loss: 0.0042\n",
      "Epoch 5/10, Batch 400/2627, Loss: 0.0053\n",
      "Epoch 5/10, Batch 450/2627, Loss: 0.0021\n",
      "Epoch 5/10, Batch 500/2627, Loss: 0.0068\n",
      "Epoch 5/10, Batch 550/2627, Loss: 0.0053\n",
      "Epoch 5/10, Batch 600/2627, Loss: 0.0112\n",
      "Epoch 5/10, Batch 650/2627, Loss: 0.0006\n",
      "Epoch 5/10, Batch 700/2627, Loss: 0.0035\n",
      "Epoch 5/10, Batch 750/2627, Loss: 0.0014\n",
      "Epoch 5/10, Batch 800/2627, Loss: 0.0008\n",
      "Epoch 5/10, Batch 850/2627, Loss: 0.0031\n",
      "Epoch 5/10, Batch 900/2627, Loss: 0.0046\n",
      "Epoch 5/10, Batch 950/2627, Loss: 0.0101\n",
      "Epoch 5/10, Batch 1000/2627, Loss: 0.0055\n",
      "Epoch 5/10, Batch 1050/2627, Loss: 0.0049\n",
      "Epoch 5/10, Batch 1100/2627, Loss: 0.0050\n",
      "Epoch 5/10, Batch 1150/2627, Loss: 0.0016\n",
      "Epoch 5/10, Batch 1200/2627, Loss: 0.0031\n",
      "Epoch 5/10, Batch 1250/2627, Loss: 0.0008\n",
      "Epoch 5/10, Batch 1300/2627, Loss: 0.0007\n",
      "Epoch 5/10, Batch 1350/2627, Loss: 0.0049\n",
      "Epoch 5/10, Batch 1400/2627, Loss: 0.0017\n",
      "Epoch 5/10, Batch 1450/2627, Loss: 0.0012\n",
      "Epoch 5/10, Batch 1500/2627, Loss: 0.0012\n",
      "Epoch 5/10, Batch 1550/2627, Loss: 0.0016\n",
      "Epoch 5/10, Batch 1600/2627, Loss: 0.0026\n",
      "Epoch 5/10, Batch 1650/2627, Loss: 0.0031\n",
      "Epoch 5/10, Batch 1700/2627, Loss: 0.0062\n",
      "Epoch 5/10, Batch 1750/2627, Loss: 0.0070\n",
      "Epoch 5/10, Batch 1800/2627, Loss: 0.0048\n",
      "Epoch 5/10, Batch 1850/2627, Loss: 0.0017\n",
      "Epoch 5/10, Batch 1900/2627, Loss: 0.0041\n",
      "Epoch 5/10, Batch 1950/2627, Loss: 0.0057\n",
      "Epoch 5/10, Batch 2000/2627, Loss: 0.0032\n",
      "Epoch 5/10, Batch 2050/2627, Loss: 0.0056\n",
      "Epoch 5/10, Batch 2100/2627, Loss: 0.0055\n",
      "Epoch 5/10, Batch 2150/2627, Loss: 0.0036\n",
      "Epoch 5/10, Batch 2200/2627, Loss: 0.0336\n",
      "Epoch 5/10, Batch 2250/2627, Loss: 0.0022\n",
      "Epoch 5/10, Batch 2300/2627, Loss: 0.0063\n",
      "Epoch 5/10, Batch 2350/2627, Loss: 0.0015\n",
      "Epoch 5/10, Batch 2400/2627, Loss: 0.0021\n",
      "Epoch 5/10, Batch 2450/2627, Loss: 0.0029\n",
      "Epoch 5/10, Batch 2500/2627, Loss: 0.0056\n",
      "Epoch 5/10, Batch 2550/2627, Loss: 0.0227\n",
      "Epoch 5/10, Batch 2600/2627, Loss: 0.0139\n",
      "New best model saved! Substance accuracy: 0.7997\n",
      "\n",
      "Epoch 5/10 Results:\n",
      "Train Loss: 0.0067, Val Loss: 0.8613\n",
      "Substance Accuracy: 0.7997\n",
      "Symptom F1: 0.8882, Precision: 0.8950, Recall: 0.8815\n",
      "Learning Rate: 0.000251\n",
      "------------------------------------------------------------\n",
      "Epoch 6/10, Batch 0/2627, Loss: 0.0029\n",
      "Epoch 6/10, Batch 50/2627, Loss: 0.0046\n",
      "Epoch 6/10, Batch 100/2627, Loss: 0.0171\n",
      "Epoch 6/10, Batch 150/2627, Loss: 0.0112\n",
      "Epoch 6/10, Batch 200/2627, Loss: 0.0058\n",
      "Epoch 6/10, Batch 250/2627, Loss: 0.0071\n",
      "Epoch 6/10, Batch 300/2627, Loss: 0.0093\n",
      "Epoch 6/10, Batch 350/2627, Loss: 0.0006\n",
      "Epoch 6/10, Batch 400/2627, Loss: 0.0289\n",
      "Epoch 6/10, Batch 450/2627, Loss: 0.0061\n",
      "Epoch 6/10, Batch 500/2627, Loss: 0.0029\n",
      "Epoch 6/10, Batch 550/2627, Loss: 0.0034\n",
      "Epoch 6/10, Batch 600/2627, Loss: 0.0025\n",
      "Epoch 6/10, Batch 650/2627, Loss: 0.0114\n",
      "Epoch 6/10, Batch 700/2627, Loss: 0.0172\n",
      "Epoch 6/10, Batch 750/2627, Loss: 0.0064\n",
      "Epoch 6/10, Batch 800/2627, Loss: 0.0033\n",
      "Epoch 6/10, Batch 850/2627, Loss: 0.0079\n",
      "Epoch 6/10, Batch 900/2627, Loss: 0.0009\n",
      "Epoch 6/10, Batch 950/2627, Loss: 0.0041\n",
      "Epoch 6/10, Batch 1000/2627, Loss: 0.0028\n",
      "Epoch 6/10, Batch 1050/2627, Loss: 0.0038\n",
      "Epoch 6/10, Batch 1100/2627, Loss: 0.0031\n",
      "Epoch 6/10, Batch 1150/2627, Loss: 0.0056\n",
      "Epoch 6/10, Batch 1200/2627, Loss: 0.0066\n",
      "Epoch 6/10, Batch 1250/2627, Loss: 0.0077\n",
      "Epoch 6/10, Batch 1300/2627, Loss: 0.0041\n",
      "Epoch 6/10, Batch 1350/2627, Loss: 0.0008\n",
      "Epoch 6/10, Batch 1400/2627, Loss: 0.0230\n",
      "Epoch 6/10, Batch 1450/2627, Loss: 0.0114\n",
      "Epoch 6/10, Batch 1500/2627, Loss: 0.0022\n",
      "Epoch 6/10, Batch 1550/2627, Loss: 0.0008\n",
      "Epoch 6/10, Batch 1600/2627, Loss: 0.0017\n",
      "Epoch 6/10, Batch 1650/2627, Loss: 0.0051\n",
      "Epoch 6/10, Batch 1700/2627, Loss: 0.0062\n",
      "Epoch 6/10, Batch 1750/2627, Loss: 0.0105\n",
      "Epoch 6/10, Batch 1800/2627, Loss: 0.0081\n",
      "Epoch 6/10, Batch 1850/2627, Loss: 0.0002\n",
      "Epoch 6/10, Batch 1900/2627, Loss: 0.0158\n",
      "Epoch 6/10, Batch 1950/2627, Loss: 0.0035\n",
      "Epoch 6/10, Batch 2000/2627, Loss: 0.0070\n",
      "Epoch 6/10, Batch 2050/2627, Loss: 0.0053\n",
      "Epoch 6/10, Batch 2100/2627, Loss: 0.0059\n",
      "Epoch 6/10, Batch 2150/2627, Loss: 0.0041\n",
      "Epoch 6/10, Batch 2200/2627, Loss: 0.0028\n",
      "Epoch 6/10, Batch 2250/2627, Loss: 0.0044\n",
      "Epoch 6/10, Batch 2300/2627, Loss: 0.0079\n",
      "Epoch 6/10, Batch 2350/2627, Loss: 0.0044\n",
      "Epoch 6/10, Batch 2400/2627, Loss: 0.0011\n",
      "Epoch 6/10, Batch 2450/2627, Loss: 0.0026\n",
      "Epoch 6/10, Batch 2500/2627, Loss: 0.0034\n",
      "Epoch 6/10, Batch 2550/2627, Loss: 0.0057\n",
      "Epoch 6/10, Batch 2600/2627, Loss: 0.0034\n",
      "\n",
      "Epoch 6/10 Results:\n",
      "Train Loss: 0.0063, Val Loss: 0.7806\n",
      "Substance Accuracy: 0.7985\n",
      "Symptom F1: 0.8863, Precision: 0.8932, Recall: 0.8794\n",
      "Learning Rate: 0.000173\n",
      "------------------------------------------------------------\n",
      "Epoch 7/10, Batch 0/2627, Loss: 0.0093\n",
      "Epoch 7/10, Batch 50/2627, Loss: 0.0050\n",
      "Epoch 7/10, Batch 100/2627, Loss: 0.0022\n",
      "Epoch 7/10, Batch 150/2627, Loss: 0.0005\n",
      "Epoch 7/10, Batch 200/2627, Loss: 0.0023\n",
      "Epoch 7/10, Batch 250/2627, Loss: 0.0027\n",
      "Epoch 7/10, Batch 300/2627, Loss: 0.0006\n",
      "Epoch 7/10, Batch 350/2627, Loss: 0.0053\n",
      "Epoch 7/10, Batch 400/2627, Loss: 0.0060\n",
      "Epoch 7/10, Batch 450/2627, Loss: 0.0044\n",
      "Epoch 7/10, Batch 500/2627, Loss: 0.0028\n",
      "Epoch 7/10, Batch 550/2627, Loss: 0.0113\n",
      "Epoch 7/10, Batch 600/2627, Loss: 0.0054\n",
      "Epoch 7/10, Batch 650/2627, Loss: 0.0073\n",
      "Epoch 7/10, Batch 700/2627, Loss: 0.0029\n",
      "Epoch 7/10, Batch 750/2627, Loss: 0.0008\n",
      "Epoch 7/10, Batch 800/2627, Loss: 0.0004\n",
      "Epoch 7/10, Batch 850/2627, Loss: 0.0063\n",
      "Epoch 7/10, Batch 900/2627, Loss: 0.0029\n",
      "Epoch 7/10, Batch 950/2627, Loss: 0.0084\n",
      "Epoch 7/10, Batch 1000/2627, Loss: 0.0012\n",
      "Epoch 7/10, Batch 1050/2627, Loss: 0.0016\n",
      "Epoch 7/10, Batch 1100/2627, Loss: 0.0011\n",
      "Epoch 7/10, Batch 1150/2627, Loss: 0.0015\n",
      "Epoch 7/10, Batch 1200/2627, Loss: 0.0002\n",
      "Epoch 7/10, Batch 1250/2627, Loss: 0.0053\n",
      "Epoch 7/10, Batch 1300/2627, Loss: 0.0114\n",
      "Epoch 7/10, Batch 1350/2627, Loss: 0.0057\n",
      "Epoch 7/10, Batch 1400/2627, Loss: 0.0722\n",
      "Epoch 7/10, Batch 1450/2627, Loss: 0.0177\n",
      "Epoch 7/10, Batch 1500/2627, Loss: 0.0058\n",
      "Epoch 7/10, Batch 1550/2627, Loss: 0.0067\n",
      "Epoch 7/10, Batch 1600/2627, Loss: 0.0075\n",
      "Epoch 7/10, Batch 1650/2627, Loss: 0.0053\n",
      "Epoch 7/10, Batch 1700/2627, Loss: 0.0069\n",
      "Epoch 7/10, Batch 1750/2627, Loss: 0.0056\n",
      "Epoch 7/10, Batch 1800/2627, Loss: 0.0008\n",
      "Epoch 7/10, Batch 1850/2627, Loss: 0.0033\n",
      "Epoch 7/10, Batch 1900/2627, Loss: 0.0018\n",
      "Epoch 7/10, Batch 1950/2627, Loss: 0.0164\n",
      "Epoch 7/10, Batch 2000/2627, Loss: 0.0033\n",
      "Epoch 7/10, Batch 2050/2627, Loss: 0.0023\n",
      "Epoch 7/10, Batch 2100/2627, Loss: 0.0138\n",
      "Epoch 7/10, Batch 2150/2627, Loss: 0.0056\n",
      "Epoch 7/10, Batch 2200/2627, Loss: 0.0023\n",
      "Epoch 7/10, Batch 2250/2627, Loss: 0.0062\n",
      "Epoch 7/10, Batch 2300/2627, Loss: 0.0146\n",
      "Epoch 7/10, Batch 2350/2627, Loss: 0.0056\n",
      "Epoch 7/10, Batch 2400/2627, Loss: 0.0053\n",
      "Epoch 7/10, Batch 2450/2627, Loss: 0.0039\n",
      "Epoch 7/10, Batch 2500/2627, Loss: 0.0017\n",
      "Epoch 7/10, Batch 2550/2627, Loss: 0.0011\n",
      "Epoch 7/10, Batch 2600/2627, Loss: 0.0117\n",
      "New best model saved! Substance accuracy: 0.8031\n",
      "\n",
      "Epoch 7/10 Results:\n",
      "Train Loss: 0.0055, Val Loss: 0.9284\n",
      "Substance Accuracy: 0.8031\n",
      "Symptom F1: 0.8852, Precision: 0.8967, Recall: 0.8740\n",
      "Learning Rate: 0.000104\n",
      "------------------------------------------------------------\n",
      "Epoch 8/10, Batch 0/2627, Loss: 0.0081\n",
      "Epoch 8/10, Batch 50/2627, Loss: 0.0169\n",
      "Epoch 8/10, Batch 100/2627, Loss: 0.0042\n",
      "Epoch 8/10, Batch 150/2627, Loss: 0.0041\n",
      "Epoch 8/10, Batch 200/2627, Loss: 0.0007\n",
      "Epoch 8/10, Batch 250/2627, Loss: 0.0053\n",
      "Epoch 8/10, Batch 300/2627, Loss: 0.0023\n",
      "Epoch 8/10, Batch 350/2627, Loss: 0.0026\n",
      "Epoch 8/10, Batch 400/2627, Loss: 0.0005\n",
      "Epoch 8/10, Batch 450/2627, Loss: 0.0117\n",
      "Epoch 8/10, Batch 500/2627, Loss: 0.0032\n",
      "Epoch 8/10, Batch 550/2627, Loss: 0.0048\n",
      "Epoch 8/10, Batch 600/2627, Loss: 0.0017\n",
      "Epoch 8/10, Batch 650/2627, Loss: 0.0053\n",
      "Epoch 8/10, Batch 700/2627, Loss: 0.0169\n",
      "Epoch 8/10, Batch 750/2627, Loss: 0.0020\n",
      "Epoch 8/10, Batch 800/2627, Loss: 0.0002\n",
      "Epoch 8/10, Batch 850/2627, Loss: 0.0031\n",
      "Epoch 8/10, Batch 900/2627, Loss: 0.0019\n",
      "Epoch 8/10, Batch 950/2627, Loss: 0.0079\n",
      "Epoch 8/10, Batch 1000/2627, Loss: 0.0146\n",
      "Epoch 8/10, Batch 1050/2627, Loss: 0.0030\n",
      "Epoch 8/10, Batch 1100/2627, Loss: 0.0024\n",
      "Epoch 8/10, Batch 1150/2627, Loss: 0.0206\n",
      "Epoch 8/10, Batch 1200/2627, Loss: 0.0189\n",
      "Epoch 8/10, Batch 1250/2627, Loss: 0.0086\n",
      "Epoch 8/10, Batch 1300/2627, Loss: 0.0075\n",
      "Epoch 8/10, Batch 1350/2627, Loss: 0.0007\n",
      "Epoch 8/10, Batch 1400/2627, Loss: 0.0049\n",
      "Epoch 8/10, Batch 1450/2627, Loss: 0.0024\n",
      "Epoch 8/10, Batch 1500/2627, Loss: 0.0014\n",
      "Epoch 8/10, Batch 1550/2627, Loss: 0.0015\n",
      "Epoch 8/10, Batch 1600/2627, Loss: 0.0013\n",
      "Epoch 8/10, Batch 1650/2627, Loss: 0.0023\n",
      "Epoch 8/10, Batch 1700/2627, Loss: 0.0036\n",
      "Epoch 8/10, Batch 1750/2627, Loss: 0.0015\n",
      "Epoch 8/10, Batch 1800/2627, Loss: 0.0059\n",
      "Epoch 8/10, Batch 1850/2627, Loss: 0.0012\n",
      "Epoch 8/10, Batch 1900/2627, Loss: 0.0008\n",
      "Epoch 8/10, Batch 1950/2627, Loss: 0.0010\n",
      "Epoch 8/10, Batch 2000/2627, Loss: 0.0063\n",
      "Epoch 8/10, Batch 2050/2627, Loss: 0.0059\n",
      "Epoch 8/10, Batch 2100/2627, Loss: 0.0031\n",
      "Epoch 8/10, Batch 2150/2627, Loss: 0.0088\n",
      "Epoch 8/10, Batch 2200/2627, Loss: 0.0041\n",
      "Epoch 8/10, Batch 2250/2627, Loss: 0.0025\n",
      "Epoch 8/10, Batch 2300/2627, Loss: 0.0136\n",
      "Epoch 8/10, Batch 2350/2627, Loss: 0.0037\n",
      "Epoch 8/10, Batch 2400/2627, Loss: 0.0034\n",
      "Epoch 8/10, Batch 2450/2627, Loss: 0.0016\n",
      "Epoch 8/10, Batch 2500/2627, Loss: 0.0019\n",
      "Epoch 8/10, Batch 2550/2627, Loss: 0.0042\n",
      "Epoch 8/10, Batch 2600/2627, Loss: 0.0010\n",
      "\n",
      "Epoch 8/10 Results:\n",
      "Train Loss: 0.0052, Val Loss: 0.9359\n",
      "Substance Accuracy: 0.8029\n",
      "Symptom F1: 0.8910, Precision: 0.8996, Recall: 0.8826\n",
      "Learning Rate: 0.000049\n",
      "------------------------------------------------------------\n",
      "Epoch 9/10, Batch 0/2627, Loss: 0.0072\n",
      "Epoch 9/10, Batch 50/2627, Loss: 0.0044\n",
      "Epoch 9/10, Batch 100/2627, Loss: 0.0021\n",
      "Epoch 9/10, Batch 150/2627, Loss: 0.0036\n",
      "Epoch 9/10, Batch 200/2627, Loss: 0.0014\n",
      "Epoch 9/10, Batch 250/2627, Loss: 0.0054\n",
      "Epoch 9/10, Batch 300/2627, Loss: 0.0031\n",
      "Epoch 9/10, Batch 350/2627, Loss: 0.0033\n",
      "Epoch 9/10, Batch 400/2627, Loss: 0.0032\n",
      "Epoch 9/10, Batch 450/2627, Loss: 0.0066\n",
      "Epoch 9/10, Batch 500/2627, Loss: 0.0011\n",
      "Epoch 9/10, Batch 550/2627, Loss: 0.0017\n",
      "Epoch 9/10, Batch 600/2627, Loss: 0.0015\n",
      "Epoch 9/10, Batch 650/2627, Loss: 0.0041\n",
      "Epoch 9/10, Batch 700/2627, Loss: 0.0078\n",
      "Epoch 9/10, Batch 750/2627, Loss: 0.0034\n",
      "Epoch 9/10, Batch 800/2627, Loss: 0.0082\n",
      "Epoch 9/10, Batch 850/2627, Loss: 0.0024\n",
      "Epoch 9/10, Batch 900/2627, Loss: 0.0029\n",
      "Epoch 9/10, Batch 950/2627, Loss: 0.0014\n",
      "Epoch 9/10, Batch 1000/2627, Loss: 0.0045\n",
      "Epoch 9/10, Batch 1050/2627, Loss: 0.0019\n",
      "Epoch 9/10, Batch 1100/2627, Loss: 0.0010\n",
      "Epoch 9/10, Batch 1150/2627, Loss: 0.0025\n",
      "Epoch 9/10, Batch 1200/2627, Loss: 0.0047\n",
      "Epoch 9/10, Batch 1250/2627, Loss: 0.0001\n",
      "Epoch 9/10, Batch 1300/2627, Loss: 0.0048\n",
      "Epoch 9/10, Batch 1350/2627, Loss: 0.0049\n",
      "Epoch 9/10, Batch 1400/2627, Loss: 0.0021\n",
      "Epoch 9/10, Batch 1450/2627, Loss: 0.0002\n",
      "Epoch 9/10, Batch 1500/2627, Loss: 0.0082\n",
      "Epoch 9/10, Batch 1550/2627, Loss: 0.0007\n",
      "Epoch 9/10, Batch 1600/2627, Loss: 0.0063\n",
      "Epoch 9/10, Batch 1650/2627, Loss: 0.0099\n",
      "Epoch 9/10, Batch 1700/2627, Loss: 0.0006\n",
      "Epoch 9/10, Batch 1750/2627, Loss: 0.0031\n",
      "Epoch 9/10, Batch 1800/2627, Loss: 0.0015\n",
      "Epoch 9/10, Batch 1850/2627, Loss: 0.0050\n",
      "Epoch 9/10, Batch 1900/2627, Loss: 0.0006\n",
      "Epoch 9/10, Batch 1950/2627, Loss: 0.0028\n",
      "Epoch 9/10, Batch 2000/2627, Loss: 0.0033\n",
      "Epoch 9/10, Batch 2050/2627, Loss: 0.0043\n",
      "Epoch 9/10, Batch 2100/2627, Loss: 0.0032\n",
      "Epoch 9/10, Batch 2150/2627, Loss: 0.0023\n",
      "Epoch 9/10, Batch 2200/2627, Loss: 0.0086\n",
      "Epoch 9/10, Batch 2250/2627, Loss: 0.0018\n",
      "Epoch 9/10, Batch 2300/2627, Loss: 0.0063\n",
      "Epoch 9/10, Batch 2350/2627, Loss: 0.0053\n",
      "Epoch 9/10, Batch 2400/2627, Loss: 0.0020\n",
      "Epoch 9/10, Batch 2450/2627, Loss: 0.0025\n",
      "Epoch 9/10, Batch 2500/2627, Loss: 0.0015\n",
      "Epoch 9/10, Batch 2550/2627, Loss: 0.0016\n",
      "Epoch 9/10, Batch 2600/2627, Loss: 0.0069\n",
      "New best model saved! Substance accuracy: 0.8055\n",
      "\n",
      "Epoch 9/10 Results:\n",
      "Train Loss: 0.0048, Val Loss: 0.8916\n",
      "Substance Accuracy: 0.8055\n",
      "Symptom F1: 0.8839, Precision: 0.8943, Recall: 0.8738\n",
      "Learning Rate: 0.000013\n",
      "------------------------------------------------------------\n",
      "Epoch 10/10, Batch 0/2627, Loss: 0.0017\n",
      "Epoch 10/10, Batch 50/2627, Loss: 0.0032\n",
      "Epoch 10/10, Batch 100/2627, Loss: 0.0002\n",
      "Epoch 10/10, Batch 150/2627, Loss: 0.0033\n",
      "Epoch 10/10, Batch 200/2627, Loss: 0.0091\n",
      "Epoch 10/10, Batch 250/2627, Loss: 0.0048\n",
      "Epoch 10/10, Batch 300/2627, Loss: 0.0026\n",
      "Epoch 10/10, Batch 350/2627, Loss: 0.0133\n",
      "Epoch 10/10, Batch 400/2627, Loss: 0.0116\n",
      "Epoch 10/10, Batch 450/2627, Loss: 0.0043\n",
      "Epoch 10/10, Batch 500/2627, Loss: 0.0071\n",
      "Epoch 10/10, Batch 550/2627, Loss: 0.0067\n",
      "Epoch 10/10, Batch 600/2627, Loss: 0.0056\n",
      "Epoch 10/10, Batch 650/2627, Loss: 0.0137\n",
      "Epoch 10/10, Batch 700/2627, Loss: 0.0061\n",
      "Epoch 10/10, Batch 750/2627, Loss: 0.0004\n",
      "Epoch 10/10, Batch 800/2627, Loss: 0.0013\n",
      "Epoch 10/10, Batch 850/2627, Loss: 0.0027\n",
      "Epoch 10/10, Batch 900/2627, Loss: 0.0018\n",
      "Epoch 10/10, Batch 950/2627, Loss: 0.0019\n",
      "Epoch 10/10, Batch 1000/2627, Loss: 0.0036\n",
      "Epoch 10/10, Batch 1050/2627, Loss: 0.0075\n",
      "Epoch 10/10, Batch 1100/2627, Loss: 0.0029\n",
      "Epoch 10/10, Batch 1150/2627, Loss: 0.0048\n",
      "Epoch 10/10, Batch 1200/2627, Loss: 0.0048\n",
      "Epoch 10/10, Batch 1250/2627, Loss: 0.0125\n",
      "Epoch 10/10, Batch 1300/2627, Loss: 0.0037\n",
      "Epoch 10/10, Batch 1350/2627, Loss: 0.0147\n",
      "Epoch 10/10, Batch 1400/2627, Loss: 0.0029\n",
      "Epoch 10/10, Batch 1450/2627, Loss: 0.0074\n",
      "Epoch 10/10, Batch 1500/2627, Loss: 0.0044\n",
      "Epoch 10/10, Batch 1550/2627, Loss: 0.0012\n",
      "Epoch 10/10, Batch 1600/2627, Loss: 0.0028\n",
      "Epoch 10/10, Batch 1650/2627, Loss: 0.0047\n",
      "Epoch 10/10, Batch 1700/2627, Loss: 0.0010\n",
      "Epoch 10/10, Batch 1750/2627, Loss: 0.0142\n",
      "Epoch 10/10, Batch 1800/2627, Loss: 0.0104\n",
      "Epoch 10/10, Batch 1850/2627, Loss: 0.0031\n",
      "Epoch 10/10, Batch 1900/2627, Loss: 0.0004\n",
      "Epoch 10/10, Batch 1950/2627, Loss: 0.0021\n",
      "Epoch 10/10, Batch 2000/2627, Loss: 0.0146\n",
      "Epoch 10/10, Batch 2050/2627, Loss: 0.0015\n",
      "Epoch 10/10, Batch 2100/2627, Loss: 0.0044\n",
      "Epoch 10/10, Batch 2150/2627, Loss: 0.0017\n",
      "Epoch 10/10, Batch 2200/2627, Loss: 0.0025\n",
      "Epoch 10/10, Batch 2250/2627, Loss: 0.0062\n",
      "Epoch 10/10, Batch 2300/2627, Loss: 0.0041\n",
      "Epoch 10/10, Batch 2350/2627, Loss: 0.0048\n",
      "Epoch 10/10, Batch 2400/2627, Loss: 0.0013\n",
      "Epoch 10/10, Batch 2450/2627, Loss: 0.0016\n",
      "Epoch 10/10, Batch 2500/2627, Loss: 0.0021\n",
      "Epoch 10/10, Batch 2550/2627, Loss: 0.0013\n",
      "Epoch 10/10, Batch 2600/2627, Loss: 0.0056\n",
      "\n",
      "Epoch 10/10 Results:\n",
      "Train Loss: 0.0047, Val Loss: 0.9593\n",
      "Substance Accuracy: 0.8039\n",
      "Symptom F1: 0.8874, Precision: 0.8973, Recall: 0.8776\n",
      "Learning Rate: 0.000001\n",
      "------------------------------------------------------------\n",
      "\n",
      "Loaded best model with substance accuracy: 0.8055\n",
      "\n",
      "Training completed!\n",
      "\n",
      "============================================================\n",
      "TRAINING SUMMARY\n",
      "============================================================\n",
      "Total Epochs: 10\n",
      "Final Train Loss: 0.0047\n",
      "Final Val Loss: 0.9593\n",
      "Best Substance Accuracy: 0.8055\n",
      "Final Symptom F1: 0.8874\n",
      "\n",
      "Epoch-by-Epoch Progress:\n",
      "Epoch | Train Loss | Val Loss | Substance Acc | Symptom F1\n",
      "------------------------------------------------------------\n",
      "    1 |     0.0236 |   1.1389 |        0.7570 |     0.8483\n",
      "    2 |     0.0102 |   0.9491 |        0.7820 |     0.8789\n",
      "    3 |     0.0086 |   1.0665 |        0.7577 |     0.8887\n",
      "    4 |     0.0077 |   0.9356 |        0.7913 |     0.8845\n",
      "    5 |     0.0067 |   0.8613 |        0.7997 |     0.8882\n",
      "    6 |     0.0063 |   0.7806 |        0.7985 |     0.8863\n",
      "    7 |     0.0055 |   0.9284 |        0.8031 |     0.8852\n",
      "    8 |     0.0052 |   0.9359 |        0.8029 |     0.8910\n",
      "    9 |     0.0048 |   0.8916 |        0.8055 |     0.8839\n",
      "   10 |     0.0047 |   0.9593 |        0.8039 |     0.8874\n",
      "Training history saved to training_history.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "# Custom training function since Transformers Trainer expects specific model structure\n",
    "def train_model(model, train_dataset, test_dataset, num_epochs=10, batch_size=16, learning_rate=5e-4):\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=False)\n",
    "    \n",
    "    # Optimizer and scheduler\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.001)\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-6)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'substance_acc': [], 'symptom_f1': [],\n",
    "        'symptom_precision': [], 'symptom_recall': []\n",
    "    }\n",
    "    \n",
    "    best_substance_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    print(f\"Total epochs: {num_epochs}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Learning rate: {learning_rate}\")\n",
    "    print(f\"Total training batches: {len(train_loader)}\")\n",
    "    print(f\"Total validation batches: {len(test_loader)}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Move data to device\n",
    "            x = batch['x'].to(device)\n",
    "            substance_labels = batch['substance_labels'].to(device)\n",
    "            symptom_labels = batch['symptom_labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(x, substance_labels=substance_labels, symptom_labels=symptom_labels)\n",
    "            loss = outputs['loss']\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            \n",
    "            # Log progress every 50 batches\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, \"\n",
    "                      f\"Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        all_substance_preds = []\n",
    "        all_substance_labels = []\n",
    "        all_symptom_preds = []\n",
    "        all_symptom_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                x = batch['x'].to(device)\n",
    "                substance_labels = batch['substance_labels'].to(device)\n",
    "                symptom_labels = batch['symptom_labels'].to(device)\n",
    "                \n",
    "                outputs = model(x, substance_labels=substance_labels, symptom_labels=symptom_labels)\n",
    "                loss = outputs['loss']\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "                \n",
    "                # Collect predictions\n",
    "                substance_preds = torch.argmax(outputs['substance_logits'], dim=1)\n",
    "                symptom_preds = (torch.sigmoid(outputs['symptom_logits']) > 0.5).float()\n",
    "                \n",
    "                all_substance_preds.extend(substance_preds.cpu().numpy())\n",
    "                all_substance_labels.extend(substance_labels.cpu().numpy())\n",
    "                all_symptom_preds.extend(symptom_preds.cpu().numpy())\n",
    "                all_symptom_labels.extend(symptom_labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        \n",
    "        # Calculate metrics\n",
    "        all_substance_preds = np.array(all_substance_preds)\n",
    "        all_substance_labels = np.array(all_substance_labels)\n",
    "        all_symptom_preds = np.array(all_symptom_preds)\n",
    "        all_symptom_labels = np.array(all_symptom_labels)\n",
    "        \n",
    "        substance_accuracy = accuracy_score(all_substance_labels, all_substance_preds)\n",
    "        symptom_f1 = f1_score(all_symptom_labels, all_symptom_preds, average='micro', zero_division=0)\n",
    "        symptom_precision = precision_score(all_symptom_labels, all_symptom_preds, average='micro', zero_division=0)\n",
    "        symptom_recall = recall_score(all_symptom_labels, all_symptom_preds, average='micro', zero_division=0)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save best model\n",
    "        if substance_accuracy > best_substance_acc:\n",
    "            best_substance_acc = substance_accuracy\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            print(f\"New best model saved! Substance accuracy: {best_substance_acc:.4f}\")\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['substance_acc'].append(substance_accuracy)\n",
    "        history['symptom_f1'].append(symptom_f1)\n",
    "        history['symptom_precision'].append(symptom_precision)\n",
    "        history['symptom_recall'].append(symptom_recall)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} Results:\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Substance Accuracy: {substance_accuracy:.4f}\")\n",
    "        print(f\"Symptom F1: {symptom_f1:.4f}, Precision: {symptom_precision:.4f}, Recall: {symptom_recall:.4f}\")\n",
    "        print(f\"Learning Rate: {scheduler.get_last_lr()[0]:.6f}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Clear cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"\\nLoaded best model with substance accuracy: {best_substance_acc:.4f}\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# Enhanced evaluation function\n",
    "def evaluate_model(model, test_dataset, batch_size=16):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    all_substance_preds = []\n",
    "    all_substance_labels = []\n",
    "    all_symptom_preds = []\n",
    "    all_symptom_labels = []\n",
    "    all_substance_probs = []\n",
    "    all_symptom_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x = batch['x'].to(device)\n",
    "            substance_labels = batch['substance_labels'].to(device)\n",
    "            symptom_labels = batch['symptom_labels'].to(device)\n",
    "            \n",
    "            outputs = model(x)\n",
    "            \n",
    "            substance_preds = torch.argmax(outputs['substance_logits'], dim=1)\n",
    "            symptom_preds = (torch.sigmoid(outputs['symptom_logits']) > 0.5).float()\n",
    "            \n",
    "            all_substance_preds.extend(substance_preds.cpu().numpy())\n",
    "            all_substance_labels.extend(substance_labels.cpu().numpy())\n",
    "            all_symptom_preds.extend(symptom_preds.cpu().numpy())\n",
    "            all_symptom_labels.extend(symptom_labels.cpu().numpy())\n",
    "            all_substance_probs.extend(torch.softmax(outputs['substance_logits'], dim=1).cpu().numpy())\n",
    "            all_symptom_probs.extend(torch.sigmoid(outputs['symptom_logits']).cpu().numpy())\n",
    "\n",
    "    return {\n",
    "        'substance_preds': np.array(all_substance_preds),\n",
    "        'substance_labels': np.array(all_substance_labels),\n",
    "        'symptom_preds': np.array(all_symptom_preds),\n",
    "        'symptom_labels': np.array(all_symptom_labels),\n",
    "        'substance_probs': np.array(all_substance_probs),\n",
    "        'symptom_probs': np.array(all_symptom_probs)\n",
    "    }\n",
    "\n",
    "# Simple text-based visualization function (alternative to matplotlib)\n",
    "def print_training_summary(history):\n",
    "    \"\"\"\n",
    "    Print a text-based summary of training progress\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    epochs = len(history['train_loss'])\n",
    "    \n",
    "    print(f\"Total Epochs: {epochs}\")\n",
    "    print(f\"Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"Best Substance Accuracy: {max(history['substance_acc']):.4f}\")\n",
    "    print(f\"Final Symptom F1: {history['symptom_f1'][-1]:.4f}\")\n",
    "    \n",
    "    print(\"\\nEpoch-by-Epoch Progress:\")\n",
    "    print(\"Epoch | Train Loss | Val Loss | Substance Acc | Symptom F1\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        print(f\"{i+1:5d} | {history['train_loss'][i]:10.4f} | {history['val_loss'][i]:8.4f} | \"\n",
    "              f\"{history['substance_acc'][i]:13.4f} | {history['symptom_f1'][i]:10.4f}\")\n",
    "\n",
    "# Alternative: Save training history to CSV for external plotting\n",
    "def save_training_history(history, filename='training_history.csv'):\n",
    "    \"\"\"\n",
    "    Save training history to CSV file for external plotting\n",
    "    \"\"\"\n",
    "    import csv\n",
    "    \n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['epoch'] + list(history.keys())\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for i in range(len(history['train_loss'])):\n",
    "            row = {'epoch': i+1}\n",
    "            for key in history.keys():\n",
    "                row[key] = history[key][i]\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    print(f\"Training history saved to {filename}\")\n",
    "\n",
    "# Clear memory before training\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "gc.collect()\n",
    "\n",
    "# Train the model\n",
    "print(\"Starting model training...\")\n",
    "trained_model, training_history = train_model(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    num_epochs=10,  # Increased epochs for better convergence\n",
    "    batch_size=16,  # Larger batch size for stability\n",
    "    learning_rate=5e-4  # Optimized learning rate\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "\n",
    "# Print training summary instead of plotting\n",
    "print_training_summary(training_history)\n",
    "\n",
    "# Optionally save to CSV for external plotting\n",
    "save_training_history(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5bdb3c",
   "metadata": {},
   "source": [
    "6. Evaluate Model\n",
    "\n",
    "Evaluate and print results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33040f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating trained model...\n",
      "Final Evaluation Results:\n",
      "Substance Accuracy: 0.8039\n",
      "Symptom F1 Score: 0.8874\n",
      "Symptom Precision: 0.8973\n",
      "Symptom Recall: 0.8776\n",
      "\n",
      "Substance Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        none       1.00      0.44      0.61      3676\n",
      "      opioid       0.64      1.00      0.78      3502\n",
      "   stimulant       0.98      1.00      0.99      3328\n",
      "\n",
      "    accuracy                           0.80     10506\n",
      "   macro avg       0.87      0.81      0.79     10506\n",
      "weighted avg       0.87      0.80      0.79     10506\n",
      "\n",
      "\n",
      "Symptom Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "adverse_event       0.88      0.93      0.90      4559\n",
      "      anxiety       0.00      0.00      0.00         5\n",
      "    confusion       0.98      0.99      0.99       628\n",
      " constipation       0.25      0.50      0.33         2\n",
      "    dizziness       0.78      1.00      0.88        35\n",
      "   drowsiness       0.67      0.67      0.67         3\n",
      "      dyspnea       0.09      0.29      0.14        14\n",
      "      fatigue       0.12      0.50      0.20         4\n",
      "     headache       0.71      0.89      0.79        19\n",
      "     hematoma       0.00      0.00      0.00         6\n",
      "       nausea       0.90      0.94      0.92       102\n",
      "         none       0.93      0.80      0.86      4326\n",
      "     overdose       0.92      0.89      0.91       244\n",
      "         pain       0.91      0.93      0.92       530\n",
      "     pruritus       0.00      0.00      0.00         5\n",
      "         rash       0.66      0.77      0.71        30\n",
      "      seizure       0.97      0.97      0.97        79\n",
      "     vomiting       1.00      0.88      0.93        16\n",
      "\n",
      "    micro avg       0.90      0.88      0.89     10607\n",
      "    macro avg       0.60      0.66      0.62     10607\n",
      " weighted avg       0.91      0.88      0.89     10607\n",
      "  samples avg       0.87      0.88      0.88     10607\n",
      "\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 75\u001b[0m\n\u001b[0;32m     72\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Plot the training history\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m \u001b[43mplot_training_history\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining_history\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m \u001b[38;5;66;03m# Optional: Save the trained model\u001b[39;00m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;66;03m# torch.save(trained_model.state_dict(), 'best_model.pth')\u001b[39;00m\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# print(\"Model saved as 'best_model.pth'\")\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 35\u001b[0m, in \u001b[0;36mplot_training_history\u001b[1;34m(history)\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mplot_training_history\u001b[39m(history):\n\u001b[1;32m---> 35\u001b[0m     fig, axes \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m15\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;66;03m# Loss plots\u001b[39;00m\n\u001b[0;32m     38\u001b[0m     axes[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mplot(history[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_loss\u001b[39m\u001b[38;5;124m'\u001b[39m], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# Replace the trainer evaluation code with this:\n",
    "\n",
    "# Evaluate the trained model\n",
    "print(\"\\nEvaluating trained model...\")\n",
    "eval_results = evaluate_model(trained_model, test_dataset, batch_size=16)\n",
    "\n",
    "# Print evaluation metrics\n",
    "substance_accuracy = accuracy_score(eval_results['substance_labels'], eval_results['substance_preds'])\n",
    "symptom_f1 = f1_score(eval_results['symptom_labels'], eval_results['symptom_preds'], average='micro', zero_division=0)\n",
    "symptom_precision = precision_score(eval_results['symptom_labels'], eval_results['symptom_preds'], average='micro', zero_division=0)\n",
    "symptom_recall = recall_score(eval_results['symptom_labels'], eval_results['symptom_preds'], average='micro', zero_division=0)\n",
    "\n",
    "print(f'Final Evaluation Results:')\n",
    "print(f'Substance Accuracy: {substance_accuracy:.4f}')\n",
    "print(f'Symptom F1 Score: {symptom_f1:.4f}')\n",
    "print(f'Symptom Precision: {symptom_precision:.4f}')\n",
    "print(f'Symptom Recall: {symptom_recall:.4f}')\n",
    "\n",
    "# Get predictions for classification reports\n",
    "substance_preds = eval_results['substance_preds']\n",
    "symptom_preds = eval_results['symptom_preds']\n",
    "\n",
    "# Make sure you have these variables defined (they should be from your data preprocessing)\n",
    "# If not, you'll need to extract them from your datasets\n",
    "print('\\nSubstance Classification Report:')\n",
    "print(classification_report(eval_results['substance_labels'], substance_preds, \n",
    "                          target_names=substance_classes, zero_division=0))\n",
    "\n",
    "print('\\nSymptom Classification Report:')\n",
    "print(classification_report(eval_results['symptom_labels'], symptom_preds, \n",
    "                          target_names=symptom_columns, zero_division=0))\n",
    "\n",
    "# Optional: Plot training history\n",
    "def plot_training_history(history):\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    # Loss plots\n",
    "    axes[0, 0].plot(history['train_loss'], label='Training Loss')\n",
    "    axes[0, 0].plot(history['val_loss'], label='Validation Loss')\n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Substance accuracy\n",
    "    axes[0, 1].plot(history['substance_acc'], label='Substance Accuracy', color='green')\n",
    "    axes[0, 1].set_title('Substance Classification Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # Symptom F1 score\n",
    "    axes[1, 0].plot(history['symptom_f1'], label='Symptom F1', color='orange')\n",
    "    axes[1, 0].set_title('Symptom Classification F1 Score')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('F1 Score')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Symptom precision and recall\n",
    "    axes[1, 1].plot(history['symptom_precision'], label='Precision', color='red')\n",
    "    axes[1, 1].plot(history['symptom_recall'], label='Recall', color='blue')\n",
    "    axes[1, 1].set_title('Symptom Precision and Recall')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Score')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the training history\n",
    "plot_training_history(training_history)\n",
    "\n",
    "# Optional: Save the trained model\n",
    "# torch.save(trained_model.state_dict(), 'best_model.pth')\n",
    "# print(\"Model saved as 'best_model.pth'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6afcaa5",
   "metadata": {},
   "source": [
    "7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf079df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import torch\n",
    "\n",
    "# Save the trained model (use the trained_model from your custom training loop)\n",
    "torch.save(trained_model.state_dict(), './tfidf_drug_use_model.pt')\n",
    "print('Model state dict saved to ./tfidf_drug_use_model.pt')\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "with open('./tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "print('TF-IDF vectorizer saved to ./tfidf_vectorizer.pkl')\n",
    "\n",
    "# Optional: Save additional model information for easier loading later\n",
    "model_info = {\n",
    "    'model_state_dict': trained_model.state_dict(),\n",
    "    'model_config': {\n",
    "        'tfidf_dim': trained_model.tfidf_dim,\n",
    "        'hidden_dim': trained_model.hidden_dim,\n",
    "        'num_substances': trained_model.num_substances,\n",
    "        'num_symptoms': trained_model.num_symptoms,\n",
    "        'dropout_rate': trained_model.dropout_rate\n",
    "    },\n",
    "    'substance_classes': substance_classes,  # Make sure this variable exists\n",
    "    'symptom_columns': symptom_columns,     # Make sure this variable exists\n",
    "    'training_history': training_history\n",
    "}\n",
    "\n",
    "torch.save(model_info, './complete_model_info.pt')\n",
    "print('Complete model information saved to ./complete_model_info.pt')\n",
    "\n",
    "print('All files saved successfully!')\n",
    "\n",
    "# Example of how to load the model later:\n",
    "def load_trained_model(model_path, vectorizer_path, device='cpu'):\n",
    "    \"\"\"\n",
    "    Function to load the saved model and vectorizer\n",
    "    \"\"\"\n",
    "    # Load vectorizer\n",
    "    with open(vectorizer_path, 'rb') as f:\n",
    "        loaded_vectorizer = pickle.load(f)\n",
    "    \n",
    "    # Load complete model info\n",
    "    model_info = torch.load(model_path, map_location=device)\n",
    "    \n",
    "    # Recreate model (you'll need to import your DrugUseClassifier class)\n",
    "    # loaded_model = DrugUseClassifier(\n",
    "    #     tfidf_dim=model_info['model_config']['tfidf_dim'],\n",
    "    #     hidden_dim=model_info['model_config']['hidden_dim'],\n",
    "    #     num_substances=model_info['model_config']['num_substances'],\n",
    "    #     num_symptoms=model_info['model_config']['num_symptoms'],\n",
    "    #     dropout_rate=model_info['model_config']['dropout_rate']\n",
    "    # )\n",
    "    \n",
    "    # Load the trained weights\n",
    "    # loaded_model.load_state_dict(model_info['model_state_dict'])\n",
    "    # loaded_model.to(device)\n",
    "    # loaded_model.eval()\n",
    "    \n",
    "    return loaded_vectorizer, model_info\n",
    "\n",
    "# Uncomment and use this to test loading:\n",
    "# loaded_vectorizer, loaded_model_info = load_trained_model('./complete_model_info.pt', './tfidf_vectorizer.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
