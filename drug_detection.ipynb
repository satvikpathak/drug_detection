{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d1840c",
   "metadata": {},
   "source": [
    "1. Setup Environment\n",
    "\n",
    "Install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35a46b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers==4.20.1 datasets==2.10.0 pandas==1.4.2 numpy==1.22.4 scikit-learn==1.1.1 torch==1.11.0 nltk==3.7 imbalanced-learn==0.9.1 optuna==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cada9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTEENN\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85a04ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\satvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\satvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\satvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk_downloads = ['stopwords', 'punkt', 'wordnet', 'averaged_perceptron_tagger', 'omw-1.4']\n",
    "for item in nltk_downloads:\n",
    "    try:\n",
    "        nltk.data.find(f'tokenizers/{item}' if item == 'punkt' else f'corpora/{item}')\n",
    "    except LookupError:\n",
    "        nltk.download(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf111fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854e8c5",
   "metadata": {},
   "source": [
    "2. Create and Preprocess drug_use_data.csv\n",
    "\n",
    "Load SetFit/ade_corpus_v2_classification train split, create CSV, and preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0464c215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as drug_use_data.csv\n",
      "Original distribution: Counter({0: 17510, 1: 102, 2: 25})\n",
      "Balanced distribution: Counter({0: 18379, 1: 17512, 2: 16639})\n",
      "Training samples: 42024, Test samples: 10506\n",
      "\n",
      "Final dataset info:\n",
      "Substance classes: ['none', 'opioid', 'stimulant']\n",
      "Symptom classes: ['adverse_event', 'anxiety', 'confusion', 'constipation', 'dizziness', 'drowsiness', 'dyspnea', 'fatigue', 'headache', 'hematoma', 'nausea', 'none', 'overdose', 'pain', 'pruritus', 'rash', 'seizure', 'vomiting']\n",
      "Total features: text + 3 substance classes + 18 symptom classes\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Define splits\n",
    "splits = {'train': 'train.jsonl', 'test': 'test.jsonl'}\n",
    "\n",
    "# Load via hf:// protocol\n",
    "try:\n",
    "    df = pd.read_json(\"hf://datasets/SetFit/ade_corpus_v2_classification/\" + splits[\"train\"], lines=True)\n",
    "except Exception as e:\n",
    "    print(f\"hf:// loading failed: {e}\")\n",
    "    print(\"Falling back to direct URL...\")\n",
    "    url = \"https://huggingface.co/datasets/SetFit/ade_corpus_v2_classification/resolve/main/train.jsonl\"\n",
    "    urllib.request.urlretrieve(url, \"train.jsonl\")\n",
    "    df = pd.read_json(\"train.jsonl\", lines=True)\n",
    "\n",
    "# Expanded substance and symptom lists\n",
    "substance_map = {\n",
    "    'morphine': 'opioid', 'oxycodone': 'opioid', 'fentanyl': 'opioid', 'hydrocodone': 'opioid',\n",
    "    'heroin': 'opioid', 'codeine': 'opioid', 'tramadol': 'opioid',\n",
    "    'cocaine': 'stimulant', 'methamphetamine': 'stimulant', 'amphetamine': 'stimulant',\n",
    "    'placebo': 'none', 'heparin': 'none'\n",
    "}\n",
    "symptom_list = ['nausea', 'confusion', 'drowsiness', 'overdose', 'dizziness', 'vomiting',\n",
    "                'fatigue', 'headache', 'anxiety', 'seizure', 'hematoma', 'rash', 'pain',\n",
    "                'constipation', 'dyspnea', 'pruritus']\n",
    "\n",
    "def assign_labels(text, original_label=None):\n",
    "    substance = 'none'\n",
    "    symptoms = []\n",
    "    text_lower = str(text).lower()\n",
    "    \n",
    "    # Check for substances\n",
    "    for drug, subst in substance_map.items():\n",
    "        if drug in text_lower:\n",
    "            substance = subst\n",
    "            break\n",
    "    \n",
    "    # Check for symptoms\n",
    "    for symp in symptom_list:\n",
    "        if symp in text_lower:\n",
    "            symptoms.append(symp)\n",
    "    \n",
    "    # Use original ADE label if available and no symptoms found\n",
    "    if original_label == 1 and not symptoms:\n",
    "        symptoms = ['adverse_event']\n",
    "    \n",
    "    return substance, symptoms if symptoms else ['none']\n",
    "\n",
    "# Apply labels with original label information\n",
    "if 'label' in df.columns:\n",
    "    df['substance_label'], df['symptom_labels'] = zip(*[\n",
    "        assign_labels(text, label) for text, label in zip(df['text'], df['label'])\n",
    "    ])\n",
    "else:\n",
    "    df['substance_label'], df['symptom_labels'] = zip(*df['text'].apply(lambda x: assign_labels(x)))\n",
    "\n",
    "# Save to CSV BEFORE any processing that might duplicate data\n",
    "df[['text', 'substance_label', 'symptom_labels']].to_csv('drug_use_data.csv', index=False)\n",
    "print('Dataset saved as drug_use_data.csv')\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Encode labels\n",
    "substance_classes = df['substance_label'].unique()\n",
    "substance2id = {label: idx for idx, label in enumerate(substance_classes)}\n",
    "df['substance_label'] = df['substance_label'].map(substance2id)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "symptom_encoded = mlb.fit_transform(df['symptom_labels'])\n",
    "symptom_df = pd.DataFrame(symptom_encoded, columns=mlb.classes_)\n",
    "symptom_columns = mlb.classes_\n",
    "\n",
    "# Combine dataframes\n",
    "df = pd.concat([df[['text', 'substance_label']], symptom_df], axis=1)\n",
    "\n",
    "# Apply SMOTE for balanced training data\n",
    "print(\"Original distribution:\", Counter(df['substance_label']))\n",
    "\n",
    "# Use TF-IDF features for SMOTE\n",
    "temp_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_tfidf_temp = temp_vectorizer.fit_transform(df['text']).toarray()\n",
    "\n",
    "try:\n",
    "    smote = SMOTE(random_state=42, k_neighbors=min(3, Counter(df['substance_label']).most_common()[-1][1] - 1))\n",
    "    X_balanced, y_balanced = smote.fit_resample(X_tfidf_temp, df['substance_label'])\n",
    "    \n",
    "    # Create balanced dataframe by finding closest matches\n",
    "    balanced_indices = []\n",
    "    for x_sample in X_balanced:\n",
    "        similarities = np.dot(X_tfidf_temp, x_sample)\n",
    "        closest_idx = np.argmax(similarities)\n",
    "        balanced_indices.append(closest_idx)\n",
    "    \n",
    "    df_balanced = df.iloc[balanced_indices].copy()\n",
    "    df_balanced['substance_label'] = y_balanced\n",
    "    df = df_balanced\n",
    "    \n",
    "    print(\"Balanced distribution:\", Counter(df['substance_label']))\n",
    "except ValueError as e:\n",
    "    print(f\"SMOTE failed: {e}, using original data with manual balancing\")\n",
    "    # Fallback: simple oversampling for minority classes\n",
    "    minority_threshold = len(df) * 0.1  # 10% threshold\n",
    "    minority_data = []\n",
    "    for label in df['substance_label'].unique():\n",
    "        label_data = df[df['substance_label'] == label]\n",
    "        if len(label_data) < minority_threshold:\n",
    "            # Duplicate minority class samples\n",
    "            multiplier = int(minority_threshold / len(label_data)) + 1\n",
    "            minority_data.append(pd.concat([label_data] * multiplier, ignore_index=True))\n",
    "    \n",
    "    if minority_data:\n",
    "        df = pd.concat([df] + minority_data, ignore_index=True)\n",
    "        print(\"Manual balancing applied\")\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['substance_label'])\n",
    "\n",
    "print(f'Training samples: {len(train_df)}, Test samples: {len(test_df)}')\n",
    "\n",
    "# Print final data info\n",
    "print(\"\\nFinal dataset info:\")\n",
    "print(f\"Substance classes: {list(substance2id.keys())}\")\n",
    "print(f\"Symptom classes: {list(symptom_columns)}\")\n",
    "print(f\"Total features: text + {len(substance2id)} substance classes + {len(symptom_columns)} symptom classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab3f6f0",
   "metadata": {},
   "source": [
    "3. Create TF-IDF Features and Datasets\n",
    "\n",
    "Use TF-IDF features and create custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "81dee1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF features...\n",
      "TF-IDF sparse matrix shape: (42024, 2000)\n",
      "Memory usage (sparse): ~3.4 MB\n",
      "Vocabulary size: 2000\n",
      "Converting sparse matrices to dense (this may take a moment)...\n",
      "Processed 0/42024 samples...\n",
      "Processed 5000/42024 samples...\n",
      "Processed 10000/42024 samples...\n",
      "Processed 15000/42024 samples...\n",
      "Processed 20000/42024 samples...\n",
      "Processed 25000/42024 samples...\n",
      "Processed 30000/42024 samples...\n",
      "Processed 35000/42024 samples...\n",
      "Processed 40000/42024 samples...\n",
      "Processed 0/10506 samples...\n",
      "Processed 5000/10506 samples...\n",
      "Processed 10000/10506 samples...\n",
      "Conversion completed successfully!\n",
      "Final TF-IDF feature shape: (42024, 2000)\n",
      "Train symptom data shape: (42024, 18)\n",
      "Test symptom data shape: (10506, 18)\n",
      "Creating training dataset...\n",
      "Dataset created with 42024 samples\n",
      "Feature shape: (42024, 2000)\n",
      "Memory usage: ~320.6 MB\n",
      "Creating test dataset...\n",
      "Dataset created with 10506 samples\n",
      "Feature shape: (10506, 2000)\n",
      "Memory usage: ~80.2 MB\n",
      "Datasets created successfully!\n",
      "Sample data shapes - Features: torch.Size([2000]), Substance: torch.Size([]), Symptoms: torch.Size([18])\n",
      "Memory cleanup completed!\n",
      "TF-IDF processing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# Create TF-IDF features with reduced memory footprint\n",
    "print(\"Creating TF-IDF features...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=2000,  # Reduced from 5000 to save memory\n",
    "    stop_words='english', \n",
    "    ngram_range=(1, 2),  # Reduced from (1,3) to save memory\n",
    "    dtype=np.float32,\n",
    "    min_df=3,  # Increased to reduce vocabulary size\n",
    "    max_df=0.90  # More aggressive filtering\n",
    ")\n",
    "\n",
    "# Keep matrices in sparse format - DON'T convert to dense arrays\n",
    "X_train_tfidf_sparse = vectorizer.fit_transform(train_df['text'])\n",
    "X_test_tfidf_sparse = vectorizer.transform(test_df['text'])\n",
    "\n",
    "print(f\"TF-IDF sparse matrix shape: {X_train_tfidf_sparse.shape}\")\n",
    "print(f\"Memory usage (sparse): ~{X_train_tfidf_sparse.data.nbytes / 1024**2:.1f} MB\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "\n",
    "# Convert sparse matrices to dense in smaller batches to avoid memory issues\n",
    "def sparse_to_dense_batched(sparse_matrix, batch_size=1000):\n",
    "    \"\"\"Convert sparse matrix to dense in batches to manage memory\"\"\"\n",
    "    n_samples = sparse_matrix.shape[0]\n",
    "    n_features = sparse_matrix.shape[1]\n",
    "    \n",
    "    # Pre-allocate dense array\n",
    "    dense_array = np.zeros((n_samples, n_features), dtype=np.float32)\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        end_idx = min(i + batch_size, n_samples)\n",
    "        batch_sparse = sparse_matrix[i:end_idx]\n",
    "        dense_array[i:end_idx] = batch_sparse.toarray()\n",
    "        \n",
    "        if i % (batch_size * 10) == 0:  # Progress update every 10 batches\n",
    "            print(f\"Processed {i}/{n_samples} samples...\")\n",
    "    \n",
    "    return dense_array\n",
    "\n",
    "print(\"Converting sparse matrices to dense (this may take a moment)...\")\n",
    "try:\n",
    "    X_train_tfidf = sparse_to_dense_batched(X_train_tfidf_sparse, batch_size=500)\n",
    "    X_test_tfidf = sparse_to_dense_batched(X_test_tfidf_sparse, batch_size=500)\n",
    "    print(\"Conversion completed successfully!\")\n",
    "except MemoryError:\n",
    "    print(\"Still not enough memory. Using even smaller batch size...\")\n",
    "    try:\n",
    "        X_train_tfidf = sparse_to_dense_batched(X_train_tfidf_sparse, batch_size=100)\n",
    "        X_test_tfidf = sparse_to_dense_batched(X_test_tfidf_sparse, batch_size=100)\n",
    "        print(\"Conversion completed with smaller batches!\")\n",
    "    except MemoryError:\n",
    "        print(\"Memory still insufficient. Switching to sparse-compatible approach...\")\n",
    "        # Alternative: Work directly with sparse matrices (requires model modification)\n",
    "        raise MemoryError(\"Consider using a machine with more RAM or further reducing max_features\")\n",
    "\n",
    "print(f\"Final TF-IDF feature shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "# Memory-efficient dataset class\n",
    "class TFIDFDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, substance_labels, symptom_labels):\n",
    "        # Store as numpy arrays to save memory compared to tensors\n",
    "        self.features = features.astype(np.float32)\n",
    "        self.substance_labels = substance_labels.astype(np.int64)\n",
    "        self.symptom_labels = symptom_labels.astype(np.float32)\n",
    "        \n",
    "        print(f\"Dataset created with {len(self.features)} samples\")\n",
    "        print(f\"Feature shape: {self.features.shape}\")\n",
    "        print(f\"Memory usage: ~{self.features.nbytes / 1024**2:.1f} MB\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert to tensors only when needed (lazy loading)\n",
    "        return {\n",
    "            'x': torch.from_numpy(self.features[idx]).float(),\n",
    "            'substance_labels': torch.from_numpy(np.array(self.substance_labels[idx])).long(),\n",
    "            'symptom_labels': torch.from_numpy(self.symptom_labels[idx]).float()\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "# Ensure symptom columns exist in both dataframes\n",
    "missing_train_cols = [col for col in symptom_columns if col not in train_df.columns]\n",
    "missing_test_cols = [col for col in symptom_columns if col not in test_df.columns]\n",
    "\n",
    "if missing_train_cols:\n",
    "    print(f\"Adding missing columns to train_df: {missing_train_cols}\")\n",
    "    for col in missing_train_cols:\n",
    "        train_df[col] = 0\n",
    "\n",
    "if missing_test_cols:\n",
    "    print(f\"Adding missing columns to test_df: {missing_test_cols}\")\n",
    "    for col in missing_test_cols:\n",
    "        test_df[col] = 0\n",
    "\n",
    "# Get symptom data\n",
    "train_symptom_data = train_df[symptom_columns].values\n",
    "test_symptom_data = test_df[symptom_columns].values\n",
    "\n",
    "print(f\"Train symptom data shape: {train_symptom_data.shape}\")\n",
    "print(f\"Test symptom data shape: {test_symptom_data.shape}\")\n",
    "\n",
    "# Create datasets with memory management\n",
    "import gc\n",
    "\n",
    "# Clear any unnecessary variables\n",
    "if 'X_train_tfidf_sparse' in locals():\n",
    "    del X_train_tfidf_sparse\n",
    "if 'X_test_tfidf_sparse' in locals():\n",
    "    del X_test_tfidf_sparse\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    print(\"Creating training dataset...\")\n",
    "    train_dataset = TFIDFDataset(\n",
    "        X_train_tfidf,\n",
    "        train_df['substance_label'].values,\n",
    "        train_symptom_data\n",
    "    )\n",
    "    \n",
    "    print(\"Creating test dataset...\")\n",
    "    test_dataset = TFIDFDataset(\n",
    "        X_test_tfidf,\n",
    "        test_df['substance_label'].values,\n",
    "        test_symptom_data\n",
    "    )\n",
    "    \n",
    "    print(\"Datasets created successfully!\")\n",
    "    \n",
    "    # Verify dataset integrity\n",
    "    sample = train_dataset[0]\n",
    "    print(f\"Sample data shapes - Features: {sample['x'].shape}, \"\n",
    "          f\"Substance: {sample['substance_labels'].shape}, \"\n",
    "          f\"Symptoms: {sample['symptom_labels'].shape}\")\n",
    "    \n",
    "    # Clean up large arrays to free memory\n",
    "    del X_train_tfidf, X_test_tfidf\n",
    "    gc.collect()\n",
    "    print(\"Memory cleanup completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating datasets: {e}\")\n",
    "    print(\"Debugging information:\")\n",
    "    print(f\"Available memory info:\")\n",
    "    import psutil\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Total RAM: {memory.total / 1024**3:.1f} GB\")\n",
    "    print(f\"Available RAM: {memory.available / 1024**3:.1f} GB\")\n",
    "    print(f\"Used RAM: {memory.percent}%\")\n",
    "    raise\n",
    "\n",
    "print(\"TF-IDF processing completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd25d65",
   "metadata": {},
   "source": [
    "4. Define Custom Model\n",
    "\n",
    "BioBERT for multi-task classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "856710d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining input size...\n",
      "Input size from vectorizer vocabulary: 2000\n",
      "Final input size: 2000\n",
      "Substance classes: 3\n",
      "Symptom labels: 18\n",
      "\n",
      "Model created successfully!\n",
      "Total parameters: 1,501,685\n",
      "Trainable parameters: 1,501,685\n",
      "Model device: cpu\n",
      "\n",
      "Model Architecture:\n",
      "Input size: 2000\n",
      "Substance classes: 3\n",
      "Symptom labels: 18\n",
      "Hidden layers: 512 -> 256 -> 128\n",
      "Features: Batch normalization, residual connections, attention mechanisms, focal loss\n",
      "\n",
      "Testing model with sample batch...\n",
      "✓ Model test successful!\n",
      "  Loss: 0.2391\n",
      "  Substance logits shape: torch.Size([2, 3])\n",
      "  Symptom logits shape: torch.Size([2, 18])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EnhancedMultiTaskModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_substance_classes, num_symptom_labels):\n",
    "        super(EnhancedMultiTaskModel, self).__init__()\n",
    "        \n",
    "        # Input normalization\n",
    "        self.input_norm = torch.nn.BatchNorm1d(input_size)\n",
    "        \n",
    "        # Enhanced architecture with residual connections\n",
    "        self.hidden1 = torch.nn.Linear(input_size, 512)\n",
    "        self.norm1 = torch.nn.BatchNorm1d(512)\n",
    "        self.hidden2 = torch.nn.Linear(512, 256)\n",
    "        self.norm2 = torch.nn.BatchNorm1d(256)\n",
    "        self.hidden3 = torch.nn.Linear(256, 128)\n",
    "        self.norm3 = torch.nn.BatchNorm1d(128)\n",
    "        \n",
    "        # Residual connection layer\n",
    "        self.residual = torch.nn.Linear(input_size, 128)\n",
    "        \n",
    "        # Dropout with different rates\n",
    "        self.dropout1 = torch.nn.Dropout(0.2)\n",
    "        self.dropout2 = torch.nn.Dropout(0.3)\n",
    "        self.dropout3 = torch.nn.Dropout(0.2)\n",
    "        \n",
    "        # Task-specific layers with attention\n",
    "        self.substance_attention = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 128),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.symptom_attention = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 128),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.substance_classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(64, num_substance_classes)\n",
    "        )\n",
    "        \n",
    "        self.symptom_classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(64, num_symptom_labels)\n",
    "        )\n",
    "        \n",
    "        self.num_substance_classes = num_substance_classes\n",
    "        self.num_symptom_labels = num_symptom_labels\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, torch.nn.BatchNorm1d):\n",
    "                torch.nn.init.constant_(m.weight, 1)\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x, substance_labels=None, symptom_labels=None):\n",
    "        # Input normalization\n",
    "        x_norm = self.input_norm(x)\n",
    "        \n",
    "        # Forward pass through hidden layers\n",
    "        hidden = torch.relu(self.hidden1(x_norm))\n",
    "        hidden = self.norm1(hidden)\n",
    "        hidden = self.dropout1(hidden)\n",
    "        \n",
    "        hidden = torch.relu(self.hidden2(hidden))\n",
    "        hidden = self.norm2(hidden)\n",
    "        hidden = self.dropout2(hidden)\n",
    "        \n",
    "        hidden = torch.relu(self.hidden3(hidden))\n",
    "        hidden = self.norm3(hidden)\n",
    "        \n",
    "        # Residual connection\n",
    "        residual = torch.relu(self.residual(x_norm))\n",
    "        hidden = hidden + residual  # Add residual connection\n",
    "        hidden = self.dropout3(hidden)\n",
    "        \n",
    "        # Task-specific attention\n",
    "        substance_att = self.substance_attention(hidden)\n",
    "        symptom_att = self.symptom_attention(hidden)\n",
    "        \n",
    "        # Apply attention\n",
    "        substance_features = hidden * substance_att\n",
    "        symptom_features = hidden * symptom_att\n",
    "        \n",
    "        # Generate logits\n",
    "        substance_logits = self.substance_classifier(substance_features)\n",
    "        symptom_logits = self.symptom_classifier(symptom_features)\n",
    "        \n",
    "        loss = None\n",
    "        if substance_labels is not None and symptom_labels is not None:\n",
    "            # Improved loss calculation\n",
    "            \n",
    "            # Focal loss for substance classification (better for imbalanced classes)\n",
    "            alpha = 0.25\n",
    "            gamma = 2.0\n",
    "            \n",
    "            # Standard cross entropy\n",
    "            ce_loss = torch.nn.functional.cross_entropy(substance_logits, substance_labels, reduction='none')\n",
    "            pt = torch.exp(-ce_loss)\n",
    "            focal_loss = alpha * (1 - pt) ** gamma * ce_loss\n",
    "            substance_loss = focal_loss.mean()\n",
    "            \n",
    "            # Class-balanced BCE loss for symptoms\n",
    "            pos_counts = substance_labels.bincount(minlength=self.num_substance_classes).float()\n",
    "            total_count = len(substance_labels)\n",
    "            pos_weights = total_count / (2.0 * pos_counts + 1e-6)\n",
    "            \n",
    "            # For symptoms, use adaptive positive weights\n",
    "            symptom_pos_counts = symptom_labels.sum(dim=0) + 1e-6\n",
    "            symptom_neg_counts = (1 - symptom_labels).sum(dim=0) + 1e-6\n",
    "            symptom_pos_weights = symptom_neg_counts / symptom_pos_counts\n",
    "            symptom_pos_weights = torch.clamp(symptom_pos_weights, min=0.1, max=10.0)\n",
    "            \n",
    "            symptom_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "                symptom_logits, \n",
    "                symptom_labels, \n",
    "                pos_weight=symptom_pos_weights\n",
    "            )\n",
    "            \n",
    "            # Combine losses with adaptive weighting\n",
    "            substance_weight = 0.7  # Higher weight for substance classification\n",
    "            symptom_weight = 0.3\n",
    "            \n",
    "            loss = substance_weight * substance_loss + symptom_weight * symptom_loss\n",
    "        \n",
    "        return {\n",
    "            'loss': loss, \n",
    "            'substance_logits': substance_logits, \n",
    "            'symptom_logits': symptom_logits,\n",
    "            'substance_probs': torch.softmax(substance_logits, dim=-1),\n",
    "            'symptom_probs': torch.sigmoid(symptom_logits)\n",
    "        }\n",
    "\n",
    "# Get input size from the vectorizer or dataset (multiple methods)\n",
    "print(\"Determining input size...\")\n",
    "\n",
    "# Method 1: From vectorizer (most reliable)\n",
    "if 'vectorizer' in locals() and hasattr(vectorizer, 'vocabulary_'):\n",
    "    actual_input_size = len(vectorizer.vocabulary_)\n",
    "    print(f\"Input size from vectorizer vocabulary: {actual_input_size}\")\n",
    "elif 'vectorizer' in locals() and hasattr(vectorizer, 'max_features'):\n",
    "    actual_input_size = vectorizer.max_features\n",
    "    print(f\"Input size from vectorizer max_features: {actual_input_size}\")\n",
    "# Method 2: From dataset\n",
    "elif 'train_dataset' in locals():\n",
    "    sample = train_dataset[0]\n",
    "    actual_input_size = sample['x'].shape[0]\n",
    "    print(f\"Input size from dataset sample: {actual_input_size}\")\n",
    "# Method 3: Check what we set in vectorizer creation\n",
    "else:\n",
    "    # Fallback to the value we used in vectorizer creation\n",
    "    actual_input_size = 2000  # This was the max_features we set\n",
    "    print(f\"Using fallback input size: {actual_input_size}\")\n",
    "    print(\"Warning: Using fallback size. Ensure this matches your vectorizer configuration.\")\n",
    "\n",
    "# Verify the input size is correct\n",
    "if 'train_dataset' in locals():\n",
    "    sample = train_dataset[0]\n",
    "    sample_input_size = sample['x'].shape[0]\n",
    "    if sample_input_size != actual_input_size:\n",
    "        print(f\"WARNING: Mismatch detected!\")\n",
    "        print(f\"Calculated input size: {actual_input_size}\")\n",
    "        print(f\"Actual dataset input size: {sample_input_size}\")\n",
    "        actual_input_size = sample_input_size\n",
    "        print(f\"Using dataset input size: {actual_input_size}\")\n",
    "\n",
    "print(f\"Final input size: {actual_input_size}\")\n",
    "print(f\"Substance classes: {len(substance_classes)}\")\n",
    "print(f\"Symptom labels: {len(symptom_columns)}\")\n",
    "\n",
    "# Create the model\n",
    "model = EnhancedMultiTaskModel(\n",
    "    input_size=actual_input_size,\n",
    "    num_substance_classes=len(substance_classes),\n",
    "    num_symptom_labels=len(symptom_columns)\n",
    ")\n",
    "\n",
    "# Set device (make sure device is defined)\n",
    "if 'device' not in locals():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device set to: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel created successfully!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(f\"Input size: {actual_input_size}\")\n",
    "print(f\"Substance classes: {len(substance_classes)}\")\n",
    "print(f\"Symptom labels: {len(symptom_columns)}\")\n",
    "print(f\"Hidden layers: 512 -> 256 -> 128\")\n",
    "print(\"Features: Batch normalization, residual connections, attention mechanisms, focal loss\")\n",
    "\n",
    "# Test model with a sample batch to ensure everything works\n",
    "if 'train_dataset' in locals():\n",
    "    print(\"\\nTesting model with sample batch...\")\n",
    "    try:\n",
    "        sample_batch = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=False)\n",
    "        batch = next(iter(sample_batch))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = batch['x'].to(device)\n",
    "            substance_labels = batch['substance_labels'].to(device)\n",
    "            symptom_labels = batch['symptom_labels'].to(device)\n",
    "            \n",
    "            outputs = model(x, substance_labels=substance_labels, symptom_labels=symptom_labels)\n",
    "            \n",
    "            print(f\"✓ Model test successful!\")\n",
    "            print(f\"  Loss: {outputs['loss'].item():.4f}\")\n",
    "            print(f\"  Substance logits shape: {outputs['substance_logits'].shape}\")\n",
    "            print(f\"  Symptom logits shape: {outputs['symptom_logits'].shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Model test failed: {e}\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"Warning: train_dataset not available for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe4d1a",
   "metadata": {},
   "source": [
    "5. Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36396a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "Starting training...\n",
      "Total epochs: 100\n",
      "Batch size: 64\n",
      "Initial learning rate: 0.0001\n",
      "Total training batches: 657\n",
      "Total validation batches: 165\n",
      "Epoch 1/100, Batch 0/657, Loss: 0.3378\n",
      "Epoch 1/100, Batch 50/657, Loss: 0.3474\n",
      "Epoch 1/100, Batch 100/657, Loss: 0.3365\n",
      "Epoch 1/100, Batch 150/657, Loss: 0.3037\n",
      "Epoch 1/100, Batch 200/657, Loss: 0.2906\n",
      "Epoch 1/100, Batch 250/657, Loss: 0.2694\n",
      "Epoch 1/100, Batch 300/657, Loss: 0.2812\n",
      "Epoch 1/100, Batch 350/657, Loss: 0.2826\n",
      "Epoch 1/100, Batch 400/657, Loss: 0.2549\n",
      "Epoch 1/100, Batch 450/657, Loss: 0.2477\n",
      "Epoch 1/100, Batch 500/657, Loss: 0.2496\n",
      "Epoch 1/100, Batch 550/657, Loss: 0.2285\n",
      "Epoch 1/100, Batch 600/657, Loss: 0.2308\n",
      "Epoch 1/100, Batch 650/657, Loss: 0.2183\n",
      "New best model saved! Symptom F1: 0.1832, Substance Accuracy: 0.7278\n",
      "\n",
      "Epoch 1/100 Results:\n",
      "Train Loss: 0.2718, Val Loss: 0.2913\n",
      "Substance Accuracy: 0.7278\n",
      "Symptom F1: 0.1832, Precision: 0.1101, Recall: 0.5467\n",
      "Learning Rate: 0.000013\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 2/100, Batch 0/657, Loss: 0.2247\n",
      "Epoch 2/100, Batch 50/657, Loss: 0.2006\n",
      "Epoch 2/100, Batch 100/657, Loss: 0.1987\n",
      "Epoch 2/100, Batch 150/657, Loss: 0.1901\n",
      "Epoch 2/100, Batch 200/657, Loss: 0.1928\n",
      "Epoch 2/100, Batch 250/657, Loss: 0.1765\n",
      "Epoch 2/100, Batch 300/657, Loss: 0.1637\n",
      "Epoch 2/100, Batch 350/657, Loss: 0.1716\n",
      "Epoch 2/100, Batch 400/657, Loss: 0.1524\n",
      "Epoch 2/100, Batch 450/657, Loss: 0.1453\n",
      "Epoch 2/100, Batch 500/657, Loss: 0.1439\n",
      "Epoch 2/100, Batch 550/657, Loss: 0.1350\n",
      "Epoch 2/100, Batch 600/657, Loss: 0.1341\n",
      "Epoch 2/100, Batch 650/657, Loss: 0.1274\n",
      "New best model saved! Symptom F1: 0.5049, Substance Accuracy: 0.7858\n",
      "\n",
      "Epoch 2/100 Results:\n",
      "Train Loss: 0.1685, Val Loss: 0.1846\n",
      "Substance Accuracy: 0.7858\n",
      "Symptom F1: 0.5049, Precision: 0.3999, Recall: 0.6845\n",
      "Learning Rate: 0.000020\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 3/100, Batch 0/657, Loss: 0.1279\n",
      "Epoch 3/100, Batch 50/657, Loss: 0.1253\n",
      "Epoch 3/100, Batch 100/657, Loss: 0.1170\n",
      "Epoch 3/100, Batch 150/657, Loss: 0.0999\n",
      "Epoch 3/100, Batch 200/657, Loss: 0.0910\n",
      "Epoch 3/100, Batch 250/657, Loss: 0.0926\n",
      "Epoch 3/100, Batch 300/657, Loss: 0.0795\n",
      "Epoch 3/100, Batch 350/657, Loss: 0.0676\n",
      "Epoch 3/100, Batch 400/657, Loss: 0.0973\n",
      "Epoch 3/100, Batch 450/657, Loss: 0.0893\n",
      "Epoch 3/100, Batch 500/657, Loss: 0.0617\n",
      "Epoch 3/100, Batch 550/657, Loss: 0.0645\n",
      "Epoch 3/100, Batch 600/657, Loss: 0.0647\n",
      "Epoch 3/100, Batch 650/657, Loss: 0.0365\n",
      "New best model saved! Symptom F1: 0.8247, Substance Accuracy: 0.8276\n",
      "\n",
      "Epoch 3/100 Results:\n",
      "Train Loss: 0.0825, Val Loss: 0.1227\n",
      "Substance Accuracy: 0.8276\n",
      "Symptom F1: 0.8247, Precision: 0.8513, Recall: 0.7997\n",
      "Learning Rate: 0.000027\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 4/100, Batch 0/657, Loss: 0.0610\n",
      "Epoch 4/100, Batch 50/657, Loss: 0.0611\n",
      "Epoch 4/100, Batch 100/657, Loss: 0.0448\n",
      "Epoch 4/100, Batch 150/657, Loss: 0.0336\n",
      "Epoch 4/100, Batch 200/657, Loss: 0.0481\n",
      "Epoch 4/100, Batch 250/657, Loss: 0.0402\n",
      "Epoch 4/100, Batch 300/657, Loss: 0.0444\n",
      "Epoch 4/100, Batch 350/657, Loss: 0.0476\n",
      "Epoch 4/100, Batch 400/657, Loss: 0.0234\n",
      "Epoch 4/100, Batch 450/657, Loss: 0.0218\n",
      "Epoch 4/100, Batch 500/657, Loss: 0.0209\n",
      "Epoch 4/100, Batch 550/657, Loss: 0.0203\n",
      "Epoch 4/100, Batch 600/657, Loss: 0.0290\n",
      "Epoch 4/100, Batch 650/657, Loss: 0.0291\n",
      "New best model saved! Symptom F1: 0.8524, Substance Accuracy: 0.8556\n",
      "\n",
      "Epoch 4/100 Results:\n",
      "Train Loss: 0.0380, Val Loss: 0.1025\n",
      "Substance Accuracy: 0.8556\n",
      "Symptom F1: 0.8524, Precision: 0.8824, Recall: 0.8243\n",
      "Learning Rate: 0.000033\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 5/100, Batch 0/657, Loss: 0.0441\n",
      "Epoch 5/100, Batch 50/657, Loss: 0.0154\n",
      "Epoch 5/100, Batch 100/657, Loss: 0.0225\n",
      "Epoch 5/100, Batch 150/657, Loss: 0.0156\n",
      "Epoch 5/100, Batch 200/657, Loss: 0.0170\n",
      "Epoch 5/100, Batch 250/657, Loss: 0.0157\n",
      "Epoch 5/100, Batch 300/657, Loss: 0.0205\n",
      "Epoch 5/100, Batch 350/657, Loss: 0.0286\n",
      "Epoch 5/100, Batch 400/657, Loss: 0.0189\n",
      "Epoch 5/100, Batch 450/657, Loss: 0.0269\n",
      "Epoch 5/100, Batch 500/657, Loss: 0.0251\n",
      "Epoch 5/100, Batch 550/657, Loss: 0.0246\n",
      "Epoch 5/100, Batch 600/657, Loss: 0.0155\n",
      "Epoch 5/100, Batch 650/657, Loss: 0.0144\n",
      "New best model saved! Symptom F1: 0.8621, Substance Accuracy: 0.8704\n",
      "\n",
      "Epoch 5/100 Results:\n",
      "Train Loss: 0.0259, Val Loss: 0.1035\n",
      "Substance Accuracy: 0.8704\n",
      "Symptom F1: 0.8621, Precision: 0.9097, Recall: 0.8192\n",
      "Learning Rate: 0.000040\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 6/100, Batch 0/657, Loss: 0.0170\n",
      "Epoch 6/100, Batch 50/657, Loss: 0.0162\n",
      "Epoch 6/100, Batch 100/657, Loss: 0.0158\n",
      "Epoch 6/100, Batch 150/657, Loss: 0.0212\n",
      "Epoch 6/100, Batch 200/657, Loss: 0.0127\n",
      "Epoch 6/100, Batch 250/657, Loss: 0.0124\n",
      "Epoch 6/100, Batch 300/657, Loss: 0.0218\n",
      "Epoch 6/100, Batch 350/657, Loss: 0.0183\n",
      "Epoch 6/100, Batch 400/657, Loss: 0.0370\n",
      "Epoch 6/100, Batch 450/657, Loss: 0.0337\n",
      "Epoch 6/100, Batch 500/657, Loss: 0.0195\n",
      "Epoch 6/100, Batch 550/657, Loss: 0.0120\n",
      "Epoch 6/100, Batch 600/657, Loss: 0.0467\n",
      "Epoch 6/100, Batch 650/657, Loss: 0.0126\n",
      "New best model saved! Symptom F1: 0.8682, Substance Accuracy: 0.8929\n",
      "\n",
      "Epoch 6/100 Results:\n",
      "Train Loss: 0.0197, Val Loss: 0.0849\n",
      "Substance Accuracy: 0.8929\n",
      "Symptom F1: 0.8682, Precision: 0.9061, Recall: 0.8333\n",
      "Learning Rate: 0.000047\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 7/100, Batch 0/657, Loss: 0.0256\n",
      "Epoch 7/100, Batch 50/657, Loss: 0.0094\n",
      "Epoch 7/100, Batch 100/657, Loss: 0.0219\n",
      "Epoch 7/100, Batch 150/657, Loss: 0.0142\n",
      "Epoch 7/100, Batch 200/657, Loss: 0.0110\n",
      "Epoch 7/100, Batch 250/657, Loss: 0.0147\n",
      "Epoch 7/100, Batch 300/657, Loss: 0.0078\n",
      "Epoch 7/100, Batch 350/657, Loss: 0.0117\n",
      "Epoch 7/100, Batch 400/657, Loss: 0.0104\n",
      "Epoch 7/100, Batch 450/657, Loss: 0.0178\n",
      "Epoch 7/100, Batch 500/657, Loss: 0.0089\n",
      "Epoch 7/100, Batch 550/657, Loss: 0.0142\n",
      "Epoch 7/100, Batch 600/657, Loss: 0.0149\n",
      "Epoch 7/100, Batch 650/657, Loss: 0.0149\n",
      "New best model saved! Symptom F1: 0.8725, Substance Accuracy: 0.9155\n",
      "\n",
      "Epoch 7/100 Results:\n",
      "Train Loss: 0.0161, Val Loss: 0.0665\n",
      "Substance Accuracy: 0.9155\n",
      "Symptom F1: 0.8725, Precision: 0.9158, Recall: 0.8332\n",
      "Learning Rate: 0.000053\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 8/100, Batch 0/657, Loss: 0.0208\n",
      "Epoch 8/100, Batch 50/657, Loss: 0.0205\n",
      "Epoch 8/100, Batch 100/657, Loss: 0.0089\n",
      "Epoch 8/100, Batch 150/657, Loss: 0.0116\n",
      "Epoch 8/100, Batch 200/657, Loss: 0.0128\n",
      "Epoch 8/100, Batch 250/657, Loss: 0.0199\n",
      "Epoch 8/100, Batch 300/657, Loss: 0.0093\n",
      "Epoch 8/100, Batch 350/657, Loss: 0.0156\n",
      "Epoch 8/100, Batch 400/657, Loss: 0.0117\n",
      "Epoch 8/100, Batch 450/657, Loss: 0.0070\n",
      "Epoch 8/100, Batch 500/657, Loss: 0.0049\n",
      "Epoch 8/100, Batch 550/657, Loss: 0.0072\n",
      "Epoch 8/100, Batch 600/657, Loss: 0.0028\n",
      "Epoch 8/100, Batch 650/657, Loss: 0.0067\n",
      "New best model saved! Symptom F1: 0.8791, Substance Accuracy: 0.9367\n",
      "\n",
      "Epoch 8/100 Results:\n",
      "Train Loss: 0.0136, Val Loss: 0.0489\n",
      "Substance Accuracy: 0.9367\n",
      "Symptom F1: 0.8791, Precision: 0.9142, Recall: 0.8466\n",
      "Learning Rate: 0.000060\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 9/100, Batch 0/657, Loss: 0.0050\n",
      "Epoch 9/100, Batch 50/657, Loss: 0.0155\n",
      "Epoch 9/100, Batch 100/657, Loss: 0.0076\n",
      "Epoch 9/100, Batch 150/657, Loss: 0.0053\n",
      "Epoch 9/100, Batch 200/657, Loss: 0.0126\n",
      "Epoch 9/100, Batch 250/657, Loss: 0.0086\n",
      "Epoch 9/100, Batch 300/657, Loss: 0.0068\n",
      "Epoch 9/100, Batch 350/657, Loss: 0.0056\n",
      "Epoch 9/100, Batch 400/657, Loss: 0.0071\n",
      "Epoch 9/100, Batch 450/657, Loss: 0.0040\n",
      "Epoch 9/100, Batch 500/657, Loss: 0.0058\n",
      "Epoch 9/100, Batch 550/657, Loss: 0.0165\n",
      "Epoch 9/100, Batch 600/657, Loss: 0.0088\n",
      "Epoch 9/100, Batch 650/657, Loss: 0.0082\n",
      "\n",
      "Epoch 9/100 Results:\n",
      "Train Loss: 0.0117, Val Loss: 0.0453\n",
      "Substance Accuracy: 0.9481\n",
      "Symptom F1: 0.8783, Precision: 0.9134, Recall: 0.8459\n",
      "Learning Rate: 0.000067\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 10/100, Batch 0/657, Loss: 0.0082\n",
      "Epoch 10/100, Batch 50/657, Loss: 0.0112\n",
      "Epoch 10/100, Batch 100/657, Loss: 0.0104\n",
      "Epoch 10/100, Batch 150/657, Loss: 0.0172\n",
      "Epoch 10/100, Batch 200/657, Loss: 0.0096\n",
      "Epoch 10/100, Batch 250/657, Loss: 0.0099\n",
      "Epoch 10/100, Batch 300/657, Loss: 0.0091\n",
      "Epoch 10/100, Batch 350/657, Loss: 0.0079\n",
      "Epoch 10/100, Batch 400/657, Loss: 0.0219\n",
      "Epoch 10/100, Batch 450/657, Loss: 0.0146\n",
      "Epoch 10/100, Batch 500/657, Loss: 0.0048\n",
      "Epoch 10/100, Batch 550/657, Loss: 0.0197\n",
      "Epoch 10/100, Batch 600/657, Loss: 0.0065\n",
      "Epoch 10/100, Batch 650/657, Loss: 0.0124\n",
      "New best model saved! Symptom F1: 0.8823, Substance Accuracy: 0.9502\n",
      "\n",
      "Epoch 10/100 Results:\n",
      "Train Loss: 0.0103, Val Loss: 0.0536\n",
      "Substance Accuracy: 0.9502\n",
      "Symptom F1: 0.8823, Precision: 0.9162, Recall: 0.8509\n",
      "Learning Rate: 0.000073\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 11/100, Batch 0/657, Loss: 0.0082\n",
      "Epoch 11/100, Batch 50/657, Loss: 0.0035\n",
      "Epoch 11/100, Batch 100/657, Loss: 0.0303\n",
      "Epoch 11/100, Batch 150/657, Loss: 0.0094\n",
      "Epoch 11/100, Batch 200/657, Loss: 0.0174\n",
      "Epoch 11/100, Batch 250/657, Loss: 0.0033\n",
      "Epoch 11/100, Batch 300/657, Loss: 0.0033\n",
      "Epoch 11/100, Batch 350/657, Loss: 0.0121\n",
      "Epoch 11/100, Batch 400/657, Loss: 0.0103\n",
      "Epoch 11/100, Batch 450/657, Loss: 0.0073\n",
      "Epoch 11/100, Batch 500/657, Loss: 0.0081\n",
      "Epoch 11/100, Batch 550/657, Loss: 0.0066\n",
      "Epoch 11/100, Batch 600/657, Loss: 0.0111\n",
      "Epoch 11/100, Batch 650/657, Loss: 0.0089\n",
      "\n",
      "Epoch 11/100 Results:\n",
      "Train Loss: 0.0091, Val Loss: 0.0489\n",
      "Substance Accuracy: 0.9588\n",
      "Symptom F1: 0.8777, Precision: 0.9094, Recall: 0.8481\n",
      "Learning Rate: 0.000080\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 12/100, Batch 0/657, Loss: 0.0112\n",
      "Epoch 12/100, Batch 50/657, Loss: 0.0196\n",
      "Epoch 12/100, Batch 100/657, Loss: 0.0078\n",
      "Epoch 12/100, Batch 150/657, Loss: 0.0064\n",
      "Epoch 12/100, Batch 200/657, Loss: 0.0101\n",
      "Epoch 12/100, Batch 250/657, Loss: 0.0155\n",
      "Epoch 12/100, Batch 300/657, Loss: 0.0083\n",
      "Epoch 12/100, Batch 350/657, Loss: 0.0079\n",
      "Epoch 12/100, Batch 400/657, Loss: 0.0159\n",
      "Epoch 12/100, Batch 450/657, Loss: 0.0040\n",
      "Epoch 12/100, Batch 500/657, Loss: 0.0059\n",
      "Epoch 12/100, Batch 550/657, Loss: 0.0060\n",
      "Epoch 12/100, Batch 600/657, Loss: 0.0077\n",
      "Epoch 12/100, Batch 650/657, Loss: 0.0069\n",
      "New best model saved! Symptom F1: 0.8880, Substance Accuracy: 0.9632\n",
      "\n",
      "Epoch 12/100 Results:\n",
      "Train Loss: 0.0081, Val Loss: 0.0488\n",
      "Substance Accuracy: 0.9632\n",
      "Symptom F1: 0.8880, Precision: 0.9169, Recall: 0.8608\n",
      "Learning Rate: 0.000087\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 13/100, Batch 0/657, Loss: 0.0164\n",
      "Epoch 13/100, Batch 50/657, Loss: 0.0075\n",
      "Epoch 13/100, Batch 100/657, Loss: 0.0048\n",
      "Epoch 13/100, Batch 150/657, Loss: 0.0024\n",
      "Epoch 13/100, Batch 200/657, Loss: 0.0073\n",
      "Epoch 13/100, Batch 250/657, Loss: 0.0055\n",
      "Epoch 13/100, Batch 300/657, Loss: 0.0061\n",
      "Epoch 13/100, Batch 350/657, Loss: 0.0061\n",
      "Epoch 13/100, Batch 400/657, Loss: 0.0078\n",
      "Epoch 13/100, Batch 450/657, Loss: 0.0067\n",
      "Epoch 13/100, Batch 500/657, Loss: 0.0044\n",
      "Epoch 13/100, Batch 550/657, Loss: 0.0049\n",
      "Epoch 13/100, Batch 600/657, Loss: 0.0065\n",
      "Epoch 13/100, Batch 650/657, Loss: 0.0030\n",
      "\n",
      "Epoch 13/100 Results:\n",
      "Train Loss: 0.0074, Val Loss: 0.0419\n",
      "Substance Accuracy: 0.9687\n",
      "Symptom F1: 0.8842, Precision: 0.9165, Recall: 0.8542\n",
      "Learning Rate: 0.000093\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 14/100, Batch 0/657, Loss: 0.0071\n",
      "Epoch 14/100, Batch 50/657, Loss: 0.0044\n",
      "Epoch 14/100, Batch 100/657, Loss: 0.0093\n",
      "Epoch 14/100, Batch 150/657, Loss: 0.0066\n",
      "Epoch 14/100, Batch 200/657, Loss: 0.0040\n",
      "Epoch 14/100, Batch 250/657, Loss: 0.0052\n",
      "Epoch 14/100, Batch 300/657, Loss: 0.0024\n",
      "Epoch 14/100, Batch 350/657, Loss: 0.0041\n",
      "Epoch 14/100, Batch 400/657, Loss: 0.0049\n",
      "Epoch 14/100, Batch 450/657, Loss: 0.0053\n",
      "Epoch 14/100, Batch 500/657, Loss: 0.0036\n",
      "Epoch 14/100, Batch 550/657, Loss: 0.0037\n",
      "Epoch 14/100, Batch 600/657, Loss: 0.0124\n",
      "Epoch 14/100, Batch 650/657, Loss: 0.0037\n",
      "\n",
      "Epoch 14/100 Results:\n",
      "Train Loss: 0.0067, Val Loss: 0.0356\n",
      "Substance Accuracy: 0.9776\n",
      "Symptom F1: 0.8804, Precision: 0.9146, Recall: 0.8488\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 15/100, Batch 0/657, Loss: 0.0078\n",
      "Epoch 15/100, Batch 50/657, Loss: 0.0058\n",
      "Epoch 15/100, Batch 100/657, Loss: 0.0050\n",
      "Epoch 15/100, Batch 150/657, Loss: 0.0034\n",
      "Epoch 15/100, Batch 200/657, Loss: 0.0041\n",
      "Epoch 15/100, Batch 250/657, Loss: 0.0039\n",
      "Epoch 15/100, Batch 300/657, Loss: 0.0068\n",
      "Epoch 15/100, Batch 350/657, Loss: 0.0060\n",
      "Epoch 15/100, Batch 400/657, Loss: 0.0050\n",
      "Epoch 15/100, Batch 450/657, Loss: 0.0028\n",
      "Epoch 15/100, Batch 500/657, Loss: 0.0038\n",
      "Epoch 15/100, Batch 550/657, Loss: 0.0020\n",
      "Epoch 15/100, Batch 600/657, Loss: 0.0052\n",
      "Epoch 15/100, Batch 650/657, Loss: 0.0078\n",
      "\n",
      "Epoch 15/100 Results:\n",
      "Train Loss: 0.0059, Val Loss: 0.0401\n",
      "Substance Accuracy: 0.9773\n",
      "Symptom F1: 0.8792, Precision: 0.9090, Recall: 0.8512\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 16/100, Batch 0/657, Loss: 0.0043\n",
      "Epoch 16/100, Batch 50/657, Loss: 0.0053\n",
      "Epoch 16/100, Batch 100/657, Loss: 0.0074\n",
      "Epoch 16/100, Batch 150/657, Loss: 0.0082\n",
      "Epoch 16/100, Batch 200/657, Loss: 0.0056\n",
      "Epoch 16/100, Batch 250/657, Loss: 0.0026\n",
      "Epoch 16/100, Batch 300/657, Loss: 0.0057\n",
      "Epoch 16/100, Batch 350/657, Loss: 0.0029\n",
      "Epoch 16/100, Batch 400/657, Loss: 0.0021\n",
      "Epoch 16/100, Batch 450/657, Loss: 0.0051\n",
      "Epoch 16/100, Batch 500/657, Loss: 0.0089\n",
      "Epoch 16/100, Batch 550/657, Loss: 0.0059\n",
      "Epoch 16/100, Batch 600/657, Loss: 0.0063\n",
      "Epoch 16/100, Batch 650/657, Loss: 0.0085\n",
      "\n",
      "Epoch 16/100 Results:\n",
      "Train Loss: 0.0054, Val Loss: 0.0414\n",
      "Substance Accuracy: 0.9770\n",
      "Symptom F1: 0.8834, Precision: 0.9119, Recall: 0.8566\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 17/100, Batch 0/657, Loss: 0.0023\n",
      "Epoch 17/100, Batch 50/657, Loss: 0.0023\n",
      "Epoch 17/100, Batch 100/657, Loss: 0.0073\n",
      "Epoch 17/100, Batch 150/657, Loss: 0.0035\n",
      "Epoch 17/100, Batch 200/657, Loss: 0.0026\n",
      "Epoch 17/100, Batch 250/657, Loss: 0.0028\n",
      "Epoch 17/100, Batch 300/657, Loss: 0.0065\n",
      "Epoch 17/100, Batch 350/657, Loss: 0.0029\n",
      "Epoch 17/100, Batch 400/657, Loss: 0.0051\n",
      "Epoch 17/100, Batch 450/657, Loss: 0.0085\n",
      "Epoch 17/100, Batch 500/657, Loss: 0.0011\n",
      "Epoch 17/100, Batch 550/657, Loss: 0.0031\n",
      "Epoch 17/100, Batch 600/657, Loss: 0.0049\n",
      "Epoch 17/100, Batch 650/657, Loss: 0.0047\n",
      "\n",
      "Epoch 17/100 Results:\n",
      "Train Loss: 0.0049, Val Loss: 0.0350\n",
      "Substance Accuracy: 0.9841\n",
      "Symptom F1: 0.8842, Precision: 0.9106, Recall: 0.8593\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 18/100, Batch 0/657, Loss: 0.0167\n",
      "Epoch 18/100, Batch 50/657, Loss: 0.0089\n",
      "Epoch 18/100, Batch 100/657, Loss: 0.0025\n",
      "Epoch 18/100, Batch 150/657, Loss: 0.0035\n",
      "Epoch 18/100, Batch 200/657, Loss: 0.0032\n",
      "Epoch 18/100, Batch 250/657, Loss: 0.0043\n",
      "Epoch 18/100, Batch 300/657, Loss: 0.0035\n",
      "Epoch 18/100, Batch 350/657, Loss: 0.0063\n",
      "Epoch 18/100, Batch 400/657, Loss: 0.0038\n",
      "Epoch 18/100, Batch 450/657, Loss: 0.0039\n",
      "Epoch 18/100, Batch 500/657, Loss: 0.0019\n",
      "Epoch 18/100, Batch 550/657, Loss: 0.0020\n",
      "Epoch 18/100, Batch 600/657, Loss: 0.0039\n",
      "Epoch 18/100, Batch 650/657, Loss: 0.0060\n",
      "\n",
      "Epoch 18/100 Results:\n",
      "Train Loss: 0.0043, Val Loss: 0.0442\n",
      "Substance Accuracy: 0.9845\n",
      "Symptom F1: 0.8791, Precision: 0.9040, Recall: 0.8556\n",
      "Learning Rate: 0.000100\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 19/100, Batch 0/657, Loss: 0.0076\n",
      "Epoch 19/100, Batch 50/657, Loss: 0.0029\n",
      "Epoch 19/100, Batch 100/657, Loss: 0.0023\n",
      "Epoch 19/100, Batch 150/657, Loss: 0.0023\n",
      "Epoch 19/100, Batch 200/657, Loss: 0.0034\n",
      "Epoch 19/100, Batch 250/657, Loss: 0.0051\n",
      "Epoch 19/100, Batch 300/657, Loss: 0.0075\n",
      "Epoch 19/100, Batch 350/657, Loss: 0.0032\n",
      "Epoch 19/100, Batch 400/657, Loss: 0.0010\n",
      "Epoch 19/100, Batch 450/657, Loss: 0.0081\n",
      "Epoch 19/100, Batch 500/657, Loss: 0.0021\n",
      "Epoch 19/100, Batch 550/657, Loss: 0.0008\n",
      "Epoch 19/100, Batch 600/657, Loss: 0.0032\n",
      "Epoch 19/100, Batch 650/657, Loss: 0.0052\n",
      "\n",
      "Epoch 19/100 Results:\n",
      "Train Loss: 0.0038, Val Loss: 0.0433\n",
      "Substance Accuracy: 0.9832\n",
      "Symptom F1: 0.8788, Precision: 0.9091, Recall: 0.8504\n",
      "Learning Rate: 0.000099\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 20/100, Batch 0/657, Loss: 0.0031\n",
      "Epoch 20/100, Batch 50/657, Loss: 0.0066\n",
      "Epoch 20/100, Batch 100/657, Loss: 0.0026\n",
      "Epoch 20/100, Batch 150/657, Loss: 0.0037\n",
      "Epoch 20/100, Batch 200/657, Loss: 0.0041\n",
      "Epoch 20/100, Batch 250/657, Loss: 0.0027\n",
      "Epoch 20/100, Batch 300/657, Loss: 0.0011\n",
      "Epoch 20/100, Batch 350/657, Loss: 0.0030\n",
      "Epoch 20/100, Batch 400/657, Loss: 0.0020\n",
      "Epoch 20/100, Batch 450/657, Loss: 0.0052\n",
      "Epoch 20/100, Batch 500/657, Loss: 0.0023\n",
      "Epoch 20/100, Batch 550/657, Loss: 0.0021\n",
      "Epoch 20/100, Batch 600/657, Loss: 0.0015\n",
      "Epoch 20/100, Batch 650/657, Loss: 0.0020\n",
      "\n",
      "Epoch 20/100 Results:\n",
      "Train Loss: 0.0035, Val Loss: 0.0452\n",
      "Substance Accuracy: 0.9848\n",
      "Symptom F1: 0.8748, Precision: 0.8987, Recall: 0.8522\n",
      "Learning Rate: 0.000099\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 21/100, Batch 0/657, Loss: 0.0026\n",
      "Epoch 21/100, Batch 50/657, Loss: 0.0008\n",
      "Epoch 21/100, Batch 100/657, Loss: 0.0031\n",
      "Epoch 21/100, Batch 150/657, Loss: 0.0078\n",
      "Epoch 21/100, Batch 200/657, Loss: 0.0078\n",
      "Epoch 21/100, Batch 250/657, Loss: 0.0022\n",
      "Epoch 21/100, Batch 300/657, Loss: 0.0040\n",
      "Epoch 21/100, Batch 350/657, Loss: 0.0009\n",
      "Epoch 21/100, Batch 400/657, Loss: 0.0019\n",
      "Epoch 21/100, Batch 450/657, Loss: 0.0074\n",
      "Epoch 21/100, Batch 500/657, Loss: 0.0031\n",
      "Epoch 21/100, Batch 550/657, Loss: 0.0030\n",
      "Epoch 21/100, Batch 600/657, Loss: 0.0020\n",
      "Epoch 21/100, Batch 650/657, Loss: 0.0008\n",
      "\n",
      "Epoch 21/100 Results:\n",
      "Train Loss: 0.0032, Val Loss: 0.0544\n",
      "Substance Accuracy: 0.9833\n",
      "Symptom F1: 0.8714, Precision: 0.9016, Recall: 0.8432\n",
      "Learning Rate: 0.000099\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 22/100, Batch 0/657, Loss: 0.0012\n",
      "Epoch 22/100, Batch 50/657, Loss: 0.0019\n",
      "Epoch 22/100, Batch 100/657, Loss: 0.0019\n",
      "Epoch 22/100, Batch 150/657, Loss: 0.0002\n",
      "Epoch 22/100, Batch 200/657, Loss: 0.0021\n",
      "Epoch 22/100, Batch 250/657, Loss: 0.0026\n",
      "Epoch 22/100, Batch 300/657, Loss: 0.0019\n",
      "Epoch 22/100, Batch 350/657, Loss: 0.0057\n",
      "Epoch 22/100, Batch 400/657, Loss: 0.0011\n",
      "Epoch 22/100, Batch 450/657, Loss: 0.0019\n",
      "Epoch 22/100, Batch 500/657, Loss: 0.0013\n",
      "Epoch 22/100, Batch 550/657, Loss: 0.0039\n",
      "Epoch 22/100, Batch 600/657, Loss: 0.0033\n",
      "Epoch 22/100, Batch 650/657, Loss: 0.0062\n",
      "\n",
      "Epoch 22/100 Results:\n",
      "Train Loss: 0.0029, Val Loss: 0.0593\n",
      "Substance Accuracy: 0.9785\n",
      "Symptom F1: 0.8753, Precision: 0.9113, Recall: 0.8421\n",
      "Learning Rate: 0.000098\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 23/100, Batch 0/657, Loss: 0.0004\n",
      "Epoch 23/100, Batch 50/657, Loss: 0.0034\n",
      "Epoch 23/100, Batch 100/657, Loss: 0.0031\n",
      "Epoch 23/100, Batch 150/657, Loss: 0.0031\n",
      "Epoch 23/100, Batch 200/657, Loss: 0.0010\n",
      "Epoch 23/100, Batch 250/657, Loss: 0.0028\n",
      "Epoch 23/100, Batch 300/657, Loss: 0.0019\n",
      "Epoch 23/100, Batch 350/657, Loss: 0.0015\n",
      "Epoch 23/100, Batch 400/657, Loss: 0.0012\n",
      "Epoch 23/100, Batch 450/657, Loss: 0.0019\n",
      "Epoch 23/100, Batch 500/657, Loss: 0.0008\n",
      "Epoch 23/100, Batch 550/657, Loss: 0.0032\n",
      "Epoch 23/100, Batch 600/657, Loss: 0.0009\n",
      "Epoch 23/100, Batch 650/657, Loss: 0.0007\n",
      "\n",
      "Epoch 23/100 Results:\n",
      "Train Loss: 0.0026, Val Loss: 0.0667\n",
      "Substance Accuracy: 0.9808\n",
      "Symptom F1: 0.8698, Precision: 0.9021, Recall: 0.8396\n",
      "Learning Rate: 0.000098\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 24/100, Batch 0/657, Loss: 0.0008\n",
      "Epoch 24/100, Batch 50/657, Loss: 0.0009\n",
      "Epoch 24/100, Batch 100/657, Loss: 0.0018\n",
      "Epoch 24/100, Batch 150/657, Loss: 0.0057\n",
      "Epoch 24/100, Batch 200/657, Loss: 0.0029\n",
      "Epoch 24/100, Batch 250/657, Loss: 0.0039\n",
      "Epoch 24/100, Batch 300/657, Loss: 0.0009\n",
      "Epoch 24/100, Batch 350/657, Loss: 0.0007\n",
      "Epoch 24/100, Batch 400/657, Loss: 0.0045\n",
      "Epoch 24/100, Batch 450/657, Loss: 0.0006\n",
      "Epoch 24/100, Batch 500/657, Loss: 0.0014\n",
      "Epoch 24/100, Batch 550/657, Loss: 0.0004\n",
      "Epoch 24/100, Batch 600/657, Loss: 0.0025\n",
      "Epoch 24/100, Batch 650/657, Loss: 0.0010\n",
      "\n",
      "Epoch 24/100 Results:\n",
      "Train Loss: 0.0023, Val Loss: 0.0537\n",
      "Substance Accuracy: 0.9852\n",
      "Symptom F1: 0.8835, Precision: 0.9179, Recall: 0.8516\n",
      "Learning Rate: 0.000097\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 25/100, Batch 0/657, Loss: 0.0042\n",
      "Epoch 25/100, Batch 50/657, Loss: 0.0009\n",
      "Epoch 25/100, Batch 100/657, Loss: 0.0049\n",
      "Epoch 25/100, Batch 150/657, Loss: 0.0010\n",
      "Epoch 25/100, Batch 200/657, Loss: 0.0030\n",
      "Epoch 25/100, Batch 250/657, Loss: 0.0011\n",
      "Epoch 25/100, Batch 300/657, Loss: 0.0034\n",
      "Epoch 25/100, Batch 350/657, Loss: 0.0012\n",
      "Epoch 25/100, Batch 400/657, Loss: 0.0006\n",
      "Epoch 25/100, Batch 450/657, Loss: 0.0025\n",
      "Epoch 25/100, Batch 500/657, Loss: 0.0053\n",
      "Epoch 25/100, Batch 550/657, Loss: 0.0004\n",
      "Epoch 25/100, Batch 600/657, Loss: 0.0007\n",
      "Epoch 25/100, Batch 650/657, Loss: 0.0014\n",
      "\n",
      "Epoch 25/100 Results:\n",
      "Train Loss: 0.0022, Val Loss: 0.0474\n",
      "Substance Accuracy: 0.9865\n",
      "Symptom F1: 0.8842, Precision: 0.9134, Recall: 0.8569\n",
      "Learning Rate: 0.000097\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 26/100, Batch 0/657, Loss: 0.0046\n",
      "Epoch 26/100, Batch 50/657, Loss: 0.0006\n",
      "Epoch 26/100, Batch 100/657, Loss: 0.0012\n",
      "Epoch 26/100, Batch 150/657, Loss: 0.0019\n",
      "Epoch 26/100, Batch 200/657, Loss: 0.0028\n",
      "Epoch 26/100, Batch 250/657, Loss: 0.0014\n",
      "Epoch 26/100, Batch 300/657, Loss: 0.0011\n",
      "Epoch 26/100, Batch 350/657, Loss: 0.0008\n",
      "Epoch 26/100, Batch 400/657, Loss: 0.0024\n",
      "Epoch 26/100, Batch 450/657, Loss: 0.0016\n",
      "Epoch 26/100, Batch 500/657, Loss: 0.0014\n",
      "Epoch 26/100, Batch 550/657, Loss: 0.0040\n",
      "Epoch 26/100, Batch 600/657, Loss: 0.0027\n",
      "Epoch 26/100, Batch 650/657, Loss: 0.0004\n",
      "\n",
      "Epoch 26/100 Results:\n",
      "Train Loss: 0.0020, Val Loss: 0.0488\n",
      "Substance Accuracy: 0.9897\n",
      "Symptom F1: 0.8831, Precision: 0.9087, Recall: 0.8589\n",
      "Learning Rate: 0.000096\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 27/100, Batch 0/657, Loss: 0.0011\n",
      "Epoch 27/100, Batch 50/657, Loss: 0.0002\n",
      "Epoch 27/100, Batch 100/657, Loss: 0.0015\n",
      "Epoch 27/100, Batch 150/657, Loss: 0.0015\n",
      "Epoch 27/100, Batch 200/657, Loss: 0.0021\n",
      "Epoch 27/100, Batch 250/657, Loss: 0.0006\n",
      "Epoch 27/100, Batch 300/657, Loss: 0.0034\n",
      "Epoch 27/100, Batch 350/657, Loss: 0.0036\n",
      "Epoch 27/100, Batch 400/657, Loss: 0.0009\n",
      "Epoch 27/100, Batch 450/657, Loss: 0.0011\n",
      "Epoch 27/100, Batch 500/657, Loss: 0.0015\n",
      "Epoch 27/100, Batch 550/657, Loss: 0.0006\n",
      "Epoch 27/100, Batch 600/657, Loss: 0.0134\n",
      "Epoch 27/100, Batch 650/657, Loss: 0.0006\n",
      "\n",
      "Epoch 27/100 Results:\n",
      "Train Loss: 0.0018, Val Loss: 0.0621\n",
      "Substance Accuracy: 0.9801\n",
      "Symptom F1: 0.8817, Precision: 0.9119, Recall: 0.8535\n",
      "Learning Rate: 0.000095\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 28/100, Batch 0/657, Loss: 0.0012\n",
      "Epoch 28/100, Batch 50/657, Loss: 0.0014\n",
      "Epoch 28/100, Batch 100/657, Loss: 0.0019\n",
      "Epoch 28/100, Batch 150/657, Loss: 0.0024\n",
      "Epoch 28/100, Batch 200/657, Loss: 0.0031\n",
      "Epoch 28/100, Batch 250/657, Loss: 0.0003\n",
      "Epoch 28/100, Batch 300/657, Loss: 0.0002\n",
      "Epoch 28/100, Batch 350/657, Loss: 0.0015\n",
      "Epoch 28/100, Batch 400/657, Loss: 0.0013\n",
      "Epoch 28/100, Batch 450/657, Loss: 0.0018\n",
      "Epoch 28/100, Batch 500/657, Loss: 0.0005\n",
      "Epoch 28/100, Batch 550/657, Loss: 0.0007\n",
      "Epoch 28/100, Batch 600/657, Loss: 0.0007\n",
      "Epoch 28/100, Batch 650/657, Loss: 0.0029\n",
      "\n",
      "Epoch 28/100 Results:\n",
      "Train Loss: 0.0018, Val Loss: 0.0548\n",
      "Substance Accuracy: 0.9912\n",
      "Symptom F1: 0.8778, Precision: 0.9051, Recall: 0.8521\n",
      "Learning Rate: 0.000094\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 29/100, Batch 0/657, Loss: 0.0013\n",
      "Epoch 29/100, Batch 50/657, Loss: 0.0001\n",
      "Epoch 29/100, Batch 100/657, Loss: 0.0004\n",
      "Epoch 29/100, Batch 150/657, Loss: 0.0020\n",
      "Epoch 29/100, Batch 200/657, Loss: 0.0021\n",
      "Epoch 29/100, Batch 250/657, Loss: 0.0012\n",
      "Epoch 29/100, Batch 300/657, Loss: 0.0009\n",
      "Epoch 29/100, Batch 350/657, Loss: 0.0007\n",
      "Epoch 29/100, Batch 400/657, Loss: 0.0045\n",
      "Epoch 29/100, Batch 450/657, Loss: 0.0017\n",
      "Epoch 29/100, Batch 500/657, Loss: 0.0011\n",
      "Epoch 29/100, Batch 550/657, Loss: 0.0023\n",
      "Epoch 29/100, Batch 600/657, Loss: 0.0001\n",
      "Epoch 29/100, Batch 650/657, Loss: 0.0095\n",
      "\n",
      "Epoch 29/100 Results:\n",
      "Train Loss: 0.0017, Val Loss: 0.0650\n",
      "Substance Accuracy: 0.9811\n",
      "Symptom F1: 0.8825, Precision: 0.9130, Recall: 0.8540\n",
      "Learning Rate: 0.000093\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 30/100, Batch 0/657, Loss: 0.0012\n",
      "Epoch 30/100, Batch 50/657, Loss: 0.0025\n",
      "Epoch 30/100, Batch 100/657, Loss: 0.0002\n",
      "Epoch 30/100, Batch 150/657, Loss: 0.0030\n",
      "Epoch 30/100, Batch 200/657, Loss: 0.0042\n",
      "Epoch 30/100, Batch 250/657, Loss: 0.0018\n",
      "Epoch 30/100, Batch 300/657, Loss: 0.0010\n",
      "Epoch 30/100, Batch 350/657, Loss: 0.0004\n",
      "Epoch 30/100, Batch 400/657, Loss: 0.0006\n",
      "Epoch 30/100, Batch 450/657, Loss: 0.0003\n",
      "Epoch 30/100, Batch 500/657, Loss: 0.0020\n",
      "Epoch 30/100, Batch 550/657, Loss: 0.0035\n",
      "Epoch 30/100, Batch 600/657, Loss: 0.0101\n",
      "Epoch 30/100, Batch 650/657, Loss: 0.0022\n",
      "\n",
      "Epoch 30/100 Results:\n",
      "Train Loss: 0.0015, Val Loss: 0.0534\n",
      "Substance Accuracy: 0.9916\n",
      "Symptom F1: 0.8778, Precision: 0.9084, Recall: 0.8492\n",
      "Learning Rate: 0.000093\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 31/100, Batch 0/657, Loss: 0.0010\n",
      "Epoch 31/100, Batch 50/657, Loss: 0.0008\n",
      "Epoch 31/100, Batch 100/657, Loss: 0.0003\n",
      "Epoch 31/100, Batch 150/657, Loss: 0.0017\n",
      "Epoch 31/100, Batch 200/657, Loss: 0.0003\n",
      "Epoch 31/100, Batch 250/657, Loss: 0.0007\n",
      "Epoch 31/100, Batch 300/657, Loss: 0.0004\n",
      "Epoch 31/100, Batch 350/657, Loss: 0.0041\n",
      "Epoch 31/100, Batch 400/657, Loss: 0.0013\n",
      "Epoch 31/100, Batch 450/657, Loss: 0.0024\n",
      "Epoch 31/100, Batch 500/657, Loss: 0.0003\n",
      "Epoch 31/100, Batch 550/657, Loss: 0.0005\n",
      "Epoch 31/100, Batch 600/657, Loss: 0.0004\n",
      "Epoch 31/100, Batch 650/657, Loss: 0.0005\n",
      "\n",
      "Epoch 31/100 Results:\n",
      "Train Loss: 0.0013, Val Loss: 0.0635\n",
      "Substance Accuracy: 0.9882\n",
      "Symptom F1: 0.8844, Precision: 0.9195, Recall: 0.8519\n",
      "Learning Rate: 0.000092\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 32/100, Batch 0/657, Loss: 0.0003\n",
      "Epoch 32/100, Batch 50/657, Loss: 0.0004\n",
      "Epoch 32/100, Batch 100/657, Loss: 0.0009\n",
      "Epoch 32/100, Batch 150/657, Loss: 0.0013\n",
      "Epoch 32/100, Batch 200/657, Loss: 0.0004\n",
      "Epoch 32/100, Batch 250/657, Loss: 0.0007\n",
      "Epoch 32/100, Batch 300/657, Loss: 0.0005\n",
      "Epoch 32/100, Batch 350/657, Loss: 0.0006\n",
      "Epoch 32/100, Batch 400/657, Loss: 0.0025\n",
      "Epoch 32/100, Batch 450/657, Loss: 0.0002\n",
      "Epoch 32/100, Batch 500/657, Loss: 0.0003\n",
      "Epoch 32/100, Batch 550/657, Loss: 0.0003\n",
      "Epoch 32/100, Batch 600/657, Loss: 0.0005\n",
      "Epoch 32/100, Batch 650/657, Loss: 0.0010\n",
      "\n",
      "Epoch 32/100 Results:\n",
      "Train Loss: 0.0013, Val Loss: 0.0590\n",
      "Substance Accuracy: 0.9918\n",
      "Symptom F1: 0.8796, Precision: 0.9122, Recall: 0.8493\n",
      "Learning Rate: 0.000090\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 33/100, Batch 0/657, Loss: 0.0000\n",
      "Epoch 33/100, Batch 50/657, Loss: 0.0013\n",
      "Epoch 33/100, Batch 100/657, Loss: 0.0007\n",
      "Epoch 33/100, Batch 150/657, Loss: 0.0004\n",
      "Epoch 33/100, Batch 200/657, Loss: 0.0006\n",
      "Epoch 33/100, Batch 250/657, Loss: 0.0019\n",
      "Epoch 33/100, Batch 300/657, Loss: 0.0001\n",
      "Epoch 33/100, Batch 350/657, Loss: 0.0015\n",
      "Epoch 33/100, Batch 400/657, Loss: 0.0032\n",
      "Epoch 33/100, Batch 450/657, Loss: 0.0017\n",
      "Epoch 33/100, Batch 500/657, Loss: 0.0211\n",
      "Epoch 33/100, Batch 550/657, Loss: 0.0002\n",
      "Epoch 33/100, Batch 600/657, Loss: 0.0010\n",
      "Epoch 33/100, Batch 650/657, Loss: 0.0014\n",
      "\n",
      "Epoch 33/100 Results:\n",
      "Train Loss: 0.0013, Val Loss: 0.0564\n",
      "Substance Accuracy: 0.9946\n",
      "Symptom F1: 0.8806, Precision: 0.9138, Recall: 0.8497\n",
      "Learning Rate: 0.000089\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 34/100, Batch 0/657, Loss: 0.0017\n",
      "Epoch 34/100, Batch 50/657, Loss: 0.0021\n",
      "Epoch 34/100, Batch 100/657, Loss: 0.0016\n",
      "Epoch 34/100, Batch 150/657, Loss: 0.0007\n",
      "Epoch 34/100, Batch 200/657, Loss: 0.0014\n",
      "Epoch 34/100, Batch 250/657, Loss: 0.0001\n",
      "Epoch 34/100, Batch 300/657, Loss: 0.0005\n",
      "Epoch 34/100, Batch 350/657, Loss: 0.0002\n",
      "Epoch 34/100, Batch 400/657, Loss: 0.0004\n",
      "Epoch 34/100, Batch 450/657, Loss: 0.0005\n",
      "Epoch 34/100, Batch 500/657, Loss: 0.0042\n",
      "Epoch 34/100, Batch 550/657, Loss: 0.0009\n",
      "Epoch 34/100, Batch 600/657, Loss: 0.0024\n",
      "Epoch 34/100, Batch 650/657, Loss: 0.0001\n",
      "\n",
      "Epoch 34/100 Results:\n",
      "Train Loss: 0.0011, Val Loss: 0.0558\n",
      "Substance Accuracy: 0.9934\n",
      "Symptom F1: 0.8796, Precision: 0.9159, Recall: 0.8460\n",
      "Learning Rate: 0.000088\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 35/100, Batch 0/657, Loss: 0.0004\n",
      "Epoch 35/100, Batch 50/657, Loss: 0.0002\n",
      "Epoch 35/100, Batch 100/657, Loss: 0.0007\n",
      "Epoch 35/100, Batch 150/657, Loss: 0.0001\n",
      "Epoch 35/100, Batch 200/657, Loss: 0.0004\n",
      "Epoch 35/100, Batch 250/657, Loss: 0.0009\n",
      "Epoch 35/100, Batch 300/657, Loss: 0.0003\n",
      "Epoch 35/100, Batch 350/657, Loss: 0.0068\n",
      "Epoch 35/100, Batch 400/657, Loss: 0.0011\n",
      "Epoch 35/100, Batch 450/657, Loss: 0.0006\n",
      "Epoch 35/100, Batch 500/657, Loss: 0.0013\n",
      "Epoch 35/100, Batch 550/657, Loss: 0.0001\n",
      "Epoch 35/100, Batch 600/657, Loss: 0.0004\n",
      "Epoch 35/100, Batch 650/657, Loss: 0.0013\n",
      "\n",
      "Epoch 35/100 Results:\n",
      "Train Loss: 0.0012, Val Loss: 0.0543\n",
      "Substance Accuracy: 0.9958\n",
      "Symptom F1: 0.8852, Precision: 0.9151, Recall: 0.8571\n",
      "Learning Rate: 0.000087\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 36/100, Batch 0/657, Loss: 0.0004\n",
      "Epoch 36/100, Batch 50/657, Loss: 0.0002\n",
      "Epoch 36/100, Batch 100/657, Loss: 0.0004\n",
      "Epoch 36/100, Batch 150/657, Loss: 0.0017\n",
      "Epoch 36/100, Batch 200/657, Loss: 0.0007\n",
      "Epoch 36/100, Batch 250/657, Loss: 0.0002\n",
      "Epoch 36/100, Batch 300/657, Loss: 0.0001\n",
      "Epoch 36/100, Batch 350/657, Loss: 0.0006\n",
      "Epoch 36/100, Batch 400/657, Loss: 0.0005\n",
      "Epoch 36/100, Batch 450/657, Loss: 0.0001\n",
      "Epoch 36/100, Batch 500/657, Loss: 0.0010\n",
      "Epoch 36/100, Batch 550/657, Loss: 0.0005\n",
      "Epoch 36/100, Batch 600/657, Loss: 0.0019\n",
      "Epoch 36/100, Batch 650/657, Loss: 0.0007\n",
      "\n",
      "Epoch 36/100 Results:\n",
      "Train Loss: 0.0011, Val Loss: 0.0619\n",
      "Substance Accuracy: 0.9931\n",
      "Symptom F1: 0.8800, Precision: 0.9104, Recall: 0.8515\n",
      "Learning Rate: 0.000086\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 37/100, Batch 0/657, Loss: 0.0012\n",
      "Epoch 37/100, Batch 50/657, Loss: 0.0023\n",
      "Epoch 37/100, Batch 100/657, Loss: 0.0003\n",
      "Epoch 37/100, Batch 150/657, Loss: 0.0001\n",
      "Epoch 37/100, Batch 200/657, Loss: 0.0002\n",
      "Epoch 37/100, Batch 250/657, Loss: 0.0000\n",
      "Epoch 37/100, Batch 300/657, Loss: 0.0003\n",
      "Epoch 37/100, Batch 350/657, Loss: 0.0010\n",
      "Epoch 37/100, Batch 400/657, Loss: 0.0014\n",
      "Epoch 37/100, Batch 450/657, Loss: 0.0001\n",
      "Epoch 37/100, Batch 500/657, Loss: 0.0007\n",
      "Epoch 37/100, Batch 550/657, Loss: 0.0001\n",
      "Epoch 37/100, Batch 600/657, Loss: 0.0000\n",
      "Epoch 37/100, Batch 650/657, Loss: 0.0014\n",
      "New best model saved! Symptom F1: 0.8886, Substance Accuracy: 0.9951\n",
      "\n",
      "Epoch 37/100 Results:\n",
      "Train Loss: 0.0010, Val Loss: 0.0506\n",
      "Substance Accuracy: 0.9951\n",
      "Symptom F1: 0.8886, Precision: 0.9205, Recall: 0.8588\n",
      "Learning Rate: 0.000084\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 38/100, Batch 0/657, Loss: 0.0001\n",
      "Epoch 38/100, Batch 50/657, Loss: 0.0002\n",
      "Epoch 38/100, Batch 100/657, Loss: 0.0008\n",
      "Epoch 38/100, Batch 150/657, Loss: 0.0007\n",
      "Epoch 38/100, Batch 200/657, Loss: 0.0003\n",
      "Epoch 38/100, Batch 250/657, Loss: 0.0001\n",
      "Epoch 38/100, Batch 300/657, Loss: 0.0009\n",
      "Epoch 38/100, Batch 350/657, Loss: 0.0009\n",
      "Epoch 38/100, Batch 400/657, Loss: 0.0045\n",
      "Epoch 38/100, Batch 450/657, Loss: 0.0016\n",
      "Epoch 38/100, Batch 500/657, Loss: 0.0020\n",
      "Epoch 38/100, Batch 550/657, Loss: 0.0171\n",
      "Epoch 38/100, Batch 600/657, Loss: 0.0005\n",
      "Epoch 38/100, Batch 650/657, Loss: 0.0015\n",
      "\n",
      "Epoch 38/100 Results:\n",
      "Train Loss: 0.0010, Val Loss: 0.0579\n",
      "Substance Accuracy: 0.9924\n",
      "Symptom F1: 0.8768, Precision: 0.9130, Recall: 0.8434\n",
      "Learning Rate: 0.000083\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 39/100, Batch 0/657, Loss: 0.0002\n",
      "Epoch 39/100, Batch 50/657, Loss: 0.0016\n",
      "Epoch 39/100, Batch 100/657, Loss: 0.0013\n",
      "Epoch 39/100, Batch 150/657, Loss: 0.0012\n",
      "Epoch 39/100, Batch 200/657, Loss: 0.0008\n",
      "Epoch 39/100, Batch 250/657, Loss: 0.0007\n",
      "Epoch 39/100, Batch 300/657, Loss: 0.0004\n",
      "Epoch 39/100, Batch 350/657, Loss: 0.0003\n",
      "Epoch 39/100, Batch 400/657, Loss: 0.0006\n",
      "Epoch 39/100, Batch 450/657, Loss: 0.0015\n",
      "Epoch 39/100, Batch 500/657, Loss: 0.0009\n",
      "Epoch 39/100, Batch 550/657, Loss: 0.0006\n",
      "Epoch 39/100, Batch 600/657, Loss: 0.0003\n",
      "Epoch 39/100, Batch 650/657, Loss: 0.0010\n",
      "\n",
      "Epoch 39/100 Results:\n",
      "Train Loss: 0.0010, Val Loss: 0.0586\n",
      "Substance Accuracy: 0.9964\n",
      "Symptom F1: 0.8856, Precision: 0.9196, Recall: 0.8541\n",
      "Learning Rate: 0.000082\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 40/100, Batch 0/657, Loss: 0.0001\n",
      "Epoch 40/100, Batch 50/657, Loss: 0.0013\n",
      "Epoch 40/100, Batch 100/657, Loss: 0.0000\n",
      "Epoch 40/100, Batch 150/657, Loss: 0.0005\n",
      "Epoch 40/100, Batch 200/657, Loss: 0.0005\n",
      "Epoch 40/100, Batch 250/657, Loss: 0.0004\n",
      "Epoch 40/100, Batch 300/657, Loss: 0.0002\n",
      "Epoch 40/100, Batch 350/657, Loss: 0.0003\n",
      "Epoch 40/100, Batch 400/657, Loss: 0.0012\n",
      "Epoch 40/100, Batch 450/657, Loss: 0.0006\n",
      "Epoch 40/100, Batch 500/657, Loss: 0.0010\n",
      "Epoch 40/100, Batch 550/657, Loss: 0.0003\n",
      "Epoch 40/100, Batch 600/657, Loss: 0.0008\n",
      "Epoch 40/100, Batch 650/657, Loss: 0.0001\n",
      "\n",
      "Epoch 40/100 Results:\n",
      "Train Loss: 0.0009, Val Loss: 0.0550\n",
      "Substance Accuracy: 0.9961\n",
      "Symptom F1: 0.8856, Precision: 0.9235, Recall: 0.8506\n",
      "Learning Rate: 0.000080\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 41/100, Batch 0/657, Loss: 0.0002\n",
      "Epoch 41/100, Batch 50/657, Loss: 0.0010\n",
      "Epoch 41/100, Batch 100/657, Loss: 0.0000\n",
      "Epoch 41/100, Batch 150/657, Loss: 0.0009\n",
      "Epoch 41/100, Batch 200/657, Loss: 0.0002\n",
      "Epoch 41/100, Batch 250/657, Loss: 0.0001\n",
      "Epoch 41/100, Batch 300/657, Loss: 0.0003\n",
      "Epoch 41/100, Batch 350/657, Loss: 0.0004\n",
      "Epoch 41/100, Batch 400/657, Loss: 0.0008\n",
      "Epoch 41/100, Batch 450/657, Loss: 0.0001\n",
      "Epoch 41/100, Batch 500/657, Loss: 0.0018\n",
      "Epoch 41/100, Batch 550/657, Loss: 0.0017\n",
      "Epoch 41/100, Batch 600/657, Loss: 0.0016\n",
      "Epoch 41/100, Batch 650/657, Loss: 0.0005\n",
      "New best model saved! Symptom F1: 0.8893, Substance Accuracy: 0.9954\n",
      "\n",
      "Epoch 41/100 Results:\n",
      "Train Loss: 0.0008, Val Loss: 0.0568\n",
      "Substance Accuracy: 0.9954\n",
      "Symptom F1: 0.8893, Precision: 0.9235, Recall: 0.8575\n",
      "Learning Rate: 0.000079\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 42/100, Batch 0/657, Loss: 0.0038\n",
      "Epoch 42/100, Batch 50/657, Loss: 0.0002\n",
      "Epoch 42/100, Batch 100/657, Loss: 0.0000\n",
      "Epoch 42/100, Batch 150/657, Loss: 0.0005\n",
      "Epoch 42/100, Batch 200/657, Loss: 0.0003\n",
      "Epoch 42/100, Batch 250/657, Loss: 0.0001\n",
      "Epoch 42/100, Batch 300/657, Loss: 0.0020\n",
      "Epoch 42/100, Batch 350/657, Loss: 0.0000\n",
      "Epoch 42/100, Batch 400/657, Loss: 0.0008\n",
      "Epoch 42/100, Batch 450/657, Loss: 0.0023\n",
      "Epoch 42/100, Batch 500/657, Loss: 0.0010\n",
      "Epoch 42/100, Batch 550/657, Loss: 0.0003\n",
      "Epoch 42/100, Batch 600/657, Loss: 0.0003\n",
      "Epoch 42/100, Batch 650/657, Loss: 0.0001\n",
      "\n",
      "Epoch 42/100 Results:\n",
      "Train Loss: 0.0008, Val Loss: 0.0619\n",
      "Substance Accuracy: 0.9919\n",
      "Symptom F1: 0.8883, Precision: 0.9260, Recall: 0.8535\n",
      "Learning Rate: 0.000077\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 43/100, Batch 0/657, Loss: 0.0003\n",
      "Epoch 43/100, Batch 50/657, Loss: 0.0001\n",
      "Epoch 43/100, Batch 100/657, Loss: 0.0000\n",
      "Epoch 43/100, Batch 150/657, Loss: 0.0001\n",
      "Epoch 43/100, Batch 200/657, Loss: 0.0001\n",
      "Epoch 43/100, Batch 250/657, Loss: 0.0005\n",
      "Epoch 43/100, Batch 300/657, Loss: 0.0002\n",
      "Epoch 43/100, Batch 350/657, Loss: 0.0002\n",
      "Epoch 43/100, Batch 400/657, Loss: 0.0003\n",
      "Epoch 43/100, Batch 450/657, Loss: 0.0014\n",
      "Epoch 43/100, Batch 500/657, Loss: 0.0005\n",
      "Epoch 43/100, Batch 550/657, Loss: 0.0006\n",
      "Epoch 43/100, Batch 600/657, Loss: 0.0001\n",
      "Epoch 43/100, Batch 650/657, Loss: 0.0007\n",
      "\n",
      "Epoch 43/100 Results:\n",
      "Train Loss: 0.0008, Val Loss: 0.0560\n",
      "Substance Accuracy: 0.9954\n",
      "Symptom F1: 0.8870, Precision: 0.9227, Recall: 0.8540\n",
      "Learning Rate: 0.000076\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 44/100, Batch 0/657, Loss: 0.0001\n",
      "Epoch 44/100, Batch 50/657, Loss: 0.0004\n",
      "Epoch 44/100, Batch 100/657, Loss: 0.0004\n",
      "Epoch 44/100, Batch 150/657, Loss: 0.0002\n",
      "Epoch 44/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 44/100, Batch 250/657, Loss: 0.0009\n",
      "Epoch 44/100, Batch 300/657, Loss: 0.0022\n",
      "Epoch 44/100, Batch 350/657, Loss: 0.0042\n",
      "Epoch 44/100, Batch 400/657, Loss: 0.0002\n",
      "Epoch 44/100, Batch 450/657, Loss: 0.0027\n",
      "Epoch 44/100, Batch 500/657, Loss: 0.0001\n",
      "Epoch 44/100, Batch 550/657, Loss: 0.0017\n",
      "Epoch 44/100, Batch 600/657, Loss: 0.0013\n",
      "Epoch 44/100, Batch 650/657, Loss: 0.0042\n",
      "\n",
      "Epoch 44/100 Results:\n",
      "Train Loss: 0.0008, Val Loss: 0.0588\n",
      "Substance Accuracy: 0.9948\n",
      "Symptom F1: 0.8838, Precision: 0.9188, Recall: 0.8514\n",
      "Learning Rate: 0.000074\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 45/100, Batch 0/657, Loss: 0.0012\n",
      "Epoch 45/100, Batch 50/657, Loss: 0.0021\n",
      "Epoch 45/100, Batch 100/657, Loss: 0.0001\n",
      "Epoch 45/100, Batch 150/657, Loss: 0.0002\n",
      "Epoch 45/100, Batch 200/657, Loss: 0.0001\n",
      "Epoch 45/100, Batch 250/657, Loss: 0.0009\n",
      "Epoch 45/100, Batch 300/657, Loss: 0.0004\n",
      "Epoch 45/100, Batch 350/657, Loss: 0.0033\n",
      "Epoch 45/100, Batch 400/657, Loss: 0.0001\n",
      "Epoch 45/100, Batch 450/657, Loss: 0.0017\n",
      "Epoch 45/100, Batch 500/657, Loss: 0.0007\n",
      "Epoch 45/100, Batch 550/657, Loss: 0.0001\n",
      "Epoch 45/100, Batch 600/657, Loss: 0.0001\n",
      "Epoch 45/100, Batch 650/657, Loss: 0.0001\n",
      "\n",
      "Epoch 45/100 Results:\n",
      "Train Loss: 0.0007, Val Loss: 0.0599\n",
      "Substance Accuracy: 0.9963\n",
      "Symptom F1: 0.8882, Precision: 0.9258, Recall: 0.8535\n",
      "Learning Rate: 0.000072\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 46/100, Batch 0/657, Loss: 0.0002\n",
      "Epoch 46/100, Batch 50/657, Loss: 0.0011\n",
      "Epoch 46/100, Batch 100/657, Loss: 0.0011\n",
      "Epoch 46/100, Batch 150/657, Loss: 0.0017\n",
      "Epoch 46/100, Batch 200/657, Loss: 0.0005\n",
      "Epoch 46/100, Batch 250/657, Loss: 0.0003\n",
      "Epoch 46/100, Batch 300/657, Loss: 0.0001\n",
      "Epoch 46/100, Batch 350/657, Loss: 0.0001\n",
      "Epoch 46/100, Batch 400/657, Loss: 0.0016\n",
      "Epoch 46/100, Batch 450/657, Loss: 0.0002\n",
      "Epoch 46/100, Batch 500/657, Loss: 0.0003\n",
      "Epoch 46/100, Batch 550/657, Loss: 0.0000\n",
      "Epoch 46/100, Batch 600/657, Loss: 0.0001\n",
      "Epoch 46/100, Batch 650/657, Loss: 0.0023\n",
      "\n",
      "Epoch 46/100 Results:\n",
      "Train Loss: 0.0007, Val Loss: 0.0558\n",
      "Substance Accuracy: 0.9961\n",
      "Symptom F1: 0.8835, Precision: 0.9207, Recall: 0.8492\n",
      "Learning Rate: 0.000071\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 47/100, Batch 0/657, Loss: 0.0010\n",
      "Epoch 47/100, Batch 50/657, Loss: 0.0001\n",
      "Epoch 47/100, Batch 100/657, Loss: 0.0011\n",
      "Epoch 47/100, Batch 150/657, Loss: 0.0004\n",
      "Epoch 47/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 47/100, Batch 250/657, Loss: 0.0004\n",
      "Epoch 47/100, Batch 300/657, Loss: 0.0002\n",
      "Epoch 47/100, Batch 350/657, Loss: 0.0000\n",
      "Epoch 47/100, Batch 400/657, Loss: 0.0002\n",
      "Epoch 47/100, Batch 450/657, Loss: 0.0006\n",
      "Epoch 47/100, Batch 500/657, Loss: 0.0002\n",
      "Epoch 47/100, Batch 550/657, Loss: 0.0010\n",
      "Epoch 47/100, Batch 600/657, Loss: 0.0001\n",
      "Epoch 47/100, Batch 650/657, Loss: 0.0005\n",
      "\n",
      "Epoch 47/100 Results:\n",
      "Train Loss: 0.0007, Val Loss: 0.0602\n",
      "Substance Accuracy: 0.9960\n",
      "Symptom F1: 0.8885, Precision: 0.9197, Recall: 0.8594\n",
      "Learning Rate: 0.000069\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 48/100, Batch 0/657, Loss: 0.0000\n",
      "Epoch 48/100, Batch 50/657, Loss: 0.0002\n",
      "Epoch 48/100, Batch 100/657, Loss: 0.0008\n",
      "Epoch 48/100, Batch 150/657, Loss: 0.0005\n",
      "Epoch 48/100, Batch 200/657, Loss: 0.0004\n",
      "Epoch 48/100, Batch 250/657, Loss: 0.0002\n",
      "Epoch 48/100, Batch 300/657, Loss: 0.0004\n",
      "Epoch 48/100, Batch 350/657, Loss: 0.0008\n",
      "Epoch 48/100, Batch 400/657, Loss: 0.0018\n",
      "Epoch 48/100, Batch 450/657, Loss: 0.0002\n",
      "Epoch 48/100, Batch 500/657, Loss: 0.0006\n",
      "Epoch 48/100, Batch 550/657, Loss: 0.0007\n",
      "Epoch 48/100, Batch 600/657, Loss: 0.0001\n",
      "Epoch 48/100, Batch 650/657, Loss: 0.0009\n",
      "\n",
      "Epoch 48/100 Results:\n",
      "Train Loss: 0.0006, Val Loss: 0.0631\n",
      "Substance Accuracy: 0.9948\n",
      "Symptom F1: 0.8882, Precision: 0.9232, Recall: 0.8558\n",
      "Learning Rate: 0.000067\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 49/100, Batch 0/657, Loss: 0.0001\n",
      "Epoch 49/100, Batch 50/657, Loss: 0.0002\n",
      "Epoch 49/100, Batch 100/657, Loss: 0.0001\n",
      "Epoch 49/100, Batch 150/657, Loss: 0.0002\n",
      "Epoch 49/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 49/100, Batch 250/657, Loss: 0.0001\n",
      "Epoch 49/100, Batch 300/657, Loss: 0.0000\n",
      "Epoch 49/100, Batch 350/657, Loss: 0.0001\n",
      "Epoch 49/100, Batch 400/657, Loss: 0.0003\n",
      "Epoch 49/100, Batch 450/657, Loss: 0.0067\n",
      "Epoch 49/100, Batch 500/657, Loss: 0.0004\n",
      "Epoch 49/100, Batch 550/657, Loss: 0.0002\n",
      "Epoch 49/100, Batch 600/657, Loss: 0.0003\n",
      "Epoch 49/100, Batch 650/657, Loss: 0.0002\n",
      "\n",
      "Epoch 49/100 Results:\n",
      "Train Loss: 0.0006, Val Loss: 0.0617\n",
      "Substance Accuracy: 0.9963\n",
      "Symptom F1: 0.8885, Precision: 0.9274, Recall: 0.8527\n",
      "Learning Rate: 0.000065\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 50/100, Batch 0/657, Loss: 0.0010\n",
      "Epoch 50/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 50/100, Batch 100/657, Loss: 0.0005\n",
      "Epoch 50/100, Batch 150/657, Loss: 0.0000\n",
      "Epoch 50/100, Batch 200/657, Loss: 0.0006\n",
      "Epoch 50/100, Batch 250/657, Loss: 0.0001\n",
      "Epoch 50/100, Batch 300/657, Loss: 0.0001\n",
      "Epoch 50/100, Batch 350/657, Loss: 0.0000\n",
      "Epoch 50/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 50/100, Batch 450/657, Loss: 0.0000\n",
      "Epoch 50/100, Batch 500/657, Loss: 0.0001\n",
      "Epoch 50/100, Batch 550/657, Loss: 0.0002\n",
      "Epoch 50/100, Batch 600/657, Loss: 0.0003\n",
      "Epoch 50/100, Batch 650/657, Loss: 0.0002\n",
      "\n",
      "Epoch 50/100 Results:\n",
      "Train Loss: 0.0007, Val Loss: 0.0639\n",
      "Substance Accuracy: 0.9982\n",
      "Symptom F1: 0.8861, Precision: 0.9222, Recall: 0.8526\n",
      "Learning Rate: 0.000064\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 51/100, Batch 0/657, Loss: 0.0009\n",
      "Epoch 51/100, Batch 50/657, Loss: 0.0006\n",
      "Epoch 51/100, Batch 100/657, Loss: 0.0005\n",
      "Epoch 51/100, Batch 150/657, Loss: 0.0000\n",
      "Epoch 51/100, Batch 200/657, Loss: 0.0005\n",
      "Epoch 51/100, Batch 250/657, Loss: 0.0004\n",
      "Epoch 51/100, Batch 300/657, Loss: 0.0001\n",
      "Epoch 51/100, Batch 350/657, Loss: 0.0002\n",
      "Epoch 51/100, Batch 400/657, Loss: 0.0001\n",
      "Epoch 51/100, Batch 450/657, Loss: 0.0005\n",
      "Epoch 51/100, Batch 500/657, Loss: 0.0000\n",
      "Epoch 51/100, Batch 550/657, Loss: 0.0007\n",
      "Epoch 51/100, Batch 600/657, Loss: 0.0001\n",
      "Epoch 51/100, Batch 650/657, Loss: 0.0001\n",
      "\n",
      "Epoch 51/100 Results:\n",
      "Train Loss: 0.0006, Val Loss: 0.0694\n",
      "Substance Accuracy: 0.9955\n",
      "Symptom F1: 0.8836, Precision: 0.9241, Recall: 0.8465\n",
      "Learning Rate: 0.000062\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 52/100, Batch 0/657, Loss: 0.0006\n",
      "Epoch 52/100, Batch 50/657, Loss: 0.0003\n",
      "Epoch 52/100, Batch 100/657, Loss: 0.0000\n",
      "Epoch 52/100, Batch 150/657, Loss: 0.0002\n",
      "Epoch 52/100, Batch 200/657, Loss: 0.0006\n",
      "Epoch 52/100, Batch 250/657, Loss: 0.0004\n",
      "Epoch 52/100, Batch 300/657, Loss: 0.0006\n",
      "Epoch 52/100, Batch 350/657, Loss: 0.0001\n",
      "Epoch 52/100, Batch 400/657, Loss: 0.0005\n",
      "Epoch 52/100, Batch 450/657, Loss: 0.0001\n",
      "Epoch 52/100, Batch 500/657, Loss: 0.0001\n",
      "Epoch 52/100, Batch 550/657, Loss: 0.0006\n",
      "Epoch 52/100, Batch 600/657, Loss: 0.0002\n",
      "Epoch 52/100, Batch 650/657, Loss: 0.0001\n",
      "New best model saved! Symptom F1: 0.8954, Substance Accuracy: 0.9967\n",
      "\n",
      "Epoch 52/100 Results:\n",
      "Train Loss: 0.0007, Val Loss: 0.0519\n",
      "Substance Accuracy: 0.9967\n",
      "Symptom F1: 0.8954, Precision: 0.9285, Recall: 0.8646\n",
      "Learning Rate: 0.000060\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 53/100, Batch 0/657, Loss: 0.0001\n",
      "Epoch 53/100, Batch 50/657, Loss: 0.0004\n",
      "Epoch 53/100, Batch 100/657, Loss: 0.0000\n",
      "Epoch 53/100, Batch 150/657, Loss: 0.0003\n",
      "Epoch 53/100, Batch 200/657, Loss: 0.0015\n",
      "Epoch 53/100, Batch 250/657, Loss: 0.0019\n",
      "Epoch 53/100, Batch 300/657, Loss: 0.0002\n",
      "Epoch 53/100, Batch 350/657, Loss: 0.0003\n",
      "Epoch 53/100, Batch 400/657, Loss: 0.0002\n",
      "Epoch 53/100, Batch 450/657, Loss: 0.0000\n",
      "Epoch 53/100, Batch 500/657, Loss: 0.0001\n",
      "Epoch 53/100, Batch 550/657, Loss: 0.0002\n",
      "Epoch 53/100, Batch 600/657, Loss: 0.0006\n",
      "Epoch 53/100, Batch 650/657, Loss: 0.0001\n",
      "\n",
      "Epoch 53/100 Results:\n",
      "Train Loss: 0.0006, Val Loss: 0.0627\n",
      "Substance Accuracy: 0.9972\n",
      "Symptom F1: 0.8858, Precision: 0.9215, Recall: 0.8527\n",
      "Learning Rate: 0.000058\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 54/100, Batch 0/657, Loss: 0.0017\n",
      "Epoch 54/100, Batch 50/657, Loss: 0.0007\n",
      "Epoch 54/100, Batch 100/657, Loss: 0.0004\n",
      "Epoch 54/100, Batch 150/657, Loss: 0.0001\n",
      "Epoch 54/100, Batch 200/657, Loss: 0.0001\n",
      "Epoch 54/100, Batch 250/657, Loss: 0.0003\n",
      "Epoch 54/100, Batch 300/657, Loss: 0.0002\n",
      "Epoch 54/100, Batch 350/657, Loss: 0.0001\n",
      "Epoch 54/100, Batch 400/657, Loss: 0.0001\n",
      "Epoch 54/100, Batch 450/657, Loss: 0.0002\n",
      "Epoch 54/100, Batch 500/657, Loss: 0.0006\n",
      "Epoch 54/100, Batch 550/657, Loss: 0.0018\n",
      "Epoch 54/100, Batch 600/657, Loss: 0.0003\n",
      "Epoch 54/100, Batch 650/657, Loss: 0.0000\n",
      "\n",
      "Epoch 54/100 Results:\n",
      "Train Loss: 0.0005, Val Loss: 0.0667\n",
      "Substance Accuracy: 0.9985\n",
      "Symptom F1: 0.8854, Precision: 0.9181, Recall: 0.8550\n",
      "Learning Rate: 0.000056\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 55/100, Batch 0/657, Loss: 0.0020\n",
      "Epoch 55/100, Batch 50/657, Loss: 0.0006\n",
      "Epoch 55/100, Batch 100/657, Loss: 0.0009\n",
      "Epoch 55/100, Batch 150/657, Loss: 0.0010\n",
      "Epoch 55/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 55/100, Batch 250/657, Loss: 0.0000\n",
      "Epoch 55/100, Batch 300/657, Loss: 0.0005\n",
      "Epoch 55/100, Batch 350/657, Loss: 0.0004\n",
      "Epoch 55/100, Batch 400/657, Loss: 0.0004\n",
      "Epoch 55/100, Batch 450/657, Loss: 0.0003\n",
      "Epoch 55/100, Batch 500/657, Loss: 0.0008\n",
      "Epoch 55/100, Batch 550/657, Loss: 0.0000\n",
      "Epoch 55/100, Batch 600/657, Loss: 0.0002\n",
      "Epoch 55/100, Batch 650/657, Loss: 0.0001\n",
      "\n",
      "Epoch 55/100 Results:\n",
      "Train Loss: 0.0005, Val Loss: 0.0630\n",
      "Substance Accuracy: 0.9964\n",
      "Symptom F1: 0.8937, Precision: 0.9309, Recall: 0.8594\n",
      "Learning Rate: 0.000055\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 56/100, Batch 0/657, Loss: 0.0006\n",
      "Epoch 56/100, Batch 50/657, Loss: 0.0001\n",
      "Epoch 56/100, Batch 100/657, Loss: 0.0001\n",
      "Epoch 56/100, Batch 150/657, Loss: 0.0028\n",
      "Epoch 56/100, Batch 200/657, Loss: 0.0005\n",
      "Epoch 56/100, Batch 250/657, Loss: 0.0001\n",
      "Epoch 56/100, Batch 300/657, Loss: 0.0001\n",
      "Epoch 56/100, Batch 350/657, Loss: 0.0000\n",
      "Epoch 56/100, Batch 400/657, Loss: 0.0003\n",
      "Epoch 56/100, Batch 450/657, Loss: 0.0000\n",
      "Epoch 56/100, Batch 500/657, Loss: 0.0001\n",
      "Epoch 56/100, Batch 550/657, Loss: 0.0003\n",
      "Epoch 56/100, Batch 600/657, Loss: 0.0002\n",
      "Epoch 56/100, Batch 650/657, Loss: 0.0001\n",
      "\n",
      "Epoch 56/100 Results:\n",
      "Train Loss: 0.0005, Val Loss: 0.0627\n",
      "Substance Accuracy: 0.9969\n",
      "Symptom F1: 0.8860, Precision: 0.9253, Recall: 0.8500\n",
      "Learning Rate: 0.000053\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 57/100, Batch 0/657, Loss: 0.0000\n",
      "Epoch 57/100, Batch 50/657, Loss: 0.0001\n",
      "Epoch 57/100, Batch 100/657, Loss: 0.0002\n",
      "Epoch 57/100, Batch 150/657, Loss: 0.0029\n",
      "Epoch 57/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 57/100, Batch 250/657, Loss: 0.0001\n",
      "Epoch 57/100, Batch 300/657, Loss: 0.0016\n",
      "Epoch 57/100, Batch 350/657, Loss: 0.0003\n",
      "Epoch 57/100, Batch 400/657, Loss: 0.0017\n",
      "Epoch 57/100, Batch 450/657, Loss: 0.0007\n",
      "Epoch 57/100, Batch 500/657, Loss: 0.0000\n",
      "Epoch 57/100, Batch 550/657, Loss: 0.0002\n",
      "Epoch 57/100, Batch 600/657, Loss: 0.0000\n",
      "Epoch 57/100, Batch 650/657, Loss: 0.0007\n",
      "\n",
      "Epoch 57/100 Results:\n",
      "Train Loss: 0.0005, Val Loss: 0.0695\n",
      "Substance Accuracy: 0.9947\n",
      "Symptom F1: 0.8887, Precision: 0.9297, Recall: 0.8511\n",
      "Learning Rate: 0.000051\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 58/100, Batch 0/657, Loss: 0.0000\n",
      "Epoch 58/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 58/100, Batch 100/657, Loss: 0.0002\n",
      "Epoch 58/100, Batch 150/657, Loss: 0.0001\n",
      "Epoch 58/100, Batch 200/657, Loss: 0.0012\n",
      "Epoch 58/100, Batch 250/657, Loss: 0.0000\n",
      "Epoch 58/100, Batch 300/657, Loss: 0.0005\n",
      "Epoch 58/100, Batch 350/657, Loss: 0.0001\n",
      "Epoch 58/100, Batch 400/657, Loss: 0.0001\n",
      "Epoch 58/100, Batch 450/657, Loss: 0.0004\n",
      "Epoch 58/100, Batch 500/657, Loss: 0.0013\n",
      "Epoch 58/100, Batch 550/657, Loss: 0.0009\n",
      "Epoch 58/100, Batch 600/657, Loss: 0.0002\n",
      "Epoch 58/100, Batch 650/657, Loss: 0.0001\n",
      "\n",
      "Epoch 58/100 Results:\n",
      "Train Loss: 0.0004, Val Loss: 0.0651\n",
      "Substance Accuracy: 0.9963\n",
      "Symptom F1: 0.8896, Precision: 0.9331, Recall: 0.8499\n",
      "Learning Rate: 0.000049\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 59/100, Batch 0/657, Loss: 0.0000\n",
      "Epoch 59/100, Batch 50/657, Loss: 0.0003\n",
      "Epoch 59/100, Batch 100/657, Loss: 0.0014\n",
      "Epoch 59/100, Batch 150/657, Loss: 0.0008\n",
      "Epoch 59/100, Batch 200/657, Loss: 0.0001\n",
      "Epoch 59/100, Batch 250/657, Loss: 0.0036\n",
      "Epoch 59/100, Batch 300/657, Loss: 0.0005\n",
      "Epoch 59/100, Batch 350/657, Loss: 0.0010\n",
      "Epoch 59/100, Batch 400/657, Loss: 0.0009\n",
      "Epoch 59/100, Batch 450/657, Loss: 0.0007\n",
      "Epoch 59/100, Batch 500/657, Loss: 0.0000\n",
      "Epoch 59/100, Batch 550/657, Loss: 0.0003\n",
      "Epoch 59/100, Batch 600/657, Loss: 0.0000\n",
      "Epoch 59/100, Batch 650/657, Loss: 0.0002\n",
      "\n",
      "Epoch 59/100 Results:\n",
      "Train Loss: 0.0005, Val Loss: 0.0585\n",
      "Substance Accuracy: 0.9982\n",
      "Symptom F1: 0.8933, Precision: 0.9300, Recall: 0.8594\n",
      "Learning Rate: 0.000047\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 60/100, Batch 0/657, Loss: 0.0019\n",
      "Epoch 60/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 60/100, Batch 100/657, Loss: 0.0007\n",
      "Epoch 60/100, Batch 150/657, Loss: 0.0001\n",
      "Epoch 60/100, Batch 200/657, Loss: 0.0005\n",
      "Epoch 60/100, Batch 250/657, Loss: 0.0000\n",
      "Epoch 60/100, Batch 300/657, Loss: 0.0032\n",
      "Epoch 60/100, Batch 350/657, Loss: 0.0002\n",
      "Epoch 60/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 60/100, Batch 450/657, Loss: 0.0001\n",
      "Epoch 60/100, Batch 500/657, Loss: 0.0001\n",
      "Epoch 60/100, Batch 550/657, Loss: 0.0005\n",
      "Epoch 60/100, Batch 600/657, Loss: 0.0002\n",
      "Epoch 60/100, Batch 650/657, Loss: 0.0001\n",
      "\n",
      "Epoch 60/100 Results:\n",
      "Train Loss: 0.0005, Val Loss: 0.0576\n",
      "Substance Accuracy: 0.9969\n",
      "Symptom F1: 0.8889, Precision: 0.9297, Recall: 0.8516\n",
      "Learning Rate: 0.000045\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 61/100, Batch 0/657, Loss: 0.0005\n",
      "Epoch 61/100, Batch 50/657, Loss: 0.0001\n",
      "Epoch 61/100, Batch 100/657, Loss: 0.0000\n",
      "Epoch 61/100, Batch 150/657, Loss: 0.0001\n",
      "Epoch 61/100, Batch 200/657, Loss: 0.0014\n",
      "Epoch 61/100, Batch 250/657, Loss: 0.0006\n",
      "Epoch 61/100, Batch 300/657, Loss: 0.0002\n",
      "Epoch 61/100, Batch 350/657, Loss: 0.0003\n",
      "Epoch 61/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 61/100, Batch 450/657, Loss: 0.0003\n",
      "Epoch 61/100, Batch 500/657, Loss: 0.0002\n",
      "Epoch 61/100, Batch 550/657, Loss: 0.0003\n",
      "Epoch 61/100, Batch 600/657, Loss: 0.0002\n",
      "Epoch 61/100, Batch 650/657, Loss: 0.0007\n",
      "\n",
      "Epoch 61/100 Results:\n",
      "Train Loss: 0.0005, Val Loss: 0.0627\n",
      "Substance Accuracy: 0.9966\n",
      "Symptom F1: 0.8915, Precision: 0.9290, Recall: 0.8570\n",
      "Learning Rate: 0.000044\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 62/100, Batch 0/657, Loss: 0.0001\n",
      "Epoch 62/100, Batch 50/657, Loss: 0.0001\n",
      "Epoch 62/100, Batch 100/657, Loss: 0.0000\n",
      "Epoch 62/100, Batch 150/657, Loss: 0.0028\n",
      "Epoch 62/100, Batch 200/657, Loss: 0.0002\n",
      "Epoch 62/100, Batch 250/657, Loss: 0.0011\n",
      "Epoch 62/100, Batch 300/657, Loss: 0.0000\n",
      "Epoch 62/100, Batch 350/657, Loss: 0.0002\n",
      "Epoch 62/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 62/100, Batch 450/657, Loss: 0.0002\n",
      "Epoch 62/100, Batch 500/657, Loss: 0.0001\n",
      "Epoch 62/100, Batch 550/657, Loss: 0.0005\n",
      "Epoch 62/100, Batch 600/657, Loss: 0.0001\n",
      "Epoch 62/100, Batch 650/657, Loss: 0.0000\n",
      "\n",
      "Epoch 62/100 Results:\n",
      "Train Loss: 0.0004, Val Loss: 0.0726\n",
      "Substance Accuracy: 0.9967\n",
      "Symptom F1: 0.8828, Precision: 0.9215, Recall: 0.8473\n",
      "Learning Rate: 0.000042\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 63/100, Batch 0/657, Loss: 0.0004\n",
      "Epoch 63/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 63/100, Batch 100/657, Loss: 0.0000\n",
      "Epoch 63/100, Batch 150/657, Loss: 0.0001\n",
      "Epoch 63/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 63/100, Batch 250/657, Loss: 0.0004\n",
      "Epoch 63/100, Batch 300/657, Loss: 0.0001\n",
      "Epoch 63/100, Batch 350/657, Loss: 0.0006\n",
      "Epoch 63/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 63/100, Batch 450/657, Loss: 0.0000\n",
      "Epoch 63/100, Batch 500/657, Loss: 0.0001\n",
      "Epoch 63/100, Batch 550/657, Loss: 0.0003\n",
      "Epoch 63/100, Batch 600/657, Loss: 0.0008\n",
      "Epoch 63/100, Batch 650/657, Loss: 0.0001\n",
      "\n",
      "Epoch 63/100 Results:\n",
      "Train Loss: 0.0005, Val Loss: 0.0626\n",
      "Substance Accuracy: 0.9979\n",
      "Symptom F1: 0.8914, Precision: 0.9303, Recall: 0.8556\n",
      "Learning Rate: 0.000040\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 64/100, Batch 0/657, Loss: 0.0000\n",
      "Epoch 64/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 64/100, Batch 100/657, Loss: 0.0001\n",
      "Epoch 64/100, Batch 150/657, Loss: 0.0001\n",
      "Epoch 64/100, Batch 200/657, Loss: 0.0013\n",
      "Epoch 64/100, Batch 250/657, Loss: 0.0003\n",
      "Epoch 64/100, Batch 300/657, Loss: 0.0001\n",
      "Epoch 64/100, Batch 350/657, Loss: 0.0000\n",
      "Epoch 64/100, Batch 400/657, Loss: 0.0001\n",
      "Epoch 64/100, Batch 450/657, Loss: 0.0010\n",
      "Epoch 64/100, Batch 500/657, Loss: 0.0012\n",
      "Epoch 64/100, Batch 550/657, Loss: 0.0000\n",
      "Epoch 64/100, Batch 600/657, Loss: 0.0000\n",
      "Epoch 64/100, Batch 650/657, Loss: 0.0000\n",
      "\n",
      "Epoch 64/100 Results:\n",
      "Train Loss: 0.0004, Val Loss: 0.0739\n",
      "Substance Accuracy: 0.9964\n",
      "Symptom F1: 0.8844, Precision: 0.9263, Recall: 0.8461\n",
      "Learning Rate: 0.000038\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 65/100, Batch 0/657, Loss: 0.0005\n",
      "Epoch 65/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 65/100, Batch 100/657, Loss: 0.0001\n",
      "Epoch 65/100, Batch 150/657, Loss: 0.0026\n",
      "Epoch 65/100, Batch 200/657, Loss: 0.0001\n",
      "Epoch 65/100, Batch 250/657, Loss: 0.0002\n",
      "Epoch 65/100, Batch 300/657, Loss: 0.0003\n",
      "Epoch 65/100, Batch 350/657, Loss: 0.0004\n",
      "Epoch 65/100, Batch 400/657, Loss: 0.0004\n",
      "Epoch 65/100, Batch 450/657, Loss: 0.0014\n",
      "Epoch 65/100, Batch 500/657, Loss: 0.0004\n",
      "Epoch 65/100, Batch 550/657, Loss: 0.0001\n",
      "Epoch 65/100, Batch 600/657, Loss: 0.0016\n",
      "Epoch 65/100, Batch 650/657, Loss: 0.0001\n",
      "\n",
      "Epoch 65/100 Results:\n",
      "Train Loss: 0.0004, Val Loss: 0.0688\n",
      "Substance Accuracy: 0.9955\n",
      "Symptom F1: 0.8900, Precision: 0.9277, Recall: 0.8552\n",
      "Learning Rate: 0.000036\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 66/100, Batch 0/657, Loss: 0.0001\n",
      "Epoch 66/100, Batch 50/657, Loss: 0.0014\n",
      "Epoch 66/100, Batch 100/657, Loss: 0.0000\n",
      "Epoch 66/100, Batch 150/657, Loss: 0.0013\n",
      "Epoch 66/100, Batch 200/657, Loss: 0.0002\n",
      "Epoch 66/100, Batch 250/657, Loss: 0.0003\n",
      "Epoch 66/100, Batch 300/657, Loss: 0.0002\n",
      "Epoch 66/100, Batch 350/657, Loss: 0.0001\n",
      "Epoch 66/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 66/100, Batch 450/657, Loss: 0.0000\n",
      "Epoch 66/100, Batch 500/657, Loss: 0.0004\n",
      "Epoch 66/100, Batch 550/657, Loss: 0.0001\n",
      "Epoch 66/100, Batch 600/657, Loss: 0.0000\n",
      "Epoch 66/100, Batch 650/657, Loss: 0.0001\n",
      "\n",
      "Epoch 66/100 Results:\n",
      "Train Loss: 0.0004, Val Loss: 0.0721\n",
      "Substance Accuracy: 0.9957\n",
      "Symptom F1: 0.8820, Precision: 0.9283, Recall: 0.8400\n",
      "Learning Rate: 0.000035\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 67/100, Batch 0/657, Loss: 0.0001\n",
      "Epoch 67/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 67/100, Batch 100/657, Loss: 0.0018\n",
      "Epoch 67/100, Batch 150/657, Loss: 0.0000\n",
      "Epoch 67/100, Batch 200/657, Loss: 0.0003\n",
      "Epoch 67/100, Batch 250/657, Loss: 0.0004\n",
      "Epoch 67/100, Batch 300/657, Loss: 0.0000\n",
      "Epoch 67/100, Batch 350/657, Loss: 0.0000\n",
      "Epoch 67/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 67/100, Batch 450/657, Loss: 0.0001\n",
      "Epoch 67/100, Batch 500/657, Loss: 0.0000\n",
      "Epoch 67/100, Batch 550/657, Loss: 0.0010\n",
      "Epoch 67/100, Batch 600/657, Loss: 0.0004\n",
      "Epoch 67/100, Batch 650/657, Loss: 0.0009\n",
      "\n",
      "Epoch 67/100 Results:\n",
      "Train Loss: 0.0004, Val Loss: 0.0708\n",
      "Substance Accuracy: 0.9960\n",
      "Symptom F1: 0.8841, Precision: 0.9290, Recall: 0.8434\n",
      "Learning Rate: 0.000033\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 68/100, Batch 0/657, Loss: 0.0001\n",
      "Epoch 68/100, Batch 50/657, Loss: 0.0001\n",
      "Epoch 68/100, Batch 100/657, Loss: 0.0011\n",
      "Epoch 68/100, Batch 150/657, Loss: 0.0006\n",
      "Epoch 68/100, Batch 200/657, Loss: 0.0002\n",
      "Epoch 68/100, Batch 250/657, Loss: 0.0002\n",
      "Epoch 68/100, Batch 300/657, Loss: 0.0000\n",
      "Epoch 68/100, Batch 350/657, Loss: 0.0002\n",
      "Epoch 68/100, Batch 400/657, Loss: 0.0003\n",
      "Epoch 68/100, Batch 450/657, Loss: 0.0000\n",
      "Epoch 68/100, Batch 500/657, Loss: 0.0002\n",
      "Epoch 68/100, Batch 550/657, Loss: 0.0005\n",
      "Epoch 68/100, Batch 600/657, Loss: 0.0000\n",
      "Epoch 68/100, Batch 650/657, Loss: 0.0000\n",
      "\n",
      "Epoch 68/100 Results:\n",
      "Train Loss: 0.0004, Val Loss: 0.0702\n",
      "Substance Accuracy: 0.9968\n",
      "Symptom F1: 0.8890, Precision: 0.9332, Recall: 0.8488\n",
      "Learning Rate: 0.000031\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 69/100, Batch 0/657, Loss: 0.0000\n",
      "Epoch 69/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 69/100, Batch 100/657, Loss: 0.0008\n",
      "Epoch 69/100, Batch 150/657, Loss: 0.0000\n",
      "Epoch 69/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 69/100, Batch 250/657, Loss: 0.0001\n",
      "Epoch 69/100, Batch 300/657, Loss: 0.0001\n",
      "Epoch 69/100, Batch 350/657, Loss: 0.0000\n",
      "Epoch 69/100, Batch 400/657, Loss: 0.0001\n",
      "Epoch 69/100, Batch 450/657, Loss: 0.0000\n",
      "Epoch 69/100, Batch 500/657, Loss: 0.0000\n",
      "Epoch 69/100, Batch 550/657, Loss: 0.0001\n",
      "Epoch 69/100, Batch 600/657, Loss: 0.0002\n",
      "Epoch 69/100, Batch 650/657, Loss: 0.0007\n",
      "\n",
      "Epoch 69/100 Results:\n",
      "Train Loss: 0.0004, Val Loss: 0.0704\n",
      "Substance Accuracy: 0.9964\n",
      "Symptom F1: 0.8893, Precision: 0.9294, Recall: 0.8525\n",
      "Learning Rate: 0.000029\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 70/100, Batch 0/657, Loss: 0.0002\n",
      "Epoch 70/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 70/100, Batch 100/657, Loss: 0.0000\n",
      "Epoch 70/100, Batch 150/657, Loss: 0.0000\n",
      "Epoch 70/100, Batch 200/657, Loss: 0.0003\n",
      "Epoch 70/100, Batch 250/657, Loss: 0.0000\n",
      "Epoch 70/100, Batch 300/657, Loss: 0.0001\n",
      "Epoch 70/100, Batch 350/657, Loss: 0.0000\n",
      "Epoch 70/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 70/100, Batch 450/657, Loss: 0.0038\n",
      "Epoch 70/100, Batch 500/657, Loss: 0.0002\n",
      "Epoch 70/100, Batch 550/657, Loss: 0.0003\n",
      "Epoch 70/100, Batch 600/657, Loss: 0.0002\n",
      "Epoch 70/100, Batch 650/657, Loss: 0.0001\n",
      "\n",
      "Epoch 70/100 Results:\n",
      "Train Loss: 0.0004, Val Loss: 0.0688\n",
      "Substance Accuracy: 0.9971\n",
      "Symptom F1: 0.8894, Precision: 0.9330, Recall: 0.8496\n",
      "Learning Rate: 0.000028\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 71/100, Batch 0/657, Loss: 0.0007\n",
      "Epoch 71/100, Batch 50/657, Loss: 0.0001\n",
      "Epoch 71/100, Batch 100/657, Loss: 0.0013\n",
      "Epoch 71/100, Batch 150/657, Loss: 0.0001\n",
      "Epoch 71/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 71/100, Batch 250/657, Loss: 0.0001\n",
      "Epoch 71/100, Batch 300/657, Loss: 0.0001\n",
      "Epoch 71/100, Batch 350/657, Loss: 0.0001\n",
      "Epoch 71/100, Batch 400/657, Loss: 0.0005\n",
      "Epoch 71/100, Batch 450/657, Loss: 0.0002\n",
      "Epoch 71/100, Batch 500/657, Loss: 0.0000\n",
      "Epoch 71/100, Batch 550/657, Loss: 0.0001\n",
      "Epoch 71/100, Batch 600/657, Loss: 0.0011\n",
      "Epoch 71/100, Batch 650/657, Loss: 0.0007\n",
      "\n",
      "Epoch 71/100 Results:\n",
      "Train Loss: 0.0004, Val Loss: 0.0666\n",
      "Substance Accuracy: 0.9981\n",
      "Symptom F1: 0.8895, Precision: 0.9335, Recall: 0.8494\n",
      "Learning Rate: 0.000026\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 72/100, Batch 0/657, Loss: 0.0000\n",
      "Epoch 72/100, Batch 50/657, Loss: 0.0001\n",
      "Epoch 72/100, Batch 100/657, Loss: 0.0032\n",
      "Epoch 72/100, Batch 150/657, Loss: 0.0000\n",
      "Epoch 72/100, Batch 200/657, Loss: 0.0001\n",
      "Epoch 72/100, Batch 250/657, Loss: 0.0001\n",
      "Epoch 72/100, Batch 300/657, Loss: 0.0002\n",
      "Epoch 72/100, Batch 350/657, Loss: 0.0001\n",
      "Epoch 72/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 72/100, Batch 450/657, Loss: 0.0001\n",
      "Epoch 72/100, Batch 500/657, Loss: 0.0001\n",
      "Epoch 72/100, Batch 550/657, Loss: 0.0005\n",
      "Epoch 72/100, Batch 600/657, Loss: 0.0005\n",
      "Epoch 72/100, Batch 650/657, Loss: 0.0002\n",
      "\n",
      "Epoch 72/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0651\n",
      "Substance Accuracy: 0.9962\n",
      "Symptom F1: 0.8922, Precision: 0.9322, Recall: 0.8556\n",
      "Learning Rate: 0.000024\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 73/100, Batch 0/657, Loss: 0.0000\n",
      "Epoch 73/100, Batch 50/657, Loss: 0.0001\n",
      "Epoch 73/100, Batch 100/657, Loss: 0.0001\n",
      "Epoch 73/100, Batch 150/657, Loss: 0.0001\n",
      "Epoch 73/100, Batch 200/657, Loss: 0.0013\n",
      "Epoch 73/100, Batch 250/657, Loss: 0.0015\n",
      "Epoch 73/100, Batch 300/657, Loss: 0.0003\n",
      "Epoch 73/100, Batch 350/657, Loss: 0.0001\n",
      "Epoch 73/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 73/100, Batch 450/657, Loss: 0.0009\n",
      "Epoch 73/100, Batch 500/657, Loss: 0.0004\n",
      "Epoch 73/100, Batch 550/657, Loss: 0.0005\n",
      "Epoch 73/100, Batch 600/657, Loss: 0.0008\n",
      "Epoch 73/100, Batch 650/657, Loss: 0.0003\n",
      "\n",
      "Epoch 73/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0707\n",
      "Substance Accuracy: 0.9959\n",
      "Symptom F1: 0.8905, Precision: 0.9354, Recall: 0.8498\n",
      "Learning Rate: 0.000023\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 74/100, Batch 0/657, Loss: 0.0002\n",
      "Epoch 74/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 74/100, Batch 100/657, Loss: 0.0000\n",
      "Epoch 74/100, Batch 150/657, Loss: 0.0011\n",
      "Epoch 74/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 74/100, Batch 250/657, Loss: 0.0000\n",
      "Epoch 74/100, Batch 300/657, Loss: 0.0003\n",
      "Epoch 74/100, Batch 350/657, Loss: 0.0000\n",
      "Epoch 74/100, Batch 400/657, Loss: 0.0002\n",
      "Epoch 74/100, Batch 450/657, Loss: 0.0000\n",
      "Epoch 74/100, Batch 500/657, Loss: 0.0014\n",
      "Epoch 74/100, Batch 550/657, Loss: 0.0007\n",
      "Epoch 74/100, Batch 600/657, Loss: 0.0007\n",
      "Epoch 74/100, Batch 650/657, Loss: 0.0000\n",
      "\n",
      "Epoch 74/100 Results:\n",
      "Train Loss: 0.0004, Val Loss: 0.0769\n",
      "Substance Accuracy: 0.9963\n",
      "Symptom F1: 0.8864, Precision: 0.9277, Recall: 0.8487\n",
      "Learning Rate: 0.000021\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 75/100, Batch 0/657, Loss: 0.0008\n",
      "Epoch 75/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 75/100, Batch 100/657, Loss: 0.0000\n",
      "Epoch 75/100, Batch 150/657, Loss: 0.0012\n",
      "Epoch 75/100, Batch 200/657, Loss: 0.0008\n",
      "Epoch 75/100, Batch 250/657, Loss: 0.0001\n",
      "Epoch 75/100, Batch 300/657, Loss: 0.0001\n",
      "Epoch 75/100, Batch 350/657, Loss: 0.0002\n",
      "Epoch 75/100, Batch 400/657, Loss: 0.0001\n",
      "Epoch 75/100, Batch 450/657, Loss: 0.0000\n",
      "Epoch 75/100, Batch 500/657, Loss: 0.0001\n",
      "Epoch 75/100, Batch 550/657, Loss: 0.0018\n",
      "Epoch 75/100, Batch 600/657, Loss: 0.0007\n",
      "Epoch 75/100, Batch 650/657, Loss: 0.0017\n",
      "\n",
      "Epoch 75/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0662\n",
      "Substance Accuracy: 0.9958\n",
      "Symptom F1: 0.8896, Precision: 0.9297, Recall: 0.8527\n",
      "Learning Rate: 0.000020\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 76/100, Batch 0/657, Loss: 0.0000\n",
      "Epoch 76/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 76/100, Batch 100/657, Loss: 0.0003\n",
      "Epoch 76/100, Batch 150/657, Loss: 0.0000\n",
      "Epoch 76/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 76/100, Batch 250/657, Loss: 0.0001\n",
      "Epoch 76/100, Batch 300/657, Loss: 0.0005\n",
      "Epoch 76/100, Batch 350/657, Loss: 0.0001\n",
      "Epoch 76/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 76/100, Batch 450/657, Loss: 0.0035\n",
      "Epoch 76/100, Batch 500/657, Loss: 0.0005\n",
      "Epoch 76/100, Batch 550/657, Loss: 0.0001\n",
      "Epoch 76/100, Batch 600/657, Loss: 0.0002\n",
      "Epoch 76/100, Batch 650/657, Loss: 0.0001\n",
      "\n",
      "Epoch 76/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0724\n",
      "Substance Accuracy: 0.9959\n",
      "Symptom F1: 0.8858, Precision: 0.9303, Recall: 0.8454\n",
      "Learning Rate: 0.000018\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 77/100, Batch 0/657, Loss: 0.0002\n",
      "Epoch 77/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 77/100, Batch 100/657, Loss: 0.0001\n",
      "Epoch 77/100, Batch 150/657, Loss: 0.0000\n",
      "Epoch 77/100, Batch 200/657, Loss: 0.0006\n",
      "Epoch 77/100, Batch 250/657, Loss: 0.0002\n",
      "Epoch 77/100, Batch 300/657, Loss: 0.0001\n",
      "Epoch 77/100, Batch 350/657, Loss: 0.0004\n",
      "Epoch 77/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 77/100, Batch 450/657, Loss: 0.0000\n",
      "Epoch 77/100, Batch 500/657, Loss: 0.0001\n",
      "Epoch 77/100, Batch 550/657, Loss: 0.0000\n",
      "Epoch 77/100, Batch 600/657, Loss: 0.0000\n",
      "Epoch 77/100, Batch 650/657, Loss: 0.0008\n",
      "\n",
      "Epoch 77/100 Results:\n",
      "Train Loss: 0.0004, Val Loss: 0.0615\n",
      "Substance Accuracy: 0.9977\n",
      "Symptom F1: 0.8947, Precision: 0.9363, Recall: 0.8566\n",
      "Learning Rate: 0.000017\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 78/100, Batch 0/657, Loss: 0.0003\n",
      "Epoch 78/100, Batch 50/657, Loss: 0.0026\n",
      "Epoch 78/100, Batch 100/657, Loss: 0.0008\n",
      "Epoch 78/100, Batch 150/657, Loss: 0.0000\n",
      "Epoch 78/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 78/100, Batch 250/657, Loss: 0.0000\n",
      "Epoch 78/100, Batch 300/657, Loss: 0.0000\n",
      "Epoch 78/100, Batch 350/657, Loss: 0.0003\n",
      "Epoch 78/100, Batch 400/657, Loss: 0.0001\n",
      "Epoch 78/100, Batch 450/657, Loss: 0.0004\n",
      "Epoch 78/100, Batch 500/657, Loss: 0.0000\n",
      "Epoch 78/100, Batch 550/657, Loss: 0.0000\n",
      "Epoch 78/100, Batch 600/657, Loss: 0.0000\n",
      "Epoch 78/100, Batch 650/657, Loss: 0.0014\n",
      "\n",
      "Epoch 78/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0720\n",
      "Substance Accuracy: 0.9984\n",
      "Symptom F1: 0.8906, Precision: 0.9356, Recall: 0.8497\n",
      "Learning Rate: 0.000016\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 79/100, Batch 0/657, Loss: 0.0000\n",
      "Epoch 79/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 79/100, Batch 100/657, Loss: 0.0002\n",
      "Epoch 79/100, Batch 150/657, Loss: 0.0000\n",
      "Epoch 79/100, Batch 200/657, Loss: 0.0003\n",
      "Epoch 79/100, Batch 250/657, Loss: 0.0002\n",
      "Epoch 79/100, Batch 300/657, Loss: 0.0004\n",
      "Epoch 79/100, Batch 350/657, Loss: 0.0014\n",
      "Epoch 79/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 79/100, Batch 450/657, Loss: 0.0005\n",
      "Epoch 79/100, Batch 500/657, Loss: 0.0000\n",
      "Epoch 79/100, Batch 550/657, Loss: 0.0000\n",
      "Epoch 79/100, Batch 600/657, Loss: 0.0001\n",
      "Epoch 79/100, Batch 650/657, Loss: 0.0005\n",
      "\n",
      "Epoch 79/100 Results:\n",
      "Train Loss: 0.0004, Val Loss: 0.0685\n",
      "Substance Accuracy: 0.9963\n",
      "Symptom F1: 0.8953, Precision: 0.9369, Recall: 0.8572\n",
      "Learning Rate: 0.000014\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 80/100, Batch 0/657, Loss: 0.0004\n",
      "Epoch 80/100, Batch 50/657, Loss: 0.0003\n",
      "Epoch 80/100, Batch 100/657, Loss: 0.0000\n",
      "Epoch 80/100, Batch 150/657, Loss: 0.0001\n",
      "Epoch 80/100, Batch 200/657, Loss: 0.0001\n",
      "Epoch 80/100, Batch 250/657, Loss: 0.0001\n",
      "Epoch 80/100, Batch 300/657, Loss: 0.0002\n",
      "Epoch 80/100, Batch 350/657, Loss: 0.0001\n",
      "Epoch 80/100, Batch 400/657, Loss: 0.0011\n",
      "Epoch 80/100, Batch 450/657, Loss: 0.0002\n",
      "Epoch 80/100, Batch 500/657, Loss: 0.0002\n",
      "Epoch 80/100, Batch 550/657, Loss: 0.0003\n",
      "Epoch 80/100, Batch 600/657, Loss: 0.0000\n",
      "Epoch 80/100, Batch 650/657, Loss: 0.0006\n",
      "\n",
      "Epoch 80/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0783\n",
      "Substance Accuracy: 0.9955\n",
      "Symptom F1: 0.8907, Precision: 0.9333, Recall: 0.8519\n",
      "Learning Rate: 0.000013\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 81/100, Batch 0/657, Loss: 0.0001\n",
      "Epoch 81/100, Batch 50/657, Loss: 0.0001\n",
      "Epoch 81/100, Batch 100/657, Loss: 0.0001\n",
      "Epoch 81/100, Batch 150/657, Loss: 0.0006\n",
      "Epoch 81/100, Batch 200/657, Loss: 0.0001\n",
      "Epoch 81/100, Batch 250/657, Loss: 0.0002\n",
      "Epoch 81/100, Batch 300/657, Loss: 0.0001\n",
      "Epoch 81/100, Batch 350/657, Loss: 0.0035\n",
      "Epoch 81/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 81/100, Batch 450/657, Loss: 0.0001\n",
      "Epoch 81/100, Batch 500/657, Loss: 0.0000\n",
      "Epoch 81/100, Batch 550/657, Loss: 0.0001\n",
      "Epoch 81/100, Batch 600/657, Loss: 0.0000\n",
      "Epoch 81/100, Batch 650/657, Loss: 0.0005\n",
      "\n",
      "Epoch 81/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0751\n",
      "Substance Accuracy: 0.9965\n",
      "Symptom F1: 0.8889, Precision: 0.9344, Recall: 0.8476\n",
      "Learning Rate: 0.000012\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 82/100, Batch 0/657, Loss: 0.0001\n",
      "Epoch 82/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 82/100, Batch 100/657, Loss: 0.0003\n",
      "Epoch 82/100, Batch 150/657, Loss: 0.0000\n",
      "Epoch 82/100, Batch 200/657, Loss: 0.0001\n",
      "Epoch 82/100, Batch 250/657, Loss: 0.0001\n",
      "Epoch 82/100, Batch 300/657, Loss: 0.0001\n",
      "Epoch 82/100, Batch 350/657, Loss: 0.0004\n",
      "Epoch 82/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 82/100, Batch 450/657, Loss: 0.0002\n",
      "Epoch 82/100, Batch 500/657, Loss: 0.0002\n",
      "Epoch 82/100, Batch 550/657, Loss: 0.0001\n",
      "Epoch 82/100, Batch 600/657, Loss: 0.0001\n",
      "Epoch 82/100, Batch 650/657, Loss: 0.0024\n",
      "\n",
      "Epoch 82/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0743\n",
      "Substance Accuracy: 0.9955\n",
      "Symptom F1: 0.8904, Precision: 0.9335, Recall: 0.8511\n",
      "Learning Rate: 0.000011\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 83/100, Batch 0/657, Loss: 0.0000\n",
      "Epoch 83/100, Batch 50/657, Loss: 0.0001\n",
      "Epoch 83/100, Batch 100/657, Loss: 0.0003\n",
      "Epoch 83/100, Batch 150/657, Loss: 0.0003\n",
      "Epoch 83/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 83/100, Batch 250/657, Loss: 0.0000\n",
      "Epoch 83/100, Batch 300/657, Loss: 0.0002\n",
      "Epoch 83/100, Batch 350/657, Loss: 0.0001\n",
      "Epoch 83/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 83/100, Batch 450/657, Loss: 0.0000\n",
      "Epoch 83/100, Batch 500/657, Loss: 0.0000\n",
      "Epoch 83/100, Batch 550/657, Loss: 0.0000\n",
      "Epoch 83/100, Batch 600/657, Loss: 0.0000\n",
      "Epoch 83/100, Batch 650/657, Loss: 0.0000\n",
      "\n",
      "Epoch 83/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0756\n",
      "Substance Accuracy: 0.9960\n",
      "Symptom F1: 0.8874, Precision: 0.9322, Recall: 0.8466\n",
      "Learning Rate: 0.000010\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 84/100, Batch 0/657, Loss: 0.0008\n",
      "Epoch 84/100, Batch 50/657, Loss: 0.0006\n",
      "Epoch 84/100, Batch 100/657, Loss: 0.0000\n",
      "Epoch 84/100, Batch 150/657, Loss: 0.0000\n",
      "Epoch 84/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 84/100, Batch 250/657, Loss: 0.0001\n",
      "Epoch 84/100, Batch 300/657, Loss: 0.0006\n",
      "Epoch 84/100, Batch 350/657, Loss: 0.0000\n",
      "Epoch 84/100, Batch 400/657, Loss: 0.0008\n",
      "Epoch 84/100, Batch 450/657, Loss: 0.0000\n",
      "Epoch 84/100, Batch 500/657, Loss: 0.0000\n",
      "Epoch 84/100, Batch 550/657, Loss: 0.0002\n",
      "Epoch 84/100, Batch 600/657, Loss: 0.0000\n",
      "Epoch 84/100, Batch 650/657, Loss: 0.0002\n",
      "\n",
      "Epoch 84/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0683\n",
      "Substance Accuracy: 0.9970\n",
      "Symptom F1: 0.8925, Precision: 0.9361, Recall: 0.8527\n",
      "Learning Rate: 0.000008\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 85/100, Batch 0/657, Loss: 0.0000\n",
      "Epoch 85/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 85/100, Batch 100/657, Loss: 0.0011\n",
      "Epoch 85/100, Batch 150/657, Loss: 0.0000\n",
      "Epoch 85/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 85/100, Batch 250/657, Loss: 0.0006\n",
      "Epoch 85/100, Batch 300/657, Loss: 0.0002\n",
      "Epoch 85/100, Batch 350/657, Loss: 0.0003\n",
      "Epoch 85/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 85/100, Batch 450/657, Loss: 0.0003\n",
      "Epoch 85/100, Batch 500/657, Loss: 0.0004\n",
      "Epoch 85/100, Batch 550/657, Loss: 0.0006\n",
      "Epoch 85/100, Batch 600/657, Loss: 0.0001\n",
      "Epoch 85/100, Batch 650/657, Loss: 0.0001\n",
      "\n",
      "Epoch 85/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0727\n",
      "Substance Accuracy: 0.9977\n",
      "Symptom F1: 0.8918, Precision: 0.9351, Recall: 0.8525\n",
      "Learning Rate: 0.000007\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 86/100, Batch 0/657, Loss: 0.0001\n",
      "Epoch 86/100, Batch 50/657, Loss: 0.0003\n",
      "Epoch 86/100, Batch 100/657, Loss: 0.0001\n",
      "Epoch 86/100, Batch 150/657, Loss: 0.0000\n",
      "Epoch 86/100, Batch 200/657, Loss: 0.0001\n",
      "Epoch 86/100, Batch 250/657, Loss: 0.0009\n",
      "Epoch 86/100, Batch 300/657, Loss: 0.0000\n",
      "Epoch 86/100, Batch 350/657, Loss: 0.0000\n",
      "Epoch 86/100, Batch 400/657, Loss: 0.0006\n",
      "Epoch 86/100, Batch 450/657, Loss: 0.0003\n",
      "Epoch 86/100, Batch 500/657, Loss: 0.0001\n",
      "Epoch 86/100, Batch 550/657, Loss: 0.0014\n",
      "Epoch 86/100, Batch 600/657, Loss: 0.0001\n",
      "Epoch 86/100, Batch 650/657, Loss: 0.0000\n",
      "\n",
      "Epoch 86/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0630\n",
      "Substance Accuracy: 0.9977\n",
      "Symptom F1: 0.8941, Precision: 0.9358, Recall: 0.8559\n",
      "Learning Rate: 0.000007\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 87/100, Batch 0/657, Loss: 0.0001\n",
      "Epoch 87/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 87/100, Batch 100/657, Loss: 0.0000\n",
      "Epoch 87/100, Batch 150/657, Loss: 0.0001\n",
      "Epoch 87/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 87/100, Batch 250/657, Loss: 0.0001\n",
      "Epoch 87/100, Batch 300/657, Loss: 0.0000\n",
      "Epoch 87/100, Batch 350/657, Loss: 0.0011\n",
      "Epoch 87/100, Batch 400/657, Loss: 0.0001\n",
      "Epoch 87/100, Batch 450/657, Loss: 0.0000\n",
      "Epoch 87/100, Batch 500/657, Loss: 0.0000\n",
      "Epoch 87/100, Batch 550/657, Loss: 0.0001\n",
      "Epoch 87/100, Batch 600/657, Loss: 0.0002\n",
      "Epoch 87/100, Batch 650/657, Loss: 0.0003\n",
      "\n",
      "Epoch 87/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0668\n",
      "Substance Accuracy: 0.9963\n",
      "Symptom F1: 0.8942, Precision: 0.9374, Recall: 0.8548\n",
      "Learning Rate: 0.000006\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 88/100, Batch 0/657, Loss: 0.0000\n",
      "Epoch 88/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 88/100, Batch 100/657, Loss: 0.0000\n",
      "Epoch 88/100, Batch 150/657, Loss: 0.0000\n",
      "Epoch 88/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 88/100, Batch 250/657, Loss: 0.0001\n",
      "Epoch 88/100, Batch 300/657, Loss: 0.0001\n",
      "Epoch 88/100, Batch 350/657, Loss: 0.0001\n",
      "Epoch 88/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 88/100, Batch 450/657, Loss: 0.0000\n",
      "Epoch 88/100, Batch 500/657, Loss: 0.0002\n",
      "Epoch 88/100, Batch 550/657, Loss: 0.0002\n",
      "Epoch 88/100, Batch 600/657, Loss: 0.0002\n",
      "Epoch 88/100, Batch 650/657, Loss: 0.0000\n",
      "\n",
      "Epoch 88/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0690\n",
      "Substance Accuracy: 0.9980\n",
      "Symptom F1: 0.8934, Precision: 0.9358, Recall: 0.8546\n",
      "Learning Rate: 0.000005\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 89/100, Batch 0/657, Loss: 0.0011\n",
      "Epoch 89/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 89/100, Batch 100/657, Loss: 0.0005\n",
      "Epoch 89/100, Batch 150/657, Loss: 0.0001\n",
      "Epoch 89/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 89/100, Batch 250/657, Loss: 0.0009\n",
      "Epoch 89/100, Batch 300/657, Loss: 0.0001\n",
      "Epoch 89/100, Batch 350/657, Loss: 0.0002\n",
      "Epoch 89/100, Batch 400/657, Loss: 0.0002\n",
      "Epoch 89/100, Batch 450/657, Loss: 0.0003\n",
      "Epoch 89/100, Batch 500/657, Loss: 0.0000\n",
      "Epoch 89/100, Batch 550/657, Loss: 0.0000\n",
      "Epoch 89/100, Batch 600/657, Loss: 0.0000\n",
      "Epoch 89/100, Batch 650/657, Loss: 0.0001\n",
      "\n",
      "Epoch 89/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0736\n",
      "Substance Accuracy: 0.9965\n",
      "Symptom F1: 0.8935, Precision: 0.9353, Recall: 0.8554\n",
      "Learning Rate: 0.000004\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 90/100, Batch 0/657, Loss: 0.0001\n",
      "Epoch 90/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 90/100, Batch 100/657, Loss: 0.0000\n",
      "Epoch 90/100, Batch 150/657, Loss: 0.0000\n",
      "Epoch 90/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 90/100, Batch 250/657, Loss: 0.0005\n",
      "Epoch 90/100, Batch 300/657, Loss: 0.0000\n",
      "Epoch 90/100, Batch 350/657, Loss: 0.0005\n",
      "Epoch 90/100, Batch 400/657, Loss: 0.0010\n",
      "Epoch 90/100, Batch 450/657, Loss: 0.0000\n",
      "Epoch 90/100, Batch 500/657, Loss: 0.0000\n",
      "Epoch 90/100, Batch 550/657, Loss: 0.0003\n",
      "Epoch 90/100, Batch 600/657, Loss: 0.0000\n",
      "Epoch 90/100, Batch 650/657, Loss: 0.0010\n",
      "\n",
      "Epoch 90/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0728\n",
      "Substance Accuracy: 0.9972\n",
      "Symptom F1: 0.8909, Precision: 0.9375, Recall: 0.8487\n",
      "Learning Rate: 0.000003\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 91/100, Batch 0/657, Loss: 0.0000\n",
      "Epoch 91/100, Batch 50/657, Loss: 0.0001\n",
      "Epoch 91/100, Batch 100/657, Loss: 0.0006\n",
      "Epoch 91/100, Batch 150/657, Loss: 0.0000\n",
      "Epoch 91/100, Batch 200/657, Loss: 0.0004\n",
      "Epoch 91/100, Batch 250/657, Loss: 0.0001\n",
      "Epoch 91/100, Batch 300/657, Loss: 0.0000\n",
      "Epoch 91/100, Batch 350/657, Loss: 0.0000\n",
      "Epoch 91/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 91/100, Batch 450/657, Loss: 0.0007\n",
      "Epoch 91/100, Batch 500/657, Loss: 0.0000\n",
      "Epoch 91/100, Batch 550/657, Loss: 0.0020\n",
      "Epoch 91/100, Batch 600/657, Loss: 0.0000\n",
      "Epoch 91/100, Batch 650/657, Loss: 0.0000\n",
      "\n",
      "Epoch 91/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0745\n",
      "Substance Accuracy: 0.9960\n",
      "Symptom F1: 0.8920, Precision: 0.9392, Recall: 0.8493\n",
      "Learning Rate: 0.000003\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 92/100, Batch 0/657, Loss: 0.0000\n",
      "Epoch 92/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 92/100, Batch 100/657, Loss: 0.0003\n",
      "Epoch 92/100, Batch 150/657, Loss: 0.0001\n",
      "Epoch 92/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 92/100, Batch 250/657, Loss: 0.0002\n",
      "Epoch 92/100, Batch 300/657, Loss: 0.0000\n",
      "Epoch 92/100, Batch 350/657, Loss: 0.0001\n",
      "Epoch 92/100, Batch 400/657, Loss: 0.0004\n",
      "Epoch 92/100, Batch 450/657, Loss: 0.0000\n",
      "Epoch 92/100, Batch 500/657, Loss: 0.0000\n",
      "Epoch 92/100, Batch 550/657, Loss: 0.0000\n",
      "Epoch 92/100, Batch 600/657, Loss: 0.0009\n",
      "Epoch 92/100, Batch 650/657, Loss: 0.0001\n",
      "\n",
      "Epoch 92/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0799\n",
      "Substance Accuracy: 0.9947\n",
      "Symptom F1: 0.8934, Precision: 0.9379, Recall: 0.8529\n",
      "Learning Rate: 0.000002\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 93/100, Batch 0/657, Loss: 0.0001\n",
      "Epoch 93/100, Batch 50/657, Loss: 0.0001\n",
      "Epoch 93/100, Batch 100/657, Loss: 0.0002\n",
      "Epoch 93/100, Batch 150/657, Loss: 0.0000\n",
      "Epoch 93/100, Batch 200/657, Loss: 0.0013\n",
      "Epoch 93/100, Batch 250/657, Loss: 0.0000\n",
      "Epoch 93/100, Batch 300/657, Loss: 0.0005\n",
      "Epoch 93/100, Batch 350/657, Loss: 0.0015\n",
      "Epoch 93/100, Batch 400/657, Loss: 0.0012\n",
      "Epoch 93/100, Batch 450/657, Loss: 0.0002\n",
      "Epoch 93/100, Batch 500/657, Loss: 0.0000\n",
      "Epoch 93/100, Batch 550/657, Loss: 0.0000\n",
      "Epoch 93/100, Batch 600/657, Loss: 0.0000\n",
      "Epoch 93/100, Batch 650/657, Loss: 0.0000\n",
      "\n",
      "Epoch 93/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0823\n",
      "Substance Accuracy: 0.9946\n",
      "Symptom F1: 0.8935, Precision: 0.9401, Recall: 0.8512\n",
      "Learning Rate: 0.000002\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 94/100, Batch 0/657, Loss: 0.0000\n",
      "Epoch 94/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 94/100, Batch 100/657, Loss: 0.0001\n",
      "Epoch 94/100, Batch 150/657, Loss: 0.0003\n",
      "Epoch 94/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 94/100, Batch 250/657, Loss: 0.0000\n",
      "Epoch 94/100, Batch 300/657, Loss: 0.0001\n",
      "Epoch 94/100, Batch 350/657, Loss: 0.0000\n",
      "Epoch 94/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 94/100, Batch 450/657, Loss: 0.0001\n",
      "Epoch 94/100, Batch 500/657, Loss: 0.0000\n",
      "Epoch 94/100, Batch 550/657, Loss: 0.0001\n",
      "Epoch 94/100, Batch 600/657, Loss: 0.0000\n",
      "Epoch 94/100, Batch 650/657, Loss: 0.0000\n",
      "\n",
      "Epoch 94/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0643\n",
      "Substance Accuracy: 0.9978\n",
      "Symptom F1: 0.8948, Precision: 0.9360, Recall: 0.8571\n",
      "Learning Rate: 0.000001\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 95/100, Batch 0/657, Loss: 0.0024\n",
      "Epoch 95/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 95/100, Batch 100/657, Loss: 0.0000\n",
      "Epoch 95/100, Batch 150/657, Loss: 0.0000\n",
      "Epoch 95/100, Batch 200/657, Loss: 0.0002\n",
      "Epoch 95/100, Batch 250/657, Loss: 0.0002\n",
      "Epoch 95/100, Batch 300/657, Loss: 0.0001\n",
      "Epoch 95/100, Batch 350/657, Loss: 0.0001\n",
      "Epoch 95/100, Batch 400/657, Loss: 0.0003\n",
      "Epoch 95/100, Batch 450/657, Loss: 0.0000\n",
      "Epoch 95/100, Batch 500/657, Loss: 0.0000\n",
      "Epoch 95/100, Batch 550/657, Loss: 0.0011\n",
      "Epoch 95/100, Batch 600/657, Loss: 0.0003\n",
      "Epoch 95/100, Batch 650/657, Loss: 0.0002\n",
      "\n",
      "Epoch 95/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0734\n",
      "Substance Accuracy: 0.9983\n",
      "Symptom F1: 0.8913, Precision: 0.9343, Recall: 0.8520\n",
      "Learning Rate: 0.000001\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 96/100, Batch 0/657, Loss: 0.0000\n",
      "Epoch 96/100, Batch 50/657, Loss: 0.0002\n",
      "Epoch 96/100, Batch 100/657, Loss: 0.0001\n",
      "Epoch 96/100, Batch 150/657, Loss: 0.0001\n",
      "Epoch 96/100, Batch 200/657, Loss: 0.0002\n",
      "Epoch 96/100, Batch 250/657, Loss: 0.0000\n",
      "Epoch 96/100, Batch 300/657, Loss: 0.0002\n",
      "Epoch 96/100, Batch 350/657, Loss: 0.0000\n",
      "Epoch 96/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 96/100, Batch 450/657, Loss: 0.0000\n",
      "Epoch 96/100, Batch 500/657, Loss: 0.0002\n",
      "Epoch 96/100, Batch 550/657, Loss: 0.0003\n",
      "Epoch 96/100, Batch 600/657, Loss: 0.0000\n",
      "Epoch 96/100, Batch 650/657, Loss: 0.0000\n",
      "\n",
      "Epoch 96/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0726\n",
      "Substance Accuracy: 0.9974\n",
      "Symptom F1: 0.8923, Precision: 0.9361, Recall: 0.8525\n",
      "Learning Rate: 0.000001\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 97/100, Batch 0/657, Loss: 0.0002\n",
      "Epoch 97/100, Batch 50/657, Loss: 0.0001\n",
      "Epoch 97/100, Batch 100/657, Loss: 0.0009\n",
      "Epoch 97/100, Batch 150/657, Loss: 0.0013\n",
      "Epoch 97/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 97/100, Batch 250/657, Loss: 0.0000\n",
      "Epoch 97/100, Batch 300/657, Loss: 0.0003\n",
      "Epoch 97/100, Batch 350/657, Loss: 0.0000\n",
      "Epoch 97/100, Batch 400/657, Loss: 0.0003\n",
      "Epoch 97/100, Batch 450/657, Loss: 0.0000\n",
      "Epoch 97/100, Batch 500/657, Loss: 0.0007\n",
      "Epoch 97/100, Batch 550/657, Loss: 0.0000\n",
      "Epoch 97/100, Batch 600/657, Loss: 0.0000\n",
      "Epoch 97/100, Batch 650/657, Loss: 0.0001\n",
      "\n",
      "Epoch 97/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0803\n",
      "Substance Accuracy: 0.9960\n",
      "Symptom F1: 0.8899, Precision: 0.9354, Recall: 0.8487\n",
      "Learning Rate: 0.000000\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 98/100, Batch 0/657, Loss: 0.0000\n",
      "Epoch 98/100, Batch 50/657, Loss: 0.0000\n",
      "Epoch 98/100, Batch 100/657, Loss: 0.0001\n",
      "Epoch 98/100, Batch 150/657, Loss: 0.0001\n",
      "Epoch 98/100, Batch 200/657, Loss: 0.0001\n",
      "Epoch 98/100, Batch 250/657, Loss: 0.0002\n",
      "Epoch 98/100, Batch 300/657, Loss: 0.0001\n",
      "Epoch 98/100, Batch 350/657, Loss: 0.0011\n",
      "Epoch 98/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 98/100, Batch 450/657, Loss: 0.0000\n",
      "Epoch 98/100, Batch 500/657, Loss: 0.0000\n",
      "Epoch 98/100, Batch 550/657, Loss: 0.0000\n",
      "Epoch 98/100, Batch 600/657, Loss: 0.0001\n",
      "Epoch 98/100, Batch 650/657, Loss: 0.0000\n",
      "\n",
      "Epoch 98/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0659\n",
      "Substance Accuracy: 0.9968\n",
      "Symptom F1: 0.8917, Precision: 0.9343, Recall: 0.8528\n",
      "Learning Rate: 0.000000\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 99/100, Batch 0/657, Loss: 0.0000\n",
      "Epoch 99/100, Batch 50/657, Loss: 0.0008\n",
      "Epoch 99/100, Batch 100/657, Loss: 0.0000\n",
      "Epoch 99/100, Batch 150/657, Loss: 0.0000\n",
      "Epoch 99/100, Batch 200/657, Loss: 0.0000\n",
      "Epoch 99/100, Batch 250/657, Loss: 0.0000\n",
      "Epoch 99/100, Batch 300/657, Loss: 0.0000\n",
      "Epoch 99/100, Batch 350/657, Loss: 0.0005\n",
      "Epoch 99/100, Batch 400/657, Loss: 0.0001\n",
      "Epoch 99/100, Batch 450/657, Loss: 0.0000\n",
      "Epoch 99/100, Batch 500/657, Loss: 0.0000\n",
      "Epoch 99/100, Batch 550/657, Loss: 0.0000\n",
      "Epoch 99/100, Batch 600/657, Loss: 0.0000\n",
      "Epoch 99/100, Batch 650/657, Loss: 0.0017\n",
      "\n",
      "Epoch 99/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0724\n",
      "Substance Accuracy: 0.9969\n",
      "Symptom F1: 0.8946, Precision: 0.9369, Recall: 0.8558\n",
      "Learning Rate: 0.000000\n",
      "--------------------------------------------------------------------------------\n",
      "Epoch 100/100, Batch 0/657, Loss: 0.0011\n",
      "Epoch 100/100, Batch 50/657, Loss: 0.0001\n",
      "Epoch 100/100, Batch 100/657, Loss: 0.0006\n",
      "Epoch 100/100, Batch 150/657, Loss: 0.0000\n",
      "Epoch 100/100, Batch 200/657, Loss: 0.0003\n",
      "Epoch 100/100, Batch 250/657, Loss: 0.0000\n",
      "Epoch 100/100, Batch 300/657, Loss: 0.0013\n",
      "Epoch 100/100, Batch 350/657, Loss: 0.0009\n",
      "Epoch 100/100, Batch 400/657, Loss: 0.0000\n",
      "Epoch 100/100, Batch 450/657, Loss: 0.0003\n",
      "Epoch 100/100, Batch 500/657, Loss: 0.0001\n",
      "Epoch 100/100, Batch 550/657, Loss: 0.0000\n",
      "Epoch 100/100, Batch 600/657, Loss: 0.0001\n",
      "Epoch 100/100, Batch 650/657, Loss: 0.0005\n",
      "\n",
      "Epoch 100/100 Results:\n",
      "Train Loss: 0.0003, Val Loss: 0.0668\n",
      "Substance Accuracy: 0.9955\n",
      "Symptom F1: 0.8954, Precision: 0.9359, Recall: 0.8582\n",
      "Learning Rate: 0.000000\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "Loaded best model with symptom F1: 0.8954, Substance accuracy: 0.9967\n",
      "\n",
      "Training completed!\n",
      "\n",
      "================================================================================\n",
      "TRAINING SUMMARY\n",
      "================================================================================\n",
      "Total Epochs: 100\n",
      "Final Train Loss: 0.0003\n",
      "Final Val Loss: 0.0668\n",
      "Best Substance Accuracy: 0.9985\n",
      "Best Symptom F1: 0.8954\n",
      "\n",
      "Epoch-by-Epoch Progress:\n",
      "Epoch | Train Loss | Val Loss | Substance Acc | Symptom F1\n",
      "--------------------------------------------------------------------------------\n",
      "    1 |     0.2718 |   0.2913 |        0.7278 |     0.1832\n",
      "    2 |     0.1685 |   0.1846 |        0.7858 |     0.5049\n",
      "    3 |     0.0825 |   0.1227 |        0.8276 |     0.8247\n",
      "    4 |     0.0380 |   0.1025 |        0.8556 |     0.8524\n",
      "    5 |     0.0259 |   0.1035 |        0.8704 |     0.8621\n",
      "    6 |     0.0197 |   0.0849 |        0.8929 |     0.8682\n",
      "    7 |     0.0161 |   0.0665 |        0.9155 |     0.8725\n",
      "    8 |     0.0136 |   0.0489 |        0.9367 |     0.8791\n",
      "    9 |     0.0117 |   0.0453 |        0.9481 |     0.8783\n",
      "   10 |     0.0103 |   0.0536 |        0.9502 |     0.8823\n",
      "   11 |     0.0091 |   0.0489 |        0.9588 |     0.8777\n",
      "   12 |     0.0081 |   0.0488 |        0.9632 |     0.8880\n",
      "   13 |     0.0074 |   0.0419 |        0.9687 |     0.8842\n",
      "   14 |     0.0067 |   0.0356 |        0.9776 |     0.8804\n",
      "   15 |     0.0059 |   0.0401 |        0.9773 |     0.8792\n",
      "   16 |     0.0054 |   0.0414 |        0.9770 |     0.8834\n",
      "   17 |     0.0049 |   0.0350 |        0.9841 |     0.8842\n",
      "   18 |     0.0043 |   0.0442 |        0.9845 |     0.8791\n",
      "   19 |     0.0038 |   0.0433 |        0.9832 |     0.8788\n",
      "   20 |     0.0035 |   0.0452 |        0.9848 |     0.8748\n",
      "   21 |     0.0032 |   0.0544 |        0.9833 |     0.8714\n",
      "   22 |     0.0029 |   0.0593 |        0.9785 |     0.8753\n",
      "   23 |     0.0026 |   0.0667 |        0.9808 |     0.8698\n",
      "   24 |     0.0023 |   0.0537 |        0.9852 |     0.8835\n",
      "   25 |     0.0022 |   0.0474 |        0.9865 |     0.8842\n",
      "   26 |     0.0020 |   0.0488 |        0.9897 |     0.8831\n",
      "   27 |     0.0018 |   0.0621 |        0.9801 |     0.8817\n",
      "   28 |     0.0018 |   0.0548 |        0.9912 |     0.8778\n",
      "   29 |     0.0017 |   0.0650 |        0.9811 |     0.8825\n",
      "   30 |     0.0015 |   0.0534 |        0.9916 |     0.8778\n",
      "   31 |     0.0013 |   0.0635 |        0.9882 |     0.8844\n",
      "   32 |     0.0013 |   0.0590 |        0.9918 |     0.8796\n",
      "   33 |     0.0013 |   0.0564 |        0.9946 |     0.8806\n",
      "   34 |     0.0011 |   0.0558 |        0.9934 |     0.8796\n",
      "   35 |     0.0012 |   0.0543 |        0.9958 |     0.8852\n",
      "   36 |     0.0011 |   0.0619 |        0.9931 |     0.8800\n",
      "   37 |     0.0010 |   0.0506 |        0.9951 |     0.8886\n",
      "   38 |     0.0010 |   0.0579 |        0.9924 |     0.8768\n",
      "   39 |     0.0010 |   0.0586 |        0.9964 |     0.8856\n",
      "   40 |     0.0009 |   0.0550 |        0.9961 |     0.8856\n",
      "   41 |     0.0008 |   0.0568 |        0.9954 |     0.8893\n",
      "   42 |     0.0008 |   0.0619 |        0.9919 |     0.8883\n",
      "   43 |     0.0008 |   0.0560 |        0.9954 |     0.8870\n",
      "   44 |     0.0008 |   0.0588 |        0.9948 |     0.8838\n",
      "   45 |     0.0007 |   0.0599 |        0.9963 |     0.8882\n",
      "   46 |     0.0007 |   0.0558 |        0.9961 |     0.8835\n",
      "   47 |     0.0007 |   0.0602 |        0.9960 |     0.8885\n",
      "   48 |     0.0006 |   0.0631 |        0.9948 |     0.8882\n",
      "   49 |     0.0006 |   0.0617 |        0.9963 |     0.8885\n",
      "   50 |     0.0007 |   0.0639 |        0.9982 |     0.8861\n",
      "   51 |     0.0006 |   0.0694 |        0.9955 |     0.8836\n",
      "   52 |     0.0007 |   0.0519 |        0.9967 |     0.8954\n",
      "   53 |     0.0006 |   0.0627 |        0.9972 |     0.8858\n",
      "   54 |     0.0005 |   0.0667 |        0.9985 |     0.8854\n",
      "   55 |     0.0005 |   0.0630 |        0.9964 |     0.8937\n",
      "   56 |     0.0005 |   0.0627 |        0.9969 |     0.8860\n",
      "   57 |     0.0005 |   0.0695 |        0.9947 |     0.8887\n",
      "   58 |     0.0004 |   0.0651 |        0.9963 |     0.8896\n",
      "   59 |     0.0005 |   0.0585 |        0.9982 |     0.8933\n",
      "   60 |     0.0005 |   0.0576 |        0.9969 |     0.8889\n",
      "   61 |     0.0005 |   0.0627 |        0.9966 |     0.8915\n",
      "   62 |     0.0004 |   0.0726 |        0.9967 |     0.8828\n",
      "   63 |     0.0005 |   0.0626 |        0.9979 |     0.8914\n",
      "   64 |     0.0004 |   0.0739 |        0.9964 |     0.8844\n",
      "   65 |     0.0004 |   0.0688 |        0.9955 |     0.8900\n",
      "   66 |     0.0004 |   0.0721 |        0.9957 |     0.8820\n",
      "   67 |     0.0004 |   0.0708 |        0.9960 |     0.8841\n",
      "   68 |     0.0004 |   0.0702 |        0.9968 |     0.8890\n",
      "   69 |     0.0004 |   0.0704 |        0.9964 |     0.8893\n",
      "   70 |     0.0004 |   0.0688 |        0.9971 |     0.8894\n",
      "   71 |     0.0004 |   0.0666 |        0.9981 |     0.8895\n",
      "   72 |     0.0003 |   0.0651 |        0.9962 |     0.8922\n",
      "   73 |     0.0003 |   0.0707 |        0.9959 |     0.8905\n",
      "   74 |     0.0004 |   0.0769 |        0.9963 |     0.8864\n",
      "   75 |     0.0003 |   0.0662 |        0.9958 |     0.8896\n",
      "   76 |     0.0003 |   0.0724 |        0.9959 |     0.8858\n",
      "   77 |     0.0004 |   0.0615 |        0.9977 |     0.8947\n",
      "   78 |     0.0003 |   0.0720 |        0.9984 |     0.8906\n",
      "   79 |     0.0004 |   0.0685 |        0.9963 |     0.8953\n",
      "   80 |     0.0003 |   0.0783 |        0.9955 |     0.8907\n",
      "   81 |     0.0003 |   0.0751 |        0.9965 |     0.8889\n",
      "   82 |     0.0003 |   0.0743 |        0.9955 |     0.8904\n",
      "   83 |     0.0003 |   0.0756 |        0.9960 |     0.8874\n",
      "   84 |     0.0003 |   0.0683 |        0.9970 |     0.8925\n",
      "   85 |     0.0003 |   0.0727 |        0.9977 |     0.8918\n",
      "   86 |     0.0003 |   0.0630 |        0.9977 |     0.8941\n",
      "   87 |     0.0003 |   0.0668 |        0.9963 |     0.8942\n",
      "   88 |     0.0003 |   0.0690 |        0.9980 |     0.8934\n",
      "   89 |     0.0003 |   0.0736 |        0.9965 |     0.8935\n",
      "   90 |     0.0003 |   0.0728 |        0.9972 |     0.8909\n",
      "   91 |     0.0003 |   0.0745 |        0.9960 |     0.8920\n",
      "   92 |     0.0003 |   0.0799 |        0.9947 |     0.8934\n",
      "   93 |     0.0003 |   0.0823 |        0.9946 |     0.8935\n",
      "   94 |     0.0003 |   0.0643 |        0.9978 |     0.8948\n",
      "   95 |     0.0003 |   0.0734 |        0.9983 |     0.8913\n",
      "   96 |     0.0003 |   0.0726 |        0.9974 |     0.8923\n",
      "   97 |     0.0003 |   0.0803 |        0.9960 |     0.8899\n",
      "   98 |     0.0003 |   0.0659 |        0.9968 |     0.8917\n",
      "   99 |     0.0003 |   0.0724 |        0.9969 |     0.8946\n",
      "  100 |     0.0003 |   0.0668 |        0.9955 |     0.8954\n",
      "Training history saved to training_history.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# Custom training function with enhancements for >95% substance accuracy and >91% symptom F1\n",
    "def train_model(model, train_dataset, test_dataset, num_epochs=100, batch_size=64, learning_rate=1e-4):\n",
    "    # Create data loaders with increased batch size\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    # Optimizer with weight decay and gradient clipping\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    # Learning rate scheduler with warmup\n",
    "    warmup_epochs = 15\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return float(epoch + 1) / warmup_epochs  # Linear warmup\n",
    "        return 0.5 * (1 + np.cos(np.pi * (epoch - warmup_epochs) / (num_epochs - warmup_epochs)))  # Cosine decay\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    # Compute class weights for symptom loss to address imbalance\n",
    "    symptom_labels = torch.cat([batch['symptom_labels'] for batch in train_loader], dim=0)\n",
    "    class_counts = symptom_labels.sum(dim=0).cpu().numpy()\n",
    "    class_weights = np.where(class_counts > 0, len(symptom_labels) / (len(class_counts) * class_counts), 1.0)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'substance_acc': [], 'symptom_f1': [],\n",
    "        'symptom_precision': [], 'symptom_recall': []\n",
    "    }\n",
    "    \n",
    "    best_substance_acc = 0.0\n",
    "    best_symptom_f1 = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    print(f\"Total epochs: {num_epochs}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Initial learning rate: {learning_rate}\")\n",
    "    print(f\"Total training batches: {len(train_loader)}\")\n",
    "    print(f\"Total validation batches: {len(test_loader)}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Move data to device\n",
    "            x = batch['x'].to(device)\n",
    "            substance_labels = batch['substance_labels'].to(device)\n",
    "            symptom_labels = batch['symptom_labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(x, substance_labels=substance_labels, symptom_labels=symptom_labels)\n",
    "            loss = outputs['loss']\n",
    "            \n",
    "            # Apply class weights to symptom loss (assuming model outputs separate losses)\n",
    "            if 'symptom_loss' in outputs:\n",
    "                symptom_loss = outputs['symptom_loss']\n",
    "                weighted_symptom_loss = (symptom_loss * class_weights).mean()\n",
    "                loss = outputs['substance_loss'] + weighted_symptom_loss\n",
    "            \n",
    "            # Backward pass with gradient clipping\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            \n",
    "            # Log progress every 50 batches\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, \"\n",
    "                      f\"Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        all_substance_preds = []\n",
    "        all_substance_labels = []\n",
    "        all_symptom_preds = []\n",
    "        all_symptom_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                x = batch['x'].to(device)\n",
    "                substance_labels = batch['substance_labels'].to(device)\n",
    "                symptom_labels = batch['symptom_labels'].to(device)\n",
    "                \n",
    "                outputs = model(x, substance_labels=substance_labels, symptom_labels=symptom_labels)\n",
    "                loss = outputs['loss']\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "                \n",
    "                # Collect predictions\n",
    "                substance_preds = torch.argmax(outputs['substance_logits'], dim=1)\n",
    "                symptom_preds = (torch.sigmoid(outputs['symptom_logits']) > 0.5).float()\n",
    "                \n",
    "                all_substance_preds.extend(substance_preds.cpu().numpy())\n",
    "                all_substance_labels.extend(substance_labels.cpu().numpy())\n",
    "                all_symptom_preds.extend(symptom_preds.cpu().numpy())\n",
    "                all_symptom_labels.extend(symptom_labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        \n",
    "        # Calculate metrics\n",
    "        all_substance_preds = np.array(all_substance_preds)\n",
    "        all_substance_labels = np.array(all_substance_labels)\n",
    "        all_symptom_preds = np.array(all_symptom_preds)\n",
    "        all_symptom_labels = np.array(all_symptom_labels)\n",
    "        \n",
    "        substance_accuracy = accuracy_score(all_substance_labels, all_substance_preds)\n",
    "        symptom_f1 = f1_score(all_symptom_labels, all_symptom_preds, average='micro', zero_division=0)\n",
    "        symptom_precision = precision_score(all_symptom_labels, all_symptom_preds, average='micro', zero_division=0)\n",
    "        symptom_recall = recall_score(all_symptom_labels, all_symptom_preds, average='micro', zero_division=0)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save best model based on symptom F1\n",
    "        if symptom_f1 > best_symptom_f1:\n",
    "            best_symptom_f1 = symptom_f1\n",
    "            best_substance_acc = substance_accuracy\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            torch.save(best_model_state, 'best_model.pth')\n",
    "            print(f\"New best model saved! Symptom F1: {best_symptom_f1:.4f}, Substance Accuracy: {best_substance_acc:.4f}\")\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['substance_acc'].append(substance_accuracy)\n",
    "        history['symptom_f1'].append(symptom_f1)\n",
    "        history['symptom_precision'].append(symptom_precision)\n",
    "        history['symptom_recall'].append(symptom_recall)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} Results:\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Substance Accuracy: {substance_accuracy:.4f}\")\n",
    "        print(f\"Symptom F1: {symptom_f1:.4f}, Precision: {symptom_precision:.4f}, Recall: {symptom_recall:.4f}\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Early stopping: continue until at least 50 epochs and target metrics are met\n",
    "        if epoch >= 49 and substance_accuracy >= 0.95 and symptom_f1 >= 0.91:\n",
    "            print(f\"Target metrics met at epoch {epoch+1}! Substance accuracy: {substance_accuracy:.4f}, Symptom F1: {symptom_f1:.4f}\")\n",
    "            break\n",
    "        \n",
    "        # Clear cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"\\nLoaded best model with symptom F1: {best_symptom_f1:.4f}, Substance accuracy: {best_substance_acc:.4f}\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# Enhanced evaluation function\n",
    "def evaluate_model(model, test_dataset, batch_size=64):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    all_substance_preds = []\n",
    "    all_substance_labels = []\n",
    "    all_symptom_preds = []\n",
    "    all_symptom_labels = []\n",
    "    all_substance_probs = []\n",
    "    all_symptom_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x = batch['x'].to(device)\n",
    "            substance_labels = batch['substance_labels'].to(device)\n",
    "            symptom_labels = batch['symptom_labels'].to(device)\n",
    "            \n",
    "            outputs = model(x)\n",
    "            \n",
    "            substance_preds = torch.argmax(outputs['substance_logits'], dim=1)\n",
    "            symptom_preds = (torch.sigmoid(outputs['symptom_logits']) > 0.5).float()\n",
    "            \n",
    "            all_substance_preds.extend(substance_preds.cpu().numpy())\n",
    "            all_substance_labels.extend(substance_labels.cpu().numpy())\n",
    "            all_symptom_preds.extend(symptom_preds.cpu().numpy())\n",
    "            all_symptom_labels.extend(symptom_labels.cpu().numpy())\n",
    "            all_substance_probs.extend(torch.softmax(outputs['substance_logits'], dim=1).cpu().numpy())\n",
    "            all_symptom_probs.extend(torch.sigmoid(outputs['symptom_logits']).cpu().numpy())\n",
    "\n",
    "    return {\n",
    "        'substance_preds': np.array(all_substance_preds),\n",
    "        'substance_labels': np.array(all_substance_labels),\n",
    "        'symptom_preds': np.array(all_symptom_preds),\n",
    "        'symptom_labels': np.array(all_symptom_labels),\n",
    "        'substance_probs': np.array(all_substance_probs),\n",
    "        'symptom_probs': np.array(all_symptom_probs)\n",
    "    }\n",
    "\n",
    "# Simple text-based visualization function\n",
    "def print_training_summary(history):\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    epochs = len(history['train_loss'])\n",
    "    \n",
    "    print(f\"Total Epochs: {epochs}\")\n",
    "    print(f\"Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"Best Substance Accuracy: {max(history['substance_acc']):.4f}\")\n",
    "    print(f\"Best Symptom F1: {max(history['symptom_f1']):.4f}\")\n",
    "    \n",
    "    print(\"\\nEpoch-by-Epoch Progress:\")\n",
    "    print(\"Epoch | Train Loss | Val Loss | Substance Acc | Symptom F1\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        print(f\"{i+1:5d} | {history['train_loss'][i]:10.4f} | {history['val_loss'][i]:8.4f} | \"\n",
    "              f\"{history['substance_acc'][i]:13.4f} | {history['symptom_f1'][i]:10.4f}\")\n",
    "\n",
    "# Save training history to CSV\n",
    "def save_training_history(history, filename='training_history.csv'):\n",
    "    import csv\n",
    "    \n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['epoch'] + list(history.keys())\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for i in range(len(history['train_loss'])):\n",
    "            row = {'epoch': i+1}\n",
    "            for key in history.keys():\n",
    "                row[key] = history[key][i]\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    print(f\"Training history saved to {filename}\")\n",
    "\n",
    "# Clear memory before training\n",
    "torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "gc.collect()\n",
    "\n",
    "# Train the model with optimized parameters\n",
    "print(\"Starting model training...\")\n",
    "trained_model, training_history = train_model(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    num_epochs=100,  # Keep max epochs at 100\n",
    "    batch_size=64,   # Increased for better generalization\n",
    "    learning_rate=1e-4  # Lowered for finer convergence\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print_training_summary(training_history)\n",
    "save_training_history(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5bdb3c",
   "metadata": {},
   "source": [
    "6. Evaluate Model\n",
    "\n",
    "Evaluate and print results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33040f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating trained model...\n",
      "Final Evaluation Results:\n",
      "Substance Accuracy: 0.9955\n",
      "Symptom F1 Score: 0.8954\n",
      "Symptom Precision: 0.9359\n",
      "Symptom Recall: 0.8582\n",
      "\n",
      "Substance Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        none       1.00      0.99      0.99      3676\n",
      "      opioid       0.99      1.00      0.99      3502\n",
      "   stimulant       1.00      1.00      1.00      3328\n",
      "\n",
      "    accuracy                           1.00     10506\n",
      "   macro avg       1.00      1.00      1.00     10506\n",
      "weighted avg       1.00      1.00      1.00     10506\n",
      "\n",
      "\n",
      "Symptom Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "adverse_event       0.93      0.94      0.94      4559\n",
      "      anxiety       0.10      0.40      0.16         5\n",
      "    confusion       0.98      0.99      0.99       628\n",
      " constipation       0.25      0.50      0.33         2\n",
      "    dizziness       0.92      1.00      0.96        35\n",
      "   drowsiness       0.50      0.67      0.57         3\n",
      "      dyspnea       0.16      0.36      0.22        14\n",
      "      fatigue       0.27      1.00      0.42         4\n",
      "     headache       0.37      0.89      0.52        19\n",
      "     hematoma       0.00      0.00      0.00         6\n",
      "       nausea       0.95      0.98      0.97       102\n",
      "         none       0.97      0.72      0.83      4326\n",
      "     overdose       0.99      0.98      0.98       244\n",
      "         pain       0.93      0.98      0.96       530\n",
      "     pruritus       0.21      0.80      0.33         5\n",
      "         rash       0.79      1.00      0.88        30\n",
      "      seizure       0.91      1.00      0.95        79\n",
      "     vomiting       0.94      1.00      0.97        16\n",
      "\n",
      "    micro avg       0.94      0.86      0.90     10607\n",
      "    macro avg       0.62      0.79      0.67     10607\n",
      " weighted avg       0.95      0.86      0.89     10607\n",
      "  samples avg       0.86      0.86      0.86     10607\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TRAINING HISTORY VISUALIZATION\n",
      "================================================================================\n",
      "\n",
      "📉 LOSS TRENDS:\n",
      "--------------------------------------------------\n",
      "E 1 Train: ███████████████████████████    0.2718\n",
      "    Val  : ██████████████████████████████ 0.2913\n",
      "\n",
      "E 2 Train: █████████████████              0.1685\n",
      "    Val  : ███████████████████            0.1846\n",
      "\n",
      "E 3 Train: ████████                       0.0825\n",
      "    Val  : ████████████                   0.1227\n",
      "\n",
      "E 4 Train: ███                            0.0380\n",
      "    Val  : ██████████                     0.1025\n",
      "\n",
      "E 5 Train: ██                             0.0259\n",
      "    Val  : ██████████                     0.1035\n",
      "\n",
      "E 6 Train: ██                             0.0197\n",
      "    Val  : ████████                       0.0849\n",
      "\n",
      "E 7 Train: █                              0.0161\n",
      "    Val  : ██████                         0.0665\n",
      "\n",
      "E 8 Train: █                              0.0136\n",
      "    Val  : █████                          0.0489\n",
      "\n",
      "E 9 Train: █                              0.0117\n",
      "    Val  : ████                           0.0453\n",
      "\n",
      "E10 Train: █                              0.0103\n",
      "    Val  : █████                          0.0536\n",
      "\n",
      "E11 Train:                                0.0091\n",
      "    Val  : █████                          0.0489\n",
      "\n",
      "E12 Train:                                0.0081\n",
      "    Val  : █████                          0.0488\n",
      "\n",
      "E13 Train:                                0.0074\n",
      "    Val  : ████                           0.0419\n",
      "\n",
      "E14 Train:                                0.0067\n",
      "    Val  : ███                            0.0356\n",
      "\n",
      "E15 Train:                                0.0059\n",
      "    Val  : ████                           0.0401\n",
      "\n",
      "E16 Train:                                0.0054\n",
      "    Val  : ████                           0.0414\n",
      "\n",
      "E17 Train:                                0.0049\n",
      "    Val  : ███                            0.0350\n",
      "\n",
      "E18 Train:                                0.0043\n",
      "    Val  : ████                           0.0442\n",
      "\n",
      "E19 Train:                                0.0038\n",
      "    Val  : ████                           0.0433\n",
      "\n",
      "E20 Train:                                0.0035\n",
      "    Val  : ████                           0.0452\n",
      "\n",
      "E21 Train:                                0.0032\n",
      "    Val  : █████                          0.0544\n",
      "\n",
      "E22 Train:                                0.0029\n",
      "    Val  : ██████                         0.0593\n",
      "\n",
      "E23 Train:                                0.0026\n",
      "    Val  : ██████                         0.0667\n",
      "\n",
      "E24 Train:                                0.0023\n",
      "    Val  : █████                          0.0537\n",
      "\n",
      "E25 Train:                                0.0022\n",
      "    Val  : ████                           0.0474\n",
      "\n",
      "E26 Train:                                0.0020\n",
      "    Val  : █████                          0.0488\n",
      "\n",
      "E27 Train:                                0.0018\n",
      "    Val  : ██████                         0.0621\n",
      "\n",
      "E28 Train:                                0.0018\n",
      "    Val  : █████                          0.0548\n",
      "\n",
      "E29 Train:                                0.0017\n",
      "    Val  : ██████                         0.0650\n",
      "\n",
      "E30 Train:                                0.0015\n",
      "    Val  : █████                          0.0534\n",
      "\n",
      "E31 Train:                                0.0013\n",
      "    Val  : ██████                         0.0635\n",
      "\n",
      "E32 Train:                                0.0013\n",
      "    Val  : ██████                         0.0590\n",
      "\n",
      "E33 Train:                                0.0013\n",
      "    Val  : █████                          0.0564\n",
      "\n",
      "E34 Train:                                0.0011\n",
      "    Val  : █████                          0.0558\n",
      "\n",
      "E35 Train:                                0.0012\n",
      "    Val  : █████                          0.0543\n",
      "\n",
      "E36 Train:                                0.0011\n",
      "    Val  : ██████                         0.0619\n",
      "\n",
      "E37 Train:                                0.0010\n",
      "    Val  : █████                          0.0506\n",
      "\n",
      "E38 Train:                                0.0010\n",
      "    Val  : █████                          0.0579\n",
      "\n",
      "E39 Train:                                0.0010\n",
      "    Val  : ██████                         0.0586\n",
      "\n",
      "E40 Train:                                0.0009\n",
      "    Val  : █████                          0.0550\n",
      "\n",
      "E41 Train:                                0.0008\n",
      "    Val  : █████                          0.0568\n",
      "\n",
      "E42 Train:                                0.0008\n",
      "    Val  : ██████                         0.0619\n",
      "\n",
      "E43 Train:                                0.0008\n",
      "    Val  : █████                          0.0560\n",
      "\n",
      "E44 Train:                                0.0008\n",
      "    Val  : ██████                         0.0588\n",
      "\n",
      "E45 Train:                                0.0007\n",
      "    Val  : ██████                         0.0599\n",
      "\n",
      "E46 Train:                                0.0007\n",
      "    Val  : █████                          0.0558\n",
      "\n",
      "E47 Train:                                0.0007\n",
      "    Val  : ██████                         0.0602\n",
      "\n",
      "E48 Train:                                0.0006\n",
      "    Val  : ██████                         0.0631\n",
      "\n",
      "E49 Train:                                0.0006\n",
      "    Val  : ██████                         0.0617\n",
      "\n",
      "E50 Train:                                0.0007\n",
      "    Val  : ██████                         0.0639\n",
      "\n",
      "E51 Train:                                0.0006\n",
      "    Val  : ███████                        0.0694\n",
      "\n",
      "E52 Train:                                0.0007\n",
      "    Val  : █████                          0.0519\n",
      "\n",
      "E53 Train:                                0.0006\n",
      "    Val  : ██████                         0.0627\n",
      "\n",
      "E54 Train:                                0.0005\n",
      "    Val  : ██████                         0.0667\n",
      "\n",
      "E55 Train:                                0.0005\n",
      "    Val  : ██████                         0.0630\n",
      "\n",
      "E56 Train:                                0.0005\n",
      "    Val  : ██████                         0.0627\n",
      "\n",
      "E57 Train:                                0.0005\n",
      "    Val  : ███████                        0.0695\n",
      "\n",
      "E58 Train:                                0.0004\n",
      "    Val  : ██████                         0.0651\n",
      "\n",
      "E59 Train:                                0.0005\n",
      "    Val  : ██████                         0.0585\n",
      "\n",
      "E60 Train:                                0.0005\n",
      "    Val  : █████                          0.0576\n",
      "\n",
      "E61 Train:                                0.0005\n",
      "    Val  : ██████                         0.0627\n",
      "\n",
      "E62 Train:                                0.0004\n",
      "    Val  : ███████                        0.0726\n",
      "\n",
      "E63 Train:                                0.0005\n",
      "    Val  : ██████                         0.0626\n",
      "\n",
      "E64 Train:                                0.0004\n",
      "    Val  : ███████                        0.0739\n",
      "\n",
      "E65 Train:                                0.0004\n",
      "    Val  : ███████                        0.0688\n",
      "\n",
      "E66 Train:                                0.0004\n",
      "    Val  : ███████                        0.0721\n",
      "\n",
      "E67 Train:                                0.0004\n",
      "    Val  : ███████                        0.0708\n",
      "\n",
      "E68 Train:                                0.0004\n",
      "    Val  : ███████                        0.0702\n",
      "\n",
      "E69 Train:                                0.0004\n",
      "    Val  : ███████                        0.0704\n",
      "\n",
      "E70 Train:                                0.0004\n",
      "    Val  : ███████                        0.0688\n",
      "\n",
      "E71 Train:                                0.0004\n",
      "    Val  : ██████                         0.0666\n",
      "\n",
      "E72 Train:                                0.0003\n",
      "    Val  : ██████                         0.0651\n",
      "\n",
      "E73 Train:                                0.0003\n",
      "    Val  : ███████                        0.0707\n",
      "\n",
      "E74 Train:                                0.0004\n",
      "    Val  : ███████                        0.0769\n",
      "\n",
      "E75 Train:                                0.0003\n",
      "    Val  : ██████                         0.0662\n",
      "\n",
      "E76 Train:                                0.0003\n",
      "    Val  : ███████                        0.0724\n",
      "\n",
      "E77 Train:                                0.0004\n",
      "    Val  : ██████                         0.0615\n",
      "\n",
      "E78 Train:                                0.0003\n",
      "    Val  : ███████                        0.0720\n",
      "\n",
      "E79 Train:                                0.0004\n",
      "    Val  : ███████                        0.0685\n",
      "\n",
      "E80 Train:                                0.0003\n",
      "    Val  : ████████                       0.0783\n",
      "\n",
      "E81 Train:                                0.0003\n",
      "    Val  : ███████                        0.0751\n",
      "\n",
      "E82 Train:                                0.0003\n",
      "    Val  : ███████                        0.0743\n",
      "\n",
      "E83 Train:                                0.0003\n",
      "    Val  : ███████                        0.0756\n",
      "\n",
      "E84 Train:                                0.0003\n",
      "    Val  : ███████                        0.0683\n",
      "\n",
      "E85 Train:                                0.0003\n",
      "    Val  : ███████                        0.0727\n",
      "\n",
      "E86 Train:                                0.0003\n",
      "    Val  : ██████                         0.0630\n",
      "\n",
      "E87 Train:                                0.0003\n",
      "    Val  : ██████                         0.0668\n",
      "\n",
      "E88 Train:                                0.0003\n",
      "    Val  : ███████                        0.0690\n",
      "\n",
      "E89 Train:                                0.0003\n",
      "    Val  : ███████                        0.0736\n",
      "\n",
      "E90 Train:                                0.0003\n",
      "    Val  : ███████                        0.0728\n",
      "\n",
      "E91 Train:                                0.0003\n",
      "    Val  : ███████                        0.0745\n",
      "\n",
      "E92 Train:                                0.0003\n",
      "    Val  : ████████                       0.0799\n",
      "\n",
      "E93 Train:                                0.0003\n",
      "    Val  : ████████                       0.0823\n",
      "\n",
      "E94 Train:                                0.0003\n",
      "    Val  : ██████                         0.0643\n",
      "\n",
      "E95 Train:                                0.0003\n",
      "    Val  : ███████                        0.0734\n",
      "\n",
      "E96 Train:                                0.0003\n",
      "    Val  : ███████                        0.0726\n",
      "\n",
      "E97 Train:                                0.0003\n",
      "    Val  : ████████                       0.0803\n",
      "\n",
      "E98 Train:                                0.0003\n",
      "    Val  : ██████                         0.0659\n",
      "\n",
      "E99 Train:                                0.0003\n",
      "    Val  : ███████                        0.0724\n",
      "\n",
      "E100 Train:                                0.0003\n",
      "    Val  : ██████                         0.0668\n",
      "\n",
      "\n",
      "📊 SUBSTANCE ACCURACY TRENDS:\n",
      "--------------------------------------------------\n",
      "E 1: █████████████████████████████            0.7278\n",
      "E 2: ███████████████████████████████          0.7858\n",
      "E 3: █████████████████████████████████        0.8276\n",
      "E 4: ██████████████████████████████████       0.8556\n",
      "E 5: ██████████████████████████████████       0.8704\n",
      "E 6: ███████████████████████████████████      0.8929\n",
      "E 7: ████████████████████████████████████     0.9155\n",
      "E 8: █████████████████████████████████████    0.9367\n",
      "E 9: █████████████████████████████████████    0.9481\n",
      "E10: ██████████████████████████████████████   0.9502\n",
      "E11: ██████████████████████████████████████   0.9588\n",
      "E12: ██████████████████████████████████████   0.9632\n",
      "E13: ██████████████████████████████████████   0.9687\n",
      "E14: ███████████████████████████████████████  0.9776\n",
      "E15: ███████████████████████████████████████  0.9773\n",
      "E16: ███████████████████████████████████████  0.9770\n",
      "E17: ███████████████████████████████████████  0.9841\n",
      "E18: ███████████████████████████████████████  0.9845\n",
      "E19: ███████████████████████████████████████  0.9832\n",
      "E20: ███████████████████████████████████████  0.9848\n",
      "E21: ███████████████████████████████████████  0.9833\n",
      "E22: ███████████████████████████████████████  0.9785\n",
      "E23: ███████████████████████████████████████  0.9808\n",
      "E24: ███████████████████████████████████████  0.9852\n",
      "E25: ███████████████████████████████████████  0.9865\n",
      "E26: ███████████████████████████████████████  0.9897\n",
      "E27: ███████████████████████████████████████  0.9801\n",
      "E28: ███████████████████████████████████████  0.9912\n",
      "E29: ███████████████████████████████████████  0.9811\n",
      "E30: ███████████████████████████████████████  0.9916\n",
      "E31: ███████████████████████████████████████  0.9882\n",
      "E32: ███████████████████████████████████████  0.9918\n",
      "E33: ███████████████████████████████████████  0.9946\n",
      "E34: ███████████████████████████████████████  0.9934\n",
      "E35: ███████████████████████████████████████  0.9958\n",
      "E36: ███████████████████████████████████████  0.9931\n",
      "E37: ███████████████████████████████████████  0.9951\n",
      "E38: ███████████████████████████████████████  0.9924\n",
      "E39: ███████████████████████████████████████  0.9964\n",
      "E40: ███████████████████████████████████████  0.9961\n",
      "E41: ███████████████████████████████████████  0.9954\n",
      "E42: ███████████████████████████████████████  0.9919\n",
      "E43: ███████████████████████████████████████  0.9954\n",
      "E44: ███████████████████████████████████████  0.9948\n",
      "E45: ███████████████████████████████████████  0.9963\n",
      "E46: ███████████████████████████████████████  0.9961\n",
      "E47: ███████████████████████████████████████  0.9960\n",
      "E48: ███████████████████████████████████████  0.9948\n",
      "E49: ███████████████████████████████████████  0.9963\n",
      "E50: ███████████████████████████████████████  0.9982\n",
      "E51: ███████████████████████████████████████  0.9955\n",
      "E52: ███████████████████████████████████████  0.9967\n",
      "E53: ███████████████████████████████████████  0.9972\n",
      "E54: ███████████████████████████████████████  0.9985\n",
      "E55: ███████████████████████████████████████  0.9964\n",
      "E56: ███████████████████████████████████████  0.9969\n",
      "E57: ███████████████████████████████████████  0.9947\n",
      "E58: ███████████████████████████████████████  0.9963\n",
      "E59: ███████████████████████████████████████  0.9982\n",
      "E60: ███████████████████████████████████████  0.9969\n",
      "E61: ███████████████████████████████████████  0.9966\n",
      "E62: ███████████████████████████████████████  0.9967\n",
      "E63: ███████████████████████████████████████  0.9979\n",
      "E64: ███████████████████████████████████████  0.9964\n",
      "E65: ███████████████████████████████████████  0.9955\n",
      "E66: ███████████████████████████████████████  0.9957\n",
      "E67: ███████████████████████████████████████  0.9960\n",
      "E68: ███████████████████████████████████████  0.9968\n",
      "E69: ███████████████████████████████████████  0.9964\n",
      "E70: ███████████████████████████████████████  0.9971\n",
      "E71: ███████████████████████████████████████  0.9981\n",
      "E72: ███████████████████████████████████████  0.9962\n",
      "E73: ███████████████████████████████████████  0.9959\n",
      "E74: ███████████████████████████████████████  0.9963\n",
      "E75: ███████████████████████████████████████  0.9958\n",
      "E76: ███████████████████████████████████████  0.9959\n",
      "E77: ███████████████████████████████████████  0.9977\n",
      "E78: ███████████████████████████████████████  0.9984\n",
      "E79: ███████████████████████████████████████  0.9963\n",
      "E80: ███████████████████████████████████████  0.9955\n",
      "E81: ███████████████████████████████████████  0.9965\n",
      "E82: ███████████████████████████████████████  0.9955\n",
      "E83: ███████████████████████████████████████  0.9960\n",
      "E84: ███████████████████████████████████████  0.9970\n",
      "E85: ███████████████████████████████████████  0.9977\n",
      "E86: ███████████████████████████████████████  0.9977\n",
      "E87: ███████████████████████████████████████  0.9963\n",
      "E88: ███████████████████████████████████████  0.9980\n",
      "E89: ███████████████████████████████████████  0.9965\n",
      "E90: ███████████████████████████████████████  0.9972\n",
      "E91: ███████████████████████████████████████  0.9960\n",
      "E92: ███████████████████████████████████████  0.9947\n",
      "E93: ███████████████████████████████████████  0.9946\n",
      "E94: ███████████████████████████████████████  0.9978\n",
      "E95: ███████████████████████████████████████  0.9983\n",
      "E96: ███████████████████████████████████████  0.9974\n",
      "E97: ███████████████████████████████████████  0.9960\n",
      "E98: ███████████████████████████████████████  0.9968\n",
      "E99: ███████████████████████████████████████  0.9969\n",
      "E100: ███████████████████████████████████████  0.9955\n",
      "\n",
      "🎯 SYMPTOM F1 SCORE TRENDS:\n",
      "--------------------------------------------------\n",
      "E 1: ████████                                 0.1832\n",
      "E 2: ██████████████████████                   0.5049\n",
      "E 3: ████████████████████████████████████     0.8247\n",
      "E 4: ██████████████████████████████████████   0.8524\n",
      "E 5: ██████████████████████████████████████   0.8621\n",
      "E 6: ██████████████████████████████████████   0.8682\n",
      "E 7: ██████████████████████████████████████   0.8725\n",
      "E 8: ███████████████████████████████████████  0.8791\n",
      "E 9: ███████████████████████████████████████  0.8783\n",
      "E10: ███████████████████████████████████████  0.8823\n",
      "E11: ███████████████████████████████████████  0.8777\n",
      "E12: ███████████████████████████████████████  0.8880\n",
      "E13: ███████████████████████████████████████  0.8842\n",
      "E14: ███████████████████████████████████████  0.8804\n",
      "E15: ███████████████████████████████████████  0.8792\n",
      "E16: ███████████████████████████████████████  0.8834\n",
      "E17: ███████████████████████████████████████  0.8842\n",
      "E18: ███████████████████████████████████████  0.8791\n",
      "E19: ███████████████████████████████████████  0.8788\n",
      "E20: ███████████████████████████████████████  0.8748\n",
      "E21: ██████████████████████████████████████   0.8714\n",
      "E22: ███████████████████████████████████████  0.8753\n",
      "E23: ██████████████████████████████████████   0.8698\n",
      "E24: ███████████████████████████████████████  0.8835\n",
      "E25: ███████████████████████████████████████  0.8842\n",
      "E26: ███████████████████████████████████████  0.8831\n",
      "E27: ███████████████████████████████████████  0.8817\n",
      "E28: ███████████████████████████████████████  0.8778\n",
      "E29: ███████████████████████████████████████  0.8825\n",
      "E30: ███████████████████████████████████████  0.8778\n",
      "E31: ███████████████████████████████████████  0.8844\n",
      "E32: ███████████████████████████████████████  0.8796\n",
      "E33: ███████████████████████████████████████  0.8806\n",
      "E34: ███████████████████████████████████████  0.8796\n",
      "E35: ███████████████████████████████████████  0.8852\n",
      "E36: ███████████████████████████████████████  0.8800\n",
      "E37: ███████████████████████████████████████  0.8886\n",
      "E38: ███████████████████████████████████████  0.8768\n",
      "E39: ███████████████████████████████████████  0.8856\n",
      "E40: ███████████████████████████████████████  0.8856\n",
      "E41: ███████████████████████████████████████  0.8893\n",
      "E42: ███████████████████████████████████████  0.8883\n",
      "E43: ███████████████████████████████████████  0.8870\n",
      "E44: ███████████████████████████████████████  0.8838\n",
      "E45: ███████████████████████████████████████  0.8882\n",
      "E46: ███████████████████████████████████████  0.8835\n",
      "E47: ███████████████████████████████████████  0.8885\n",
      "E48: ███████████████████████████████████████  0.8882\n",
      "E49: ███████████████████████████████████████  0.8885\n",
      "E50: ███████████████████████████████████████  0.8861\n",
      "E51: ███████████████████████████████████████  0.8836\n",
      "E52: ████████████████████████████████████████ 0.8954\n",
      "E53: ███████████████████████████████████████  0.8858\n",
      "E54: ███████████████████████████████████████  0.8854\n",
      "E55: ███████████████████████████████████████  0.8937\n",
      "E56: ███████████████████████████████████████  0.8860\n",
      "E57: ███████████████████████████████████████  0.8887\n",
      "E58: ███████████████████████████████████████  0.8896\n",
      "E59: ███████████████████████████████████████  0.8933\n",
      "E60: ███████████████████████████████████████  0.8889\n",
      "E61: ███████████████████████████████████████  0.8915\n",
      "E62: ███████████████████████████████████████  0.8828\n",
      "E63: ███████████████████████████████████████  0.8914\n",
      "E64: ███████████████████████████████████████  0.8844\n",
      "E65: ███████████████████████████████████████  0.8900\n",
      "E66: ███████████████████████████████████████  0.8820\n",
      "E67: ███████████████████████████████████████  0.8841\n",
      "E68: ███████████████████████████████████████  0.8890\n",
      "E69: ███████████████████████████████████████  0.8893\n",
      "E70: ███████████████████████████████████████  0.8894\n",
      "E71: ███████████████████████████████████████  0.8895\n",
      "E72: ███████████████████████████████████████  0.8922\n",
      "E73: ███████████████████████████████████████  0.8905\n",
      "E74: ███████████████████████████████████████  0.8864\n",
      "E75: ███████████████████████████████████████  0.8896\n",
      "E76: ███████████████████████████████████████  0.8858\n",
      "E77: ███████████████████████████████████████  0.8947\n",
      "E78: ███████████████████████████████████████  0.8906\n",
      "E79: ███████████████████████████████████████  0.8953\n",
      "E80: ███████████████████████████████████████  0.8907\n",
      "E81: ███████████████████████████████████████  0.8889\n",
      "E82: ███████████████████████████████████████  0.8904\n",
      "E83: ███████████████████████████████████████  0.8874\n",
      "E84: ███████████████████████████████████████  0.8925\n",
      "E85: ███████████████████████████████████████  0.8918\n",
      "E86: ███████████████████████████████████████  0.8941\n",
      "E87: ███████████████████████████████████████  0.8942\n",
      "E88: ███████████████████████████████████████  0.8934\n",
      "E89: ███████████████████████████████████████  0.8935\n",
      "E90: ███████████████████████████████████████  0.8909\n",
      "E91: ███████████████████████████████████████  0.8920\n",
      "E92: ███████████████████████████████████████  0.8934\n",
      "E93: ███████████████████████████████████████  0.8935\n",
      "E94: ███████████████████████████████████████  0.8948\n",
      "E95: ███████████████████████████████████████  0.8913\n",
      "E96: ███████████████████████████████████████  0.8923\n",
      "E97: ███████████████████████████████████████  0.8899\n",
      "E98: ███████████████████████████████████████  0.8917\n",
      "E99: ███████████████████████████████████████  0.8946\n",
      "E100: ███████████████████████████████████████  0.8954\n",
      "\n",
      "📈 TRAINING SUMMARY STATISTICS:\n",
      "--------------------------------------------------\n",
      "Best Substance Accuracy: 0.9985 (Epoch 54)\n",
      "Best Symptom F1 Score: 0.8954 (Epoch 52)\n",
      "Final Train Loss: 0.0003\n",
      "Final Val Loss: 0.0668\n",
      "Loss Improvement: 99.9%\n",
      "\n",
      "📊 Interactive charts saved to training_charts.html\n",
      "Open this file in your web browser to view the charts!\n",
      "📁 Detailed results saved to training_detailed_results.csv and evaluation_detailed_results.csv\n",
      "💾 Model saved as 'best_model.pth'\n",
      "\n",
      "🎉 Evaluation completed! Check the generated files for detailed results.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "import torch\n",
    "\n",
    "# Replace the trainer evaluation code with this:\n",
    "\n",
    "# Evaluate the trained model\n",
    "print(\"\\nEvaluating trained model...\")\n",
    "eval_results = evaluate_model(trained_model, test_dataset, batch_size=16)\n",
    "\n",
    "# Print evaluation metrics\n",
    "substance_accuracy = accuracy_score(eval_results['substance_labels'], eval_results['substance_preds'])\n",
    "symptom_f1 = f1_score(eval_results['symptom_labels'], eval_results['symptom_preds'], average='micro', zero_division=0)\n",
    "symptom_precision = precision_score(eval_results['symptom_labels'], eval_results['symptom_preds'], average='micro', zero_division=0)\n",
    "symptom_recall = recall_score(eval_results['symptom_labels'], eval_results['symptom_preds'], average='micro', zero_division=0)\n",
    "\n",
    "print(f'Final Evaluation Results:')\n",
    "print(f'Substance Accuracy: {substance_accuracy:.4f}')\n",
    "print(f'Symptom F1 Score: {symptom_f1:.4f}')\n",
    "print(f'Symptom Precision: {symptom_precision:.4f}')\n",
    "print(f'Symptom Recall: {symptom_recall:.4f}')\n",
    "\n",
    "# Get predictions for classification reports\n",
    "substance_preds = eval_results['substance_preds']\n",
    "symptom_preds = eval_results['symptom_preds']\n",
    "\n",
    "# Make sure you have these variables defined (they should be from your data preprocessing)\n",
    "# If not, you'll need to extract them from your datasets\n",
    "print('\\nSubstance Classification Report:')\n",
    "print(classification_report(eval_results['substance_labels'], substance_preds,\n",
    "                           target_names=substance_classes, zero_division=0))\n",
    "\n",
    "print('\\nSymptom Classification Report:')\n",
    "print(classification_report(eval_results['symptom_labels'], symptom_preds,\n",
    "                           target_names=symptom_columns, zero_division=0))\n",
    "\n",
    "# Text-based training history visualization (alternative to matplotlib)\n",
    "def print_training_charts(history):\n",
    "    \"\"\"\n",
    "    Create text-based charts for training history\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING HISTORY VISUALIZATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    epochs = len(history['train_loss'])\n",
    "    \n",
    "    # Loss chart\n",
    "    print(\"\\n📉 LOSS TRENDS:\")\n",
    "    print(\"-\" * 50)\n",
    "    max_train_loss = max(history['train_loss'])\n",
    "    max_val_loss = max(history['val_loss'])\n",
    "    max_loss = max(max_train_loss, max_val_loss)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        train_bar = int((history['train_loss'][i] / max_loss) * 30)\n",
    "        val_bar = int((history['val_loss'][i] / max_loss) * 30)\n",
    "        \n",
    "        print(f\"E{i+1:2d} Train: {'█' * train_bar:<30} {history['train_loss'][i]:.4f}\")\n",
    "        print(f\"    Val  : {'█' * val_bar:<30} {history['val_loss'][i]:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    # Accuracy chart\n",
    "    print(\"\\n📊 SUBSTANCE ACCURACY TRENDS:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i in range(epochs):\n",
    "        acc_bar = int(history['substance_acc'][i] * 40)\n",
    "        print(f\"E{i+1:2d}: {'█' * acc_bar:<40} {history['substance_acc'][i]:.4f}\")\n",
    "    \n",
    "    # F1 Score chart\n",
    "    print(\"\\n🎯 SYMPTOM F1 SCORE TRENDS:\")\n",
    "    print(\"-\" * 50)\n",
    "    max_f1 = max(history['symptom_f1']) if max(history['symptom_f1']) > 0 else 1\n",
    "    for i in range(epochs):\n",
    "        f1_bar = int((history['symptom_f1'][i] / max_f1) * 40)\n",
    "        print(f\"E{i+1:2d}: {'█' * f1_bar:<40} {history['symptom_f1'][i]:.4f}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n📈 TRAINING SUMMARY STATISTICS:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Best Substance Accuracy: {max(history['substance_acc']):.4f} (Epoch {history['substance_acc'].index(max(history['substance_acc'])) + 1})\")\n",
    "    print(f\"Best Symptom F1 Score: {max(history['symptom_f1']):.4f} (Epoch {history['symptom_f1'].index(max(history['symptom_f1'])) + 1})\")\n",
    "    print(f\"Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"Loss Improvement: {((history['train_loss'][0] - history['train_loss'][-1]) / history['train_loss'][0] * 100):.1f}%\")\n",
    "\n",
    "# HTML-based visualization (alternative approach)\n",
    "def create_html_charts(history, filename='training_charts.html'):\n",
    "    \"\"\"\n",
    "    Create an HTML file with interactive charts using Chart.js\n",
    "    \"\"\"\n",
    "    html_content = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Training History</title>\n",
    "        <script src=\"https://cdn.jsdelivr.net/npm/chart.js\"></script>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "            .chart-container {{ width: 45%; display: inline-block; margin: 20px; }}\n",
    "            h1 {{ text-align: center; color: #333; }}\n",
    "            h2 {{ color: #666; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Training History Dashboard</h1>\n",
    "        \n",
    "        <div class=\"chart-container\">\n",
    "            <h2>Loss Over Time</h2>\n",
    "            <canvas id=\"lossChart\"></canvas>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"chart-container\">\n",
    "            <h2>Substance Accuracy</h2>\n",
    "            <canvas id=\"accuracyChart\"></canvas>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"chart-container\">\n",
    "            <h2>Symptom F1 Score</h2>\n",
    "            <canvas id=\"f1Chart\"></canvas>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"chart-container\">\n",
    "            <h2>Symptom Precision & Recall</h2>\n",
    "            <canvas id=\"precisionRecallChart\"></canvas>\n",
    "        </div>\n",
    "        \n",
    "        <script>\n",
    "            const epochs = {list(range(1, len(history['train_loss']) + 1))};\n",
    "            \n",
    "            // Loss Chart\n",
    "            new Chart(document.getElementById('lossChart'), {{\n",
    "                type: 'line',\n",
    "                data: {{\n",
    "                    labels: epochs,\n",
    "                    datasets: [{{\n",
    "                        label: 'Training Loss',\n",
    "                        data: {history['train_loss']},\n",
    "                        borderColor: 'rgb(255, 99, 132)',\n",
    "                        backgroundColor: 'rgba(255, 99, 132, 0.2)',\n",
    "                    }}, {{\n",
    "                        label: 'Validation Loss',\n",
    "                        data: {history['val_loss']},\n",
    "                        borderColor: 'rgb(54, 162, 235)',\n",
    "                        backgroundColor: 'rgba(54, 162, 235, 0.2)',\n",
    "                    }}]\n",
    "                }},\n",
    "                options: {{\n",
    "                    responsive: true,\n",
    "                    scales: {{\n",
    "                        y: {{ beginAtZero: true }}\n",
    "                    }}\n",
    "                }}\n",
    "            }});\n",
    "            \n",
    "            // Accuracy Chart\n",
    "            new Chart(document.getElementById('accuracyChart'), {{\n",
    "                type: 'line',\n",
    "                data: {{\n",
    "                    labels: epochs,\n",
    "                    datasets: [{{\n",
    "                        label: 'Substance Accuracy',\n",
    "                        data: {history['substance_acc']},\n",
    "                        borderColor: 'rgb(75, 192, 192)',\n",
    "                        backgroundColor: 'rgba(75, 192, 192, 0.2)',\n",
    "                    }}]\n",
    "                }},\n",
    "                options: {{\n",
    "                    responsive: true,\n",
    "                    scales: {{\n",
    "                        y: {{ beginAtZero: true, max: 1 }}\n",
    "                    }}\n",
    "                }}\n",
    "            }});\n",
    "            \n",
    "            // F1 Chart\n",
    "            new Chart(document.getElementById('f1Chart'), {{\n",
    "                type: 'line',\n",
    "                data: {{\n",
    "                    labels: epochs,\n",
    "                    datasets: [{{\n",
    "                        label: 'Symptom F1 Score',\n",
    "                        data: {history['symptom_f1']},\n",
    "                        borderColor: 'rgb(255, 206, 86)',\n",
    "                        backgroundColor: 'rgba(255, 206, 86, 0.2)',\n",
    "                    }}]\n",
    "                }},\n",
    "                options: {{\n",
    "                    responsive: true,\n",
    "                    scales: {{\n",
    "                        y: {{ beginAtZero: true, max: 1 }}\n",
    "                    }}\n",
    "                }}\n",
    "            }});\n",
    "            \n",
    "            // Precision & Recall Chart\n",
    "            new Chart(document.getElementById('precisionRecallChart'), {{\n",
    "                type: 'line',\n",
    "                data: {{\n",
    "                    labels: epochs,\n",
    "                    datasets: [{{\n",
    "                        label: 'Precision',\n",
    "                        data: {history['symptom_precision']},\n",
    "                        borderColor: 'rgb(153, 102, 255)',\n",
    "                        backgroundColor: 'rgba(153, 102, 255, 0.2)',\n",
    "                    }}, {{\n",
    "                        label: 'Recall',\n",
    "                        data: {history['symptom_recall']},\n",
    "                        borderColor: 'rgb(255, 159, 64)',\n",
    "                        backgroundColor: 'rgba(255, 159, 64, 0.2)',\n",
    "                    }}]\n",
    "                }},\n",
    "                options: {{\n",
    "                    responsive: true,\n",
    "                    scales: {{\n",
    "                        y: {{ beginAtZero: true, max: 1 }}\n",
    "                    }}\n",
    "                }}\n",
    "            }});\n",
    "        </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    print(f\"\\n📊 Interactive charts saved to {filename}\")\n",
    "    print(\"Open this file in your web browser to view the charts!\")\n",
    "\n",
    "# Use text-based visualization\n",
    "print_training_charts(training_history)\n",
    "\n",
    "# Create HTML charts (optional - creates a file you can open in browser)\n",
    "create_html_charts(training_history)\n",
    "\n",
    "# Enhanced CSV export with more details\n",
    "def save_detailed_results(history, eval_results, filename='detailed_results.csv'):\n",
    "    \"\"\"\n",
    "    Save comprehensive results including training history and final evaluation\n",
    "    \"\"\"\n",
    "    import csv\n",
    "    \n",
    "    # Training history\n",
    "    with open(f'training_{filename}', 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['epoch', 'train_loss', 'val_loss', 'substance_acc', 'symptom_f1', 'symptom_precision', 'symptom_recall'])\n",
    "        for i in range(len(history['train_loss'])):\n",
    "            writer.writerow([\n",
    "                i+1, history['train_loss'][i], history['val_loss'][i],\n",
    "                history['substance_acc'][i], history['symptom_f1'][i],\n",
    "                history['symptom_precision'][i], history['symptom_recall'][i]\n",
    "            ])\n",
    "    \n",
    "    # Final evaluation results\n",
    "    with open(f'evaluation_{filename}', 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['metric', 'value'])\n",
    "        writer.writerow(['substance_accuracy', substance_accuracy])\n",
    "        writer.writerow(['symptom_f1', symptom_f1])\n",
    "        writer.writerow(['symptom_precision', symptom_precision])\n",
    "        writer.writerow(['symptom_recall', symptom_recall])\n",
    "    \n",
    "    print(f\"📁 Detailed results saved to training_{filename} and evaluation_{filename}\")\n",
    "\n",
    "# Save comprehensive results\n",
    "save_detailed_results(training_history, eval_results)\n",
    "\n",
    "# Optional: Save the trained model\n",
    "torch.save(trained_model.state_dict(), 'best_model.pth')\n",
    "print(\"💾 Model saved as 'best_model.pth'\")\n",
    "\n",
    "print(\"\\n🎉 Evaluation completed! Check the generated files for detailed results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6afcaa5",
   "metadata": {},
   "source": [
    "7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecf079df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, vectorizer, and results saved!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import csv\n",
    "\n",
    "torch.save(model.state_dict(), './tfidf_drug_use_model.pt')\n",
    "with open('./tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "model_info = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'input_size': 10000,\n",
    "        'num_substance_classes': len(substance_classes),\n",
    "        'num_symptom_labels': len(symptom_columns)\n",
    "    },\n",
    "    'substance_classes': substance_classes,\n",
    "    'symptom_columns': symptom_columns,\n",
    "    'training_history': training_history\n",
    "}\n",
    "torch.save(model_info, './complete_model_info.pt')\n",
    "\n",
    "with open('training_history.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['epoch', 'train_loss', 'val_loss', 'substance_acc', 'symptom_f1', 'symptom_precision', 'symptom_recall'])\n",
    "    for i in range(len(training_history['train_loss'])):\n",
    "        writer.writerow([\n",
    "            i+1, training_history['train_loss'][i], training_history['val_loss'][i],\n",
    "            training_history['substance_acc'][i], training_history['symptom_f1'][i],\n",
    "            training_history['symptom_precision'][i], training_history['symptom_recall'][i]\n",
    "        ])\n",
    "\n",
    "with open('evaluation_results.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['metric', 'value'])\n",
    "    writer.writerow(['substance_accuracy', substance_accuracy])\n",
    "    writer.writerow(['symptom_f1', symptom_f1])\n",
    "    writer.writerow(['symptom_precision', symptom_precision])\n",
    "    writer.writerow(['symptom_recall', symptom_recall])\n",
    "\n",
    "print('Model, vectorizer, and results saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
