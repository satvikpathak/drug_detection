{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d1840c",
   "metadata": {},
   "source": [
    "1. Setup Environment\n",
    "\n",
    "Install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a46b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q transformers==4.20.1 datasets==2.10.0 pandas==1.4.2 numpy==1.22.4 scikit-learn==1.1.1 torch==1.11.0 nltk==3.7 imbalanced-learn==0.9.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cada9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from imblearn.over_sampling import SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85a04ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf111fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854e8c5",
   "metadata": {},
   "source": [
    "2. Create and Preprocess drug_use_data.csv\n",
    "\n",
    "Load SetFit/ade_corpus_v2_classification train split, create CSV, and preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0464c215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as drug_use_data.csv\n",
      "Training samples: 14109, Test samples: 3528\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import urllib.request\n",
    "\n",
    "# Define splits\n",
    "splits = {'train': 'train.jsonl', 'test': 'test.jsonl'}\n",
    "\n",
    "# Load via hf:// protocol\n",
    "try:\n",
    "    df = pd.read_json(\"hf://datasets/SetFit/ade_corpus_v2_classification/\" + splits[\"train\"], lines=True)\n",
    "except Exception as e:\n",
    "    print(f\"hf:// loading failed: {e}\")\n",
    "    print(\"Falling back to direct URL...\")\n",
    "    url = \"https://huggingface.co/datasets/SetFit/ade_corpus_v2_classification/resolve/main/train.jsonl\"\n",
    "    urllib.request.urlretrieve(url, \"train.jsonl\")\n",
    "    df = pd.read_json(\"train.jsonl\", lines=True)\n",
    "\n",
    "# Simulate substance and symptom labels\n",
    "substance_map = {\n",
    "    'morphine': 'opioid', 'oxycodone': 'opioid', 'fentanyl': 'opioid',\n",
    "    'cocaine': 'stimulant', 'methamphetamine': 'stimulant',\n",
    "    'placebo': 'none'\n",
    "}\n",
    "symptom_list = ['nausea', 'confusion', 'drowsiness', 'overdose']\n",
    "\n",
    "def assign_labels(text):\n",
    "    substance = 'none'\n",
    "    symptoms = []\n",
    "    text_lower = str(text).lower()\n",
    "    for drug, subst in substance_map.items():\n",
    "        if drug in text_lower:\n",
    "            substance = subst\n",
    "            break\n",
    "    for symp in symptom_list:\n",
    "        if symp in text_lower:\n",
    "            symptoms.append(symp)\n",
    "    return substance, symptoms if symptoms else ['none']\n",
    "\n",
    "# Apply labels\n",
    "df['substance_label'], df['symptom_labels'] = zip(*df['text'].apply(assign_labels))\n",
    "\n",
    "# Save to CSV\n",
    "df[['text', 'substance_label', 'symptom_labels']].to_csv('drug_use_data.csv', index=False)\n",
    "print('Dataset saved as drug_use_data.csv')\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Encode labels\n",
    "substance_classes = df['substance_label'].unique()\n",
    "substance2id = {label: idx for idx, label in enumerate(substance_classes)}\n",
    "df['substance_label'] = df['substance_label'].map(substance2id)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "symptom_encoded = mlb.fit_transform(df['symptom_labels'])\n",
    "symptom_df = pd.DataFrame(symptom_encoded, columns=mlb.classes_)\n",
    "\n",
    "df = pd.concat([df[['text', 'substance_label']], symptom_df], axis=1)\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['substance_label'])\n",
    "\n",
    "print(f'Training samples: {len(train_df)}, Test samples: {len(test_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab3f6f0",
   "metadata": {},
   "source": [
    "3. Tokenize Data\n",
    "\n",
    "Tokenize using BioBERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "81dee1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 1.08k/1.08k [00:00<?, ?B/s]\n",
      "Downloading: 100%|██████████| 208k/208k [00:00<00:00, 487kB/s] \n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('dmis-lab/biobert-base-cased-v1.2')\n",
    "\n",
    "def tokenize_data(texts, max_length=128):\n",
    "    return tokenizer(\n",
    "        texts.tolist(),\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "\n",
    "train_encodings = tokenize_data(train_df['text'])\n",
    "test_encodings = tokenize_data(test_df['text'])\n",
    "\n",
    "class DrugUseDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings, substance_labels, symptom_labels):\n",
    "        self.encodings = encodings\n",
    "        self.substance_labels = substance_labels\n",
    "        self.symptom_labels = symptom_labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['substance_labels'] = torch.tensor(self.substance_labels[idx], dtype=torch.long)\n",
    "        item['symptom_labels'] = torch.tensor(self.symptom_labels[idx], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.substance_labels)\n",
    "\n",
    "symptom_columns = mlb.classes_\n",
    "train_dataset = DrugUseDataset(\n",
    "    train_encodings,\n",
    "    train_df['substance_label'].values,\n",
    "    train_df[symptom_columns].values\n",
    ")\n",
    "test_dataset = DrugUseDataset(\n",
    "    test_encodings,\n",
    "    test_df['substance_label'].values,\n",
    "    test_df[symptom_columns].values\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd25d65",
   "metadata": {},
   "source": [
    "4. Define Custom Model\n",
    "\n",
    "BioBERT for multi-task classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856710d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading:  94%|█████████▍| 390M/416M [01:22<00:22, 1.17MB/s] "
     ]
    }
   ],
   "source": [
    "class BioBERTMultiTask(torch.nn.Module):\n",
    "    def __init__(self, num_substance_classes, num_symptom_labels):\n",
    "        super(BioBERTMultiTask, self).__init__()\n",
    "        self.bert = AutoModelForSequenceClassification.from_pretrained(\n",
    "            'dmis-lab/biobert-base-cased-v1.2',\n",
    "            output_hidden_states=True\n",
    "        )\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "        self.substance_classifier = torch.nn.Linear(768, num_substance_classes)\n",
    "        self.symptom_classifier = torch.nn.Linear(768, num_symptom_labels)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, substance_labels=None, symptom_labels=None):\n",
    "        outputs = self.bert(input_ids, attention_mask=attention_mask)\n",
    "        pooled_output = outputs.hidden_states[-1][:, 0]\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "\n",
    "        substance_logits = self.substance_classifier(pooled_output)\n",
    "        symptom_logits = self.symptom_classifier(pooled_output)\n",
    "\n",
    "        loss = None\n",
    "        if substance_labels is not None and symptom_labels is not None:\n",
    "            # Compute class weights for substance labels\n",
    "            class_counts = np.bincount(substance_labels.cpu().numpy())\n",
    "            class_weights = torch.tensor(1.0 / class_counts, dtype=torch.float).to(device)\n",
    "            substance_loss = torch.nn.CrossEntropyLoss(weight=class_weights)(substance_logits, substance_labels)\n",
    "            symptom_loss = torch.nn.BCEWithLogitsLoss()(symptom_logits, symptom_labels)\n",
    "            loss = substance_loss + symptom_loss\n",
    "\n",
    "        return {'loss': loss, 'substance_logits': substance_logits, 'symptom_logits': symptom_logits}\n",
    "\n",
    "model = BioBERTMultiTask(num_substance_classes=len(substance_classes), num_symptom_labels=len(symptom_columns))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe4d1a",
   "metadata": {},
   "source": [
    "5. Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36396a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    substance_preds = np.argmax(pred.predictions[0], axis=1)\n",
    "    substance_labels = pred.label_ids[0]\n",
    "    symptom_preds = (pred.predictions[1] > 0.5).astype(int)\n",
    "    symptom_labels = pred.label_ids[1]\n",
    "\n",
    "    substance_accuracy = accuracy_score(substance_labels, substance_preds)\n",
    "    symptom_f1 = f1_score(symptom_labels, symptom_preds, average='micro')\n",
    "\n",
    "    return {\n",
    "        'substance_accuracy': substance_accuracy,\n",
    "        'symptom_f1': symptom_f1\n",
    "    }\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    learning_rate=2e-5,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=50,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='substance_accuracy'\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5bdb3c",
   "metadata": {},
   "source": [
    "6. Evaluate Model\n",
    "\n",
    "Evaluate and print results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33040f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f'Evaluation Results: {eval_results}')\n",
    "\n",
    "predictions = trainer.predict(test_dataset)\n",
    "substance_preds = np.argmax(predictions.predictions[0], axis=1)\n",
    "symptom_preds = (predictions.predictions[1] > 0.5).astype(int)\n",
    "\n",
    "print('Substance Classification Report:')\n",
    "print(classification_report(test_df['substance_label'], substance_preds, target_names=substance_classes))\n",
    "\n",
    "print('Symptom Classification Report:')\n",
    "print(classification_report(test_df[symptom_columns], symptom_preds, target_names=symptom_columns, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6afcaa5",
   "metadata": {},
   "source": [
    "7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf079df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained('./biobert_drug_use_model')\n",
    "tokenizer.save_pretrained('./biobert_drug_use_model')\n",
    "print('Model and tokenizer saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
