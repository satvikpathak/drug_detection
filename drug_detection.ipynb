{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7d1840c",
   "metadata": {},
   "source": [
    "1. Setup Environment\n",
    "\n",
    "Install required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35a46b7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q transformers==4.20.1 datasets==2.10.0 pandas==1.4.2 numpy==1.22.4 scikit-learn==1.1.1 torch==1.11.0 nltk==3.7 imbalanced-learn==0.9.1 optuna==3.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cada9edb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import MultiLabelBinarizer, StandardScaler\n",
    "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from imblearn.combine import SMOTEENN\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "85a04ade",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\satvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\satvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\satvi\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk_downloads = ['stopwords', 'punkt', 'wordnet', 'averaged_perceptron_tagger', 'omw-1.4']\n",
    "for item in nltk_downloads:\n",
    "    try:\n",
    "        nltk.data.find(f'tokenizers/{item}' if item == 'punkt' else f'corpora/{item}')\n",
    "    except LookupError:\n",
    "        nltk.download(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf111fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1854e8c5",
   "metadata": {},
   "source": [
    "2. Create and Preprocess drug_use_data.csv\n",
    "\n",
    "Load SetFit/ade_corpus_v2_classification train split, create CSV, and preprocess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0464c215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved as drug_use_data.csv\n",
      "Original distribution: Counter({0: 17510, 1: 102, 2: 25})\n",
      "Balanced distribution: Counter({0: 18379, 1: 17512, 2: 16639})\n",
      "Training samples: 42024, Test samples: 10506\n",
      "\n",
      "Final dataset info:\n",
      "Substance classes: ['none', 'opioid', 'stimulant']\n",
      "Symptom classes: ['adverse_event', 'anxiety', 'confusion', 'constipation', 'dizziness', 'drowsiness', 'dyspnea', 'fatigue', 'headache', 'hematoma', 'nausea', 'none', 'overdose', 'pain', 'pruritus', 'rash', 'seizure', 'vomiting']\n",
      "Total features: text + 3 substance classes + 18 symptom classes\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import urllib.request\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download required NLTK data\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except LookupError:\n",
    "    nltk.download('stopwords')\n",
    "\n",
    "# Define splits\n",
    "splits = {'train': 'train.jsonl', 'test': 'test.jsonl'}\n",
    "\n",
    "# Load via hf:// protocol\n",
    "try:\n",
    "    df = pd.read_json(\"hf://datasets/SetFit/ade_corpus_v2_classification/\" + splits[\"train\"], lines=True)\n",
    "except Exception as e:\n",
    "    print(f\"hf:// loading failed: {e}\")\n",
    "    print(\"Falling back to direct URL...\")\n",
    "    url = \"https://huggingface.co/datasets/SetFit/ade_corpus_v2_classification/resolve/main/train.jsonl\"\n",
    "    urllib.request.urlretrieve(url, \"train.jsonl\")\n",
    "    df = pd.read_json(\"train.jsonl\", lines=True)\n",
    "\n",
    "# Expanded substance and symptom lists\n",
    "substance_map = {\n",
    "    'morphine': 'opioid', 'oxycodone': 'opioid', 'fentanyl': 'opioid', 'hydrocodone': 'opioid',\n",
    "    'heroin': 'opioid', 'codeine': 'opioid', 'tramadol': 'opioid',\n",
    "    'cocaine': 'stimulant', 'methamphetamine': 'stimulant', 'amphetamine': 'stimulant',\n",
    "    'placebo': 'none', 'heparin': 'none'\n",
    "}\n",
    "symptom_list = ['nausea', 'confusion', 'drowsiness', 'overdose', 'dizziness', 'vomiting',\n",
    "                'fatigue', 'headache', 'anxiety', 'seizure', 'hematoma', 'rash', 'pain',\n",
    "                'constipation', 'dyspnea', 'pruritus']\n",
    "\n",
    "def assign_labels(text, original_label=None):\n",
    "    substance = 'none'\n",
    "    symptoms = []\n",
    "    text_lower = str(text).lower()\n",
    "    \n",
    "    # Check for substances\n",
    "    for drug, subst in substance_map.items():\n",
    "        if drug in text_lower:\n",
    "            substance = subst\n",
    "            break\n",
    "    \n",
    "    # Check for symptoms\n",
    "    for symp in symptom_list:\n",
    "        if symp in text_lower:\n",
    "            symptoms.append(symp)\n",
    "    \n",
    "    # Use original ADE label if available and no symptoms found\n",
    "    if original_label == 1 and not symptoms:\n",
    "        symptoms = ['adverse_event']\n",
    "    \n",
    "    return substance, symptoms if symptoms else ['none']\n",
    "\n",
    "# Apply labels with original label information\n",
    "if 'label' in df.columns:\n",
    "    df['substance_label'], df['symptom_labels'] = zip(*[\n",
    "        assign_labels(text, label) for text, label in zip(df['text'], df['label'])\n",
    "    ])\n",
    "else:\n",
    "    df['substance_label'], df['symptom_labels'] = zip(*df['text'].apply(lambda x: assign_labels(x)))\n",
    "\n",
    "# Save to CSV BEFORE any processing that might duplicate data\n",
    "df[['text', 'substance_label', 'symptom_labels']].to_csv('drug_use_data.csv', index=False)\n",
    "print('Dataset saved as drug_use_data.csv')\n",
    "\n",
    "# Preprocess text\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    return text\n",
    "\n",
    "df['text'] = df['text'].apply(preprocess_text)\n",
    "\n",
    "# Encode labels\n",
    "substance_classes = df['substance_label'].unique()\n",
    "substance2id = {label: idx for idx, label in enumerate(substance_classes)}\n",
    "df['substance_label'] = df['substance_label'].map(substance2id)\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "symptom_encoded = mlb.fit_transform(df['symptom_labels'])\n",
    "symptom_df = pd.DataFrame(symptom_encoded, columns=mlb.classes_)\n",
    "symptom_columns = mlb.classes_\n",
    "\n",
    "# Combine dataframes\n",
    "df = pd.concat([df[['text', 'substance_label']], symptom_df], axis=1)\n",
    "\n",
    "# Apply SMOTE for balanced training data\n",
    "print(\"Original distribution:\", Counter(df['substance_label']))\n",
    "\n",
    "# Use TF-IDF features for SMOTE\n",
    "temp_vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "X_tfidf_temp = temp_vectorizer.fit_transform(df['text']).toarray()\n",
    "\n",
    "try:\n",
    "    smote = SMOTE(random_state=42, k_neighbors=min(3, Counter(df['substance_label']).most_common()[-1][1] - 1))\n",
    "    X_balanced, y_balanced = smote.fit_resample(X_tfidf_temp, df['substance_label'])\n",
    "    \n",
    "    # Create balanced dataframe by finding closest matches\n",
    "    balanced_indices = []\n",
    "    for x_sample in X_balanced:\n",
    "        similarities = np.dot(X_tfidf_temp, x_sample)\n",
    "        closest_idx = np.argmax(similarities)\n",
    "        balanced_indices.append(closest_idx)\n",
    "    \n",
    "    df_balanced = df.iloc[balanced_indices].copy()\n",
    "    df_balanced['substance_label'] = y_balanced\n",
    "    df = df_balanced\n",
    "    \n",
    "    print(\"Balanced distribution:\", Counter(df['substance_label']))\n",
    "except ValueError as e:\n",
    "    print(f\"SMOTE failed: {e}, using original data with manual balancing\")\n",
    "    # Fallback: simple oversampling for minority classes\n",
    "    minority_threshold = len(df) * 0.1  # 10% threshold\n",
    "    minority_data = []\n",
    "    for label in df['substance_label'].unique():\n",
    "        label_data = df[df['substance_label'] == label]\n",
    "        if len(label_data) < minority_threshold:\n",
    "            # Duplicate minority class samples\n",
    "            multiplier = int(minority_threshold / len(label_data)) + 1\n",
    "            minority_data.append(pd.concat([label_data] * multiplier, ignore_index=True))\n",
    "    \n",
    "    if minority_data:\n",
    "        df = pd.concat([df] + minority_data, ignore_index=True)\n",
    "        print(\"Manual balancing applied\")\n",
    "\n",
    "# Split data\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42, stratify=df['substance_label'])\n",
    "\n",
    "print(f'Training samples: {len(train_df)}, Test samples: {len(test_df)}')\n",
    "\n",
    "# Print final data info\n",
    "print(\"\\nFinal dataset info:\")\n",
    "print(f\"Substance classes: {list(substance2id.keys())}\")\n",
    "print(f\"Symptom classes: {list(symptom_columns)}\")\n",
    "print(f\"Total features: text + {len(substance2id)} substance classes + {len(symptom_columns)} symptom classes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab3f6f0",
   "metadata": {},
   "source": [
    "3. Create TF-IDF Features and Datasets\n",
    "\n",
    "Use TF-IDF features and create custom dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81dee1ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating TF-IDF features...\n",
      "TF-IDF sparse matrix shape: (42024, 2000)\n",
      "Memory usage (sparse): ~3.4 MB\n",
      "Vocabulary size: 2000\n",
      "Converting sparse matrices to dense (this may take a moment)...\n",
      "Processed 0/42024 samples...\n",
      "Processed 5000/42024 samples...\n",
      "Processed 10000/42024 samples...\n",
      "Processed 15000/42024 samples...\n",
      "Processed 20000/42024 samples...\n",
      "Processed 25000/42024 samples...\n",
      "Processed 30000/42024 samples...\n",
      "Processed 35000/42024 samples...\n",
      "Processed 40000/42024 samples...\n",
      "Processed 0/10506 samples...\n",
      "Processed 5000/10506 samples...\n",
      "Processed 10000/10506 samples...\n",
      "Conversion completed successfully!\n",
      "Final TF-IDF feature shape: (42024, 2000)\n",
      "Train symptom data shape: (42024, 18)\n",
      "Test symptom data shape: (10506, 18)\n",
      "Creating training dataset...\n",
      "Dataset created with 42024 samples\n",
      "Feature shape: (42024, 2000)\n",
      "Memory usage: ~320.6 MB\n",
      "Creating test dataset...\n",
      "Dataset created with 10506 samples\n",
      "Feature shape: (10506, 2000)\n",
      "Memory usage: ~80.2 MB\n",
      "Datasets created successfully!\n",
      "Sample data shapes - Features: torch.Size([2000]), Substance: torch.Size([]), Symptoms: torch.Size([18])\n",
      "Memory cleanup completed!\n",
      "TF-IDF processing completed successfully!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "\n",
    "# Create TF-IDF features with reduced memory footprint\n",
    "print(\"Creating TF-IDF features...\")\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_features=2000,  # Reduced from 5000 to save memory\n",
    "    stop_words='english', \n",
    "    ngram_range=(1, 2),  # Reduced from (1,3) to save memory\n",
    "    dtype=np.float32,\n",
    "    min_df=3,  # Increased to reduce vocabulary size\n",
    "    max_df=0.90  # More aggressive filtering\n",
    ")\n",
    "\n",
    "# Keep matrices in sparse format - DON'T convert to dense arrays\n",
    "X_train_tfidf_sparse = vectorizer.fit_transform(train_df['text'])\n",
    "X_test_tfidf_sparse = vectorizer.transform(test_df['text'])\n",
    "\n",
    "print(f\"TF-IDF sparse matrix shape: {X_train_tfidf_sparse.shape}\")\n",
    "print(f\"Memory usage (sparse): ~{X_train_tfidf_sparse.data.nbytes / 1024**2:.1f} MB\")\n",
    "print(f\"Vocabulary size: {len(vectorizer.vocabulary_)}\")\n",
    "\n",
    "# Convert sparse matrices to dense in smaller batches to avoid memory issues\n",
    "def sparse_to_dense_batched(sparse_matrix, batch_size=1000):\n",
    "    \"\"\"Convert sparse matrix to dense in batches to manage memory\"\"\"\n",
    "    n_samples = sparse_matrix.shape[0]\n",
    "    n_features = sparse_matrix.shape[1]\n",
    "    \n",
    "    # Pre-allocate dense array\n",
    "    dense_array = np.zeros((n_samples, n_features), dtype=np.float32)\n",
    "    \n",
    "    # Process in batches\n",
    "    for i in range(0, n_samples, batch_size):\n",
    "        end_idx = min(i + batch_size, n_samples)\n",
    "        batch_sparse = sparse_matrix[i:end_idx]\n",
    "        dense_array[i:end_idx] = batch_sparse.toarray()\n",
    "        \n",
    "        if i % (batch_size * 10) == 0:  # Progress update every 10 batches\n",
    "            print(f\"Processed {i}/{n_samples} samples...\")\n",
    "    \n",
    "    return dense_array\n",
    "\n",
    "print(\"Converting sparse matrices to dense (this may take a moment)...\")\n",
    "try:\n",
    "    X_train_tfidf = sparse_to_dense_batched(X_train_tfidf_sparse, batch_size=500)\n",
    "    X_test_tfidf = sparse_to_dense_batched(X_test_tfidf_sparse, batch_size=500)\n",
    "    print(\"Conversion completed successfully!\")\n",
    "except MemoryError:\n",
    "    print(\"Still not enough memory. Using even smaller batch size...\")\n",
    "    try:\n",
    "        X_train_tfidf = sparse_to_dense_batched(X_train_tfidf_sparse, batch_size=100)\n",
    "        X_test_tfidf = sparse_to_dense_batched(X_test_tfidf_sparse, batch_size=100)\n",
    "        print(\"Conversion completed with smaller batches!\")\n",
    "    except MemoryError:\n",
    "        print(\"Memory still insufficient. Switching to sparse-compatible approach...\")\n",
    "        # Alternative: Work directly with sparse matrices (requires model modification)\n",
    "        raise MemoryError(\"Consider using a machine with more RAM or further reducing max_features\")\n",
    "\n",
    "print(f\"Final TF-IDF feature shape: {X_train_tfidf.shape}\")\n",
    "\n",
    "# Memory-efficient dataset class\n",
    "class TFIDFDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, features, substance_labels, symptom_labels):\n",
    "        # Store as numpy arrays to save memory compared to tensors\n",
    "        self.features = features.astype(np.float32)\n",
    "        self.substance_labels = substance_labels.astype(np.int64)\n",
    "        self.symptom_labels = symptom_labels.astype(np.float32)\n",
    "        \n",
    "        print(f\"Dataset created with {len(self.features)} samples\")\n",
    "        print(f\"Feature shape: {self.features.shape}\")\n",
    "        print(f\"Memory usage: ~{self.features.nbytes / 1024**2:.1f} MB\")\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Convert to tensors only when needed (lazy loading)\n",
    "        return {\n",
    "            'x': torch.from_numpy(self.features[idx]).float(),\n",
    "            'substance_labels': torch.from_numpy(np.array(self.substance_labels[idx])).long(),\n",
    "            'symptom_labels': torch.from_numpy(self.symptom_labels[idx]).float()\n",
    "        }\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "# Ensure symptom columns exist in both dataframes\n",
    "missing_train_cols = [col for col in symptom_columns if col not in train_df.columns]\n",
    "missing_test_cols = [col for col in symptom_columns if col not in test_df.columns]\n",
    "\n",
    "if missing_train_cols:\n",
    "    print(f\"Adding missing columns to train_df: {missing_train_cols}\")\n",
    "    for col in missing_train_cols:\n",
    "        train_df[col] = 0\n",
    "\n",
    "if missing_test_cols:\n",
    "    print(f\"Adding missing columns to test_df: {missing_test_cols}\")\n",
    "    for col in missing_test_cols:\n",
    "        test_df[col] = 0\n",
    "\n",
    "# Get symptom data\n",
    "train_symptom_data = train_df[symptom_columns].values\n",
    "test_symptom_data = test_df[symptom_columns].values\n",
    "\n",
    "print(f\"Train symptom data shape: {train_symptom_data.shape}\")\n",
    "print(f\"Test symptom data shape: {test_symptom_data.shape}\")\n",
    "\n",
    "# Create datasets with memory management\n",
    "import gc\n",
    "\n",
    "# Clear any unnecessary variables\n",
    "if 'X_train_tfidf_sparse' in locals():\n",
    "    del X_train_tfidf_sparse\n",
    "if 'X_test_tfidf_sparse' in locals():\n",
    "    del X_test_tfidf_sparse\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    print(\"Creating training dataset...\")\n",
    "    train_dataset = TFIDFDataset(\n",
    "        X_train_tfidf,\n",
    "        train_df['substance_label'].values,\n",
    "        train_symptom_data\n",
    "    )\n",
    "    \n",
    "    print(\"Creating test dataset...\")\n",
    "    test_dataset = TFIDFDataset(\n",
    "        X_test_tfidf,\n",
    "        test_df['substance_label'].values,\n",
    "        test_symptom_data\n",
    "    )\n",
    "    \n",
    "    print(\"Datasets created successfully!\")\n",
    "    \n",
    "    # Verify dataset integrity\n",
    "    sample = train_dataset[0]\n",
    "    print(f\"Sample data shapes - Features: {sample['x'].shape}, \"\n",
    "          f\"Substance: {sample['substance_labels'].shape}, \"\n",
    "          f\"Symptoms: {sample['symptom_labels'].shape}\")\n",
    "    \n",
    "    # Clean up large arrays to free memory\n",
    "    del X_train_tfidf, X_test_tfidf\n",
    "    gc.collect()\n",
    "    print(\"Memory cleanup completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error creating datasets: {e}\")\n",
    "    print(\"Debugging information:\")\n",
    "    print(f\"Available memory info:\")\n",
    "    import psutil\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Total RAM: {memory.total / 1024**3:.1f} GB\")\n",
    "    print(f\"Available RAM: {memory.available / 1024**3:.1f} GB\")\n",
    "    print(f\"Used RAM: {memory.percent}%\")\n",
    "    raise\n",
    "\n",
    "print(\"TF-IDF processing completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd25d65",
   "metadata": {},
   "source": [
    "4. Define Custom Model\n",
    "\n",
    "BioBERT for multi-task classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "856710d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determining input size...\n",
      "Input size from vectorizer vocabulary: 2000\n",
      "Final input size: 2000\n",
      "Substance classes: 3\n",
      "Symptom labels: 18\n",
      "\n",
      "Model created successfully!\n",
      "Total parameters: 1,501,685\n",
      "Trainable parameters: 1,501,685\n",
      "Model device: cpu\n",
      "\n",
      "Model Architecture:\n",
      "Input size: 2000\n",
      "Substance classes: 3\n",
      "Symptom labels: 18\n",
      "Hidden layers: 512 -> 256 -> 128\n",
      "Features: Batch normalization, residual connections, attention mechanisms, focal loss\n",
      "\n",
      "Testing model with sample batch...\n",
      "✓ Model test successful!\n",
      "  Loss: 0.2391\n",
      "  Substance logits shape: torch.Size([2, 3])\n",
      "  Symptom logits shape: torch.Size([2, 18])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class EnhancedMultiTaskModel(torch.nn.Module):\n",
    "    def __init__(self, input_size, num_substance_classes, num_symptom_labels):\n",
    "        super(EnhancedMultiTaskModel, self).__init__()\n",
    "        \n",
    "        # Input normalization\n",
    "        self.input_norm = torch.nn.BatchNorm1d(input_size)\n",
    "        \n",
    "        # Enhanced architecture with residual connections\n",
    "        self.hidden1 = torch.nn.Linear(input_size, 512)\n",
    "        self.norm1 = torch.nn.BatchNorm1d(512)\n",
    "        self.hidden2 = torch.nn.Linear(512, 256)\n",
    "        self.norm2 = torch.nn.BatchNorm1d(256)\n",
    "        self.hidden3 = torch.nn.Linear(256, 128)\n",
    "        self.norm3 = torch.nn.BatchNorm1d(128)\n",
    "        \n",
    "        # Residual connection layer\n",
    "        self.residual = torch.nn.Linear(input_size, 128)\n",
    "        \n",
    "        # Dropout with different rates\n",
    "        self.dropout1 = torch.nn.Dropout(0.2)\n",
    "        self.dropout2 = torch.nn.Dropout(0.3)\n",
    "        self.dropout3 = torch.nn.Dropout(0.2)\n",
    "        \n",
    "        # Task-specific layers with attention\n",
    "        self.substance_attention = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 128),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.symptom_attention = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 128),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.substance_classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(64, num_substance_classes)\n",
    "        )\n",
    "        \n",
    "        self.symptom_classifier = torch.nn.Sequential(\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.1),\n",
    "            torch.nn.Linear(64, num_symptom_labels)\n",
    "        )\n",
    "        \n",
    "        self.num_substance_classes = num_substance_classes\n",
    "        self.num_symptom_labels = num_symptom_labels\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, torch.nn.Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight)\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, torch.nn.BatchNorm1d):\n",
    "                torch.nn.init.constant_(m.weight, 1)\n",
    "                torch.nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x, substance_labels=None, symptom_labels=None):\n",
    "        # Input normalization\n",
    "        x_norm = self.input_norm(x)\n",
    "        \n",
    "        # Forward pass through hidden layers\n",
    "        hidden = torch.relu(self.hidden1(x_norm))\n",
    "        hidden = self.norm1(hidden)\n",
    "        hidden = self.dropout1(hidden)\n",
    "        \n",
    "        hidden = torch.relu(self.hidden2(hidden))\n",
    "        hidden = self.norm2(hidden)\n",
    "        hidden = self.dropout2(hidden)\n",
    "        \n",
    "        hidden = torch.relu(self.hidden3(hidden))\n",
    "        hidden = self.norm3(hidden)\n",
    "        \n",
    "        # Residual connection\n",
    "        residual = torch.relu(self.residual(x_norm))\n",
    "        hidden = hidden + residual  # Add residual connection\n",
    "        hidden = self.dropout3(hidden)\n",
    "        \n",
    "        # Task-specific attention\n",
    "        substance_att = self.substance_attention(hidden)\n",
    "        symptom_att = self.symptom_attention(hidden)\n",
    "        \n",
    "        # Apply attention\n",
    "        substance_features = hidden * substance_att\n",
    "        symptom_features = hidden * symptom_att\n",
    "        \n",
    "        # Generate logits\n",
    "        substance_logits = self.substance_classifier(substance_features)\n",
    "        symptom_logits = self.symptom_classifier(symptom_features)\n",
    "        \n",
    "        loss = None\n",
    "        if substance_labels is not None and symptom_labels is not None:\n",
    "            # Improved loss calculation\n",
    "            \n",
    "            # Focal loss for substance classification (better for imbalanced classes)\n",
    "            alpha = 0.25\n",
    "            gamma = 2.0\n",
    "            \n",
    "            # Standard cross entropy\n",
    "            ce_loss = torch.nn.functional.cross_entropy(substance_logits, substance_labels, reduction='none')\n",
    "            pt = torch.exp(-ce_loss)\n",
    "            focal_loss = alpha * (1 - pt) ** gamma * ce_loss\n",
    "            substance_loss = focal_loss.mean()\n",
    "            \n",
    "            # Class-balanced BCE loss for symptoms\n",
    "            pos_counts = substance_labels.bincount(minlength=self.num_substance_classes).float()\n",
    "            total_count = len(substance_labels)\n",
    "            pos_weights = total_count / (2.0 * pos_counts + 1e-6)\n",
    "            \n",
    "            # For symptoms, use adaptive positive weights\n",
    "            symptom_pos_counts = symptom_labels.sum(dim=0) + 1e-6\n",
    "            symptom_neg_counts = (1 - symptom_labels).sum(dim=0) + 1e-6\n",
    "            symptom_pos_weights = symptom_neg_counts / symptom_pos_counts\n",
    "            symptom_pos_weights = torch.clamp(symptom_pos_weights, min=0.1, max=10.0)\n",
    "            \n",
    "            symptom_loss = torch.nn.functional.binary_cross_entropy_with_logits(\n",
    "                symptom_logits, \n",
    "                symptom_labels, \n",
    "                pos_weight=symptom_pos_weights\n",
    "            )\n",
    "            \n",
    "            # Combine losses with adaptive weighting\n",
    "            substance_weight = 0.7  # Higher weight for substance classification\n",
    "            symptom_weight = 0.3\n",
    "            \n",
    "            loss = substance_weight * substance_loss + symptom_weight * symptom_loss\n",
    "        \n",
    "        return {\n",
    "            'loss': loss, \n",
    "            'substance_logits': substance_logits, \n",
    "            'symptom_logits': symptom_logits,\n",
    "            'substance_probs': torch.softmax(substance_logits, dim=-1),\n",
    "            'symptom_probs': torch.sigmoid(symptom_logits)\n",
    "        }\n",
    "\n",
    "# Get input size from the vectorizer or dataset (multiple methods)\n",
    "print(\"Determining input size...\")\n",
    "\n",
    "# Method 1: From vectorizer (most reliable)\n",
    "if 'vectorizer' in locals() and hasattr(vectorizer, 'vocabulary_'):\n",
    "    actual_input_size = len(vectorizer.vocabulary_)\n",
    "    print(f\"Input size from vectorizer vocabulary: {actual_input_size}\")\n",
    "elif 'vectorizer' in locals() and hasattr(vectorizer, 'max_features'):\n",
    "    actual_input_size = vectorizer.max_features\n",
    "    print(f\"Input size from vectorizer max_features: {actual_input_size}\")\n",
    "# Method 2: From dataset\n",
    "elif 'train_dataset' in locals():\n",
    "    sample = train_dataset[0]\n",
    "    actual_input_size = sample['x'].shape[0]\n",
    "    print(f\"Input size from dataset sample: {actual_input_size}\")\n",
    "# Method 3: Check what we set in vectorizer creation\n",
    "else:\n",
    "    # Fallback to the value we used in vectorizer creation\n",
    "    actual_input_size = 2000  # This was the max_features we set\n",
    "    print(f\"Using fallback input size: {actual_input_size}\")\n",
    "    print(\"Warning: Using fallback size. Ensure this matches your vectorizer configuration.\")\n",
    "\n",
    "# Verify the input size is correct\n",
    "if 'train_dataset' in locals():\n",
    "    sample = train_dataset[0]\n",
    "    sample_input_size = sample['x'].shape[0]\n",
    "    if sample_input_size != actual_input_size:\n",
    "        print(f\"WARNING: Mismatch detected!\")\n",
    "        print(f\"Calculated input size: {actual_input_size}\")\n",
    "        print(f\"Actual dataset input size: {sample_input_size}\")\n",
    "        actual_input_size = sample_input_size\n",
    "        print(f\"Using dataset input size: {actual_input_size}\")\n",
    "\n",
    "print(f\"Final input size: {actual_input_size}\")\n",
    "print(f\"Substance classes: {len(substance_classes)}\")\n",
    "print(f\"Symptom labels: {len(symptom_columns)}\")\n",
    "\n",
    "# Create the model\n",
    "model = EnhancedMultiTaskModel(\n",
    "    input_size=actual_input_size,\n",
    "    num_substance_classes=len(substance_classes),\n",
    "    num_symptom_labels=len(symptom_columns)\n",
    ")\n",
    "\n",
    "# Set device (make sure device is defined)\n",
    "if 'device' not in locals():\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Device set to: {device}\")\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "# Print model info\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nModel created successfully!\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model device: {next(model.parameters()).device}\")\n",
    "\n",
    "# Model summary\n",
    "print(\"\\nModel Architecture:\")\n",
    "print(f\"Input size: {actual_input_size}\")\n",
    "print(f\"Substance classes: {len(substance_classes)}\")\n",
    "print(f\"Symptom labels: {len(symptom_columns)}\")\n",
    "print(f\"Hidden layers: 512 -> 256 -> 128\")\n",
    "print(\"Features: Batch normalization, residual connections, attention mechanisms, focal loss\")\n",
    "\n",
    "# Test model with a sample batch to ensure everything works\n",
    "if 'train_dataset' in locals():\n",
    "    print(\"\\nTesting model with sample batch...\")\n",
    "    try:\n",
    "        sample_batch = torch.utils.data.DataLoader(train_dataset, batch_size=2, shuffle=False)\n",
    "        batch = next(iter(sample_batch))\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            x = batch['x'].to(device)\n",
    "            substance_labels = batch['substance_labels'].to(device)\n",
    "            symptom_labels = batch['symptom_labels'].to(device)\n",
    "            \n",
    "            outputs = model(x, substance_labels=substance_labels, symptom_labels=symptom_labels)\n",
    "            \n",
    "            print(f\"✓ Model test successful!\")\n",
    "            print(f\"  Loss: {outputs['loss'].item():.4f}\")\n",
    "            print(f\"  Substance logits shape: {outputs['substance_logits'].shape}\")\n",
    "            print(f\"  Symptom logits shape: {outputs['symptom_logits'].shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"✗ Model test failed: {e}\")\n",
    "        raise\n",
    "else:\n",
    "    print(\"Warning: train_dataset not available for testing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fe4d1a",
   "metadata": {},
   "source": [
    "5. Train Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "36396a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "Starting training...\n",
      "Total epochs: 100\n",
      "Batch size: 32\n",
      "Initial learning rate: 0.0002\n",
      "Total training batches: 1314\n",
      "Total validation batches: 329\n",
      "Epoch 1/100, Batch 0/1314, Loss: 0.3723\n",
      "Epoch 1/100, Batch 50/1314, Loss: 0.3095\n",
      "Epoch 1/100, Batch 100/1314, Loss: 0.2929\n",
      "Epoch 1/100, Batch 150/1314, Loss: 0.2714\n",
      "Epoch 1/100, Batch 200/1314, Loss: 0.2335\n",
      "Epoch 1/100, Batch 250/1314, Loss: 0.2180\n",
      "Epoch 1/100, Batch 300/1314, Loss: 0.2144\n",
      "Epoch 1/100, Batch 350/1314, Loss: 0.2313\n",
      "Epoch 1/100, Batch 400/1314, Loss: 0.2006\n",
      "Epoch 1/100, Batch 450/1314, Loss: 0.1696\n",
      "Epoch 1/100, Batch 500/1314, Loss: 0.1798\n",
      "Epoch 1/100, Batch 550/1314, Loss: 0.1579\n",
      "Epoch 1/100, Batch 600/1314, Loss: 0.1469\n",
      "Epoch 1/100, Batch 650/1314, Loss: 0.1357\n",
      "Epoch 1/100, Batch 700/1314, Loss: 0.1596\n",
      "Epoch 1/100, Batch 750/1314, Loss: 0.1180\n",
      "Epoch 1/100, Batch 800/1314, Loss: 0.1640\n",
      "Epoch 1/100, Batch 850/1314, Loss: 0.1448\n",
      "Epoch 1/100, Batch 900/1314, Loss: 0.1041\n",
      "Epoch 1/100, Batch 950/1314, Loss: 0.1165\n",
      "Epoch 1/100, Batch 1000/1314, Loss: 0.1008\n",
      "Epoch 1/100, Batch 1050/1314, Loss: 0.0691\n",
      "Epoch 1/100, Batch 1100/1314, Loss: 0.0914\n",
      "Epoch 1/100, Batch 1150/1314, Loss: 0.0826\n",
      "Epoch 1/100, Batch 1200/1314, Loss: 0.0673\n",
      "Epoch 1/100, Batch 1250/1314, Loss: 0.0579\n",
      "Epoch 1/100, Batch 1300/1314, Loss: 0.0660\n",
      "New best model saved! Substance accuracy: 0.7129\n",
      "\n",
      "Epoch 1/100 Results:\n",
      "Train Loss: 0.1628, Val Loss: 0.4193\n",
      "Substance Accuracy: 0.7129\n",
      "Symptom F1: 0.7176, Precision: 0.7436, Recall: 0.6934\n",
      "Learning Rate: 0.000040\n",
      "------------------------------------------------------------\n",
      "Epoch 2/100, Batch 0/1314, Loss: 0.1316\n",
      "Epoch 2/100, Batch 50/1314, Loss: 0.0506\n",
      "Epoch 2/100, Batch 100/1314, Loss: 0.0659\n",
      "Epoch 2/100, Batch 150/1314, Loss: 0.0544\n",
      "Epoch 2/100, Batch 200/1314, Loss: 0.0531\n",
      "Epoch 2/100, Batch 250/1314, Loss: 0.0295\n",
      "Epoch 2/100, Batch 300/1314, Loss: 0.0327\n",
      "Epoch 2/100, Batch 350/1314, Loss: 0.0276\n",
      "Epoch 2/100, Batch 400/1314, Loss: 0.0396\n",
      "Epoch 2/100, Batch 450/1314, Loss: 0.0232\n",
      "Epoch 2/100, Batch 500/1314, Loss: 0.0329\n",
      "Epoch 2/100, Batch 550/1314, Loss: 0.0641\n",
      "Epoch 2/100, Batch 600/1314, Loss: 0.0408\n",
      "Epoch 2/100, Batch 650/1314, Loss: 0.0145\n",
      "Epoch 2/100, Batch 700/1314, Loss: 0.0586\n",
      "Epoch 2/100, Batch 750/1314, Loss: 0.0433\n",
      "Epoch 2/100, Batch 800/1314, Loss: 0.0199\n",
      "Epoch 2/100, Batch 850/1314, Loss: 0.0321\n",
      "Epoch 2/100, Batch 900/1314, Loss: 0.0253\n",
      "Epoch 2/100, Batch 950/1314, Loss: 0.0123\n",
      "Epoch 2/100, Batch 1000/1314, Loss: 0.0143\n",
      "Epoch 2/100, Batch 1050/1314, Loss: 0.0227\n",
      "Epoch 2/100, Batch 1100/1314, Loss: 0.0154\n",
      "Epoch 2/100, Batch 1150/1314, Loss: 0.0418\n",
      "Epoch 2/100, Batch 1200/1314, Loss: 0.0166\n",
      "Epoch 2/100, Batch 1250/1314, Loss: 0.0151\n",
      "Epoch 2/100, Batch 1300/1314, Loss: 0.0092\n",
      "New best model saved! Substance accuracy: 0.7509\n",
      "\n",
      "Epoch 2/100 Results:\n",
      "Train Loss: 0.0359, Val Loss: 0.3822\n",
      "Substance Accuracy: 0.7509\n",
      "Symptom F1: 0.8285, Precision: 0.9100, Recall: 0.7603\n",
      "Learning Rate: 0.000060\n",
      "------------------------------------------------------------\n",
      "Epoch 3/100, Batch 0/1314, Loss: 0.0181\n",
      "Epoch 3/100, Batch 50/1314, Loss: 0.0590\n",
      "Epoch 3/100, Batch 100/1314, Loss: 0.0344\n",
      "Epoch 3/100, Batch 150/1314, Loss: 0.0165\n",
      "Epoch 3/100, Batch 200/1314, Loss: 0.0139\n",
      "Epoch 3/100, Batch 250/1314, Loss: 0.0298\n",
      "Epoch 3/100, Batch 300/1314, Loss: 0.0175\n",
      "Epoch 3/100, Batch 350/1314, Loss: 0.0076\n",
      "Epoch 3/100, Batch 400/1314, Loss: 0.0201\n",
      "Epoch 3/100, Batch 450/1314, Loss: 0.0067\n",
      "Epoch 3/100, Batch 500/1314, Loss: 0.0131\n",
      "Epoch 3/100, Batch 550/1314, Loss: 0.0111\n",
      "Epoch 3/100, Batch 600/1314, Loss: 0.0208\n",
      "Epoch 3/100, Batch 650/1314, Loss: 0.0383\n",
      "Epoch 3/100, Batch 700/1314, Loss: 0.0337\n",
      "Epoch 3/100, Batch 750/1314, Loss: 0.0103\n",
      "Epoch 3/100, Batch 800/1314, Loss: 0.0126\n",
      "Epoch 3/100, Batch 850/1314, Loss: 0.0117\n",
      "Epoch 3/100, Batch 900/1314, Loss: 0.0143\n",
      "Epoch 3/100, Batch 950/1314, Loss: 0.0145\n",
      "Epoch 3/100, Batch 1000/1314, Loss: 0.0255\n",
      "Epoch 3/100, Batch 1050/1314, Loss: 0.0150\n",
      "Epoch 3/100, Batch 1100/1314, Loss: 0.0065\n",
      "Epoch 3/100, Batch 1150/1314, Loss: 0.0115\n",
      "Epoch 3/100, Batch 1200/1314, Loss: 0.0175\n",
      "Epoch 3/100, Batch 1250/1314, Loss: 0.0113\n",
      "Epoch 3/100, Batch 1300/1314, Loss: 0.0117\n",
      "New best model saved! Substance accuracy: 0.7848\n",
      "\n",
      "Epoch 3/100 Results:\n",
      "Train Loss: 0.0199, Val Loss: 0.3675\n",
      "Substance Accuracy: 0.7848\n",
      "Symptom F1: 0.8554, Precision: 0.9290, Recall: 0.7925\n",
      "Learning Rate: 0.000080\n",
      "------------------------------------------------------------\n",
      "Epoch 4/100, Batch 0/1314, Loss: 0.0048\n",
      "Epoch 4/100, Batch 50/1314, Loss: 0.0185\n",
      "Epoch 4/100, Batch 100/1314, Loss: 0.0140\n",
      "Epoch 4/100, Batch 150/1314, Loss: 0.0105\n",
      "Epoch 4/100, Batch 200/1314, Loss: 0.0197\n",
      "Epoch 4/100, Batch 250/1314, Loss: 0.0066\n",
      "Epoch 4/100, Batch 300/1314, Loss: 0.0094\n",
      "Epoch 4/100, Batch 350/1314, Loss: 0.0098\n",
      "Epoch 4/100, Batch 400/1314, Loss: 0.0207\n",
      "Epoch 4/100, Batch 450/1314, Loss: 0.0153\n",
      "Epoch 4/100, Batch 500/1314, Loss: 0.0080\n",
      "Epoch 4/100, Batch 550/1314, Loss: 0.0164\n",
      "Epoch 4/100, Batch 600/1314, Loss: 0.0211\n",
      "Epoch 4/100, Batch 650/1314, Loss: 0.0097\n",
      "Epoch 4/100, Batch 700/1314, Loss: 0.0104\n",
      "Epoch 4/100, Batch 750/1314, Loss: 0.0143\n",
      "Epoch 4/100, Batch 800/1314, Loss: 0.0082\n",
      "Epoch 4/100, Batch 850/1314, Loss: 0.0076\n",
      "Epoch 4/100, Batch 900/1314, Loss: 0.0147\n",
      "Epoch 4/100, Batch 950/1314, Loss: 0.0076\n",
      "Epoch 4/100, Batch 1000/1314, Loss: 0.0123\n",
      "Epoch 4/100, Batch 1050/1314, Loss: 0.0063\n",
      "Epoch 4/100, Batch 1100/1314, Loss: 0.0233\n",
      "Epoch 4/100, Batch 1150/1314, Loss: 0.0060\n",
      "Epoch 4/100, Batch 1200/1314, Loss: 0.0065\n",
      "Epoch 4/100, Batch 1250/1314, Loss: 0.0273\n",
      "Epoch 4/100, Batch 1300/1314, Loss: 0.0090\n",
      "New best model saved! Substance accuracy: 0.8121\n",
      "\n",
      "Epoch 4/100 Results:\n",
      "Train Loss: 0.0144, Val Loss: 0.3085\n",
      "Substance Accuracy: 0.8121\n",
      "Symptom F1: 0.8569, Precision: 0.9191, Recall: 0.8025\n",
      "Learning Rate: 0.000100\n",
      "------------------------------------------------------------\n",
      "Epoch 5/100, Batch 0/1314, Loss: 0.0241\n",
      "Epoch 5/100, Batch 50/1314, Loss: 0.0111\n",
      "Epoch 5/100, Batch 100/1314, Loss: 0.0120\n",
      "Epoch 5/100, Batch 150/1314, Loss: 0.0112\n",
      "Epoch 5/100, Batch 200/1314, Loss: 0.0227\n",
      "Epoch 5/100, Batch 250/1314, Loss: 0.0076\n",
      "Epoch 5/100, Batch 300/1314, Loss: 0.0382\n",
      "Epoch 5/100, Batch 350/1314, Loss: 0.0093\n",
      "Epoch 5/100, Batch 400/1314, Loss: 0.0087\n",
      "Epoch 5/100, Batch 450/1314, Loss: 0.0111\n",
      "Epoch 5/100, Batch 500/1314, Loss: 0.0121\n",
      "Epoch 5/100, Batch 550/1314, Loss: 0.0117\n",
      "Epoch 5/100, Batch 600/1314, Loss: 0.0057\n",
      "Epoch 5/100, Batch 650/1314, Loss: 0.0055\n",
      "Epoch 5/100, Batch 700/1314, Loss: 0.0202\n",
      "Epoch 5/100, Batch 750/1314, Loss: 0.0069\n",
      "Epoch 5/100, Batch 800/1314, Loss: 0.0041\n",
      "Epoch 5/100, Batch 850/1314, Loss: 0.0292\n",
      "Epoch 5/100, Batch 900/1314, Loss: 0.0101\n",
      "Epoch 5/100, Batch 950/1314, Loss: 0.0057\n",
      "Epoch 5/100, Batch 1000/1314, Loss: 0.0057\n",
      "Epoch 5/100, Batch 1050/1314, Loss: 0.0079\n",
      "Epoch 5/100, Batch 1100/1314, Loss: 0.0121\n",
      "Epoch 5/100, Batch 1150/1314, Loss: 0.0096\n",
      "Epoch 5/100, Batch 1200/1314, Loss: 0.0351\n",
      "Epoch 5/100, Batch 1250/1314, Loss: 0.0023\n",
      "Epoch 5/100, Batch 1300/1314, Loss: 0.0365\n",
      "New best model saved! Substance accuracy: 0.8392\n",
      "\n",
      "Epoch 5/100 Results:\n",
      "Train Loss: 0.0112, Val Loss: 0.2563\n",
      "Substance Accuracy: 0.8392\n",
      "Symptom F1: 0.8642, Precision: 0.9211, Recall: 0.8138\n",
      "Learning Rate: 0.000120\n",
      "------------------------------------------------------------\n",
      "Epoch 6/100, Batch 0/1314, Loss: 0.0085\n",
      "Epoch 6/100, Batch 50/1314, Loss: 0.0015\n",
      "Epoch 6/100, Batch 100/1314, Loss: 0.0146\n",
      "Epoch 6/100, Batch 150/1314, Loss: 0.0256\n",
      "Epoch 6/100, Batch 200/1314, Loss: 0.0042\n",
      "Epoch 6/100, Batch 250/1314, Loss: 0.0048\n",
      "Epoch 6/100, Batch 300/1314, Loss: 0.0066\n",
      "Epoch 6/100, Batch 350/1314, Loss: 0.0047\n",
      "Epoch 6/100, Batch 400/1314, Loss: 0.0083\n",
      "Epoch 6/100, Batch 450/1314, Loss: 0.0027\n",
      "Epoch 6/100, Batch 500/1314, Loss: 0.0058\n",
      "Epoch 6/100, Batch 550/1314, Loss: 0.0274\n",
      "Epoch 6/100, Batch 600/1314, Loss: 0.0202\n",
      "Epoch 6/100, Batch 650/1314, Loss: 0.0065\n",
      "Epoch 6/100, Batch 700/1314, Loss: 0.0074\n",
      "Epoch 6/100, Batch 750/1314, Loss: 0.0111\n",
      "Epoch 6/100, Batch 800/1314, Loss: 0.0048\n",
      "Epoch 6/100, Batch 850/1314, Loss: 0.0145\n",
      "Epoch 6/100, Batch 900/1314, Loss: 0.0027\n",
      "Epoch 6/100, Batch 950/1314, Loss: 0.0016\n",
      "Epoch 6/100, Batch 1000/1314, Loss: 0.0078\n",
      "Epoch 6/100, Batch 1050/1314, Loss: 0.0065\n",
      "Epoch 6/100, Batch 1100/1314, Loss: 0.0044\n",
      "Epoch 6/100, Batch 1150/1314, Loss: 0.0191\n",
      "Epoch 6/100, Batch 1200/1314, Loss: 0.0038\n",
      "Epoch 6/100, Batch 1250/1314, Loss: 0.0076\n",
      "Epoch 6/100, Batch 1300/1314, Loss: 0.0086\n",
      "New best model saved! Substance accuracy: 0.8871\n",
      "\n",
      "Epoch 6/100 Results:\n",
      "Train Loss: 0.0094, Val Loss: 0.2105\n",
      "Substance Accuracy: 0.8871\n",
      "Symptom F1: 0.8665, Precision: 0.9226, Recall: 0.8168\n",
      "Learning Rate: 0.000140\n",
      "------------------------------------------------------------\n",
      "Epoch 7/100, Batch 0/1314, Loss: 0.0087\n",
      "Epoch 7/100, Batch 50/1314, Loss: 0.0072\n",
      "Epoch 7/100, Batch 100/1314, Loss: 0.0101\n",
      "Epoch 7/100, Batch 150/1314, Loss: 0.0263\n",
      "Epoch 7/100, Batch 200/1314, Loss: 0.0060\n",
      "Epoch 7/100, Batch 250/1314, Loss: 0.0009\n",
      "Epoch 7/100, Batch 300/1314, Loss: 0.0023\n",
      "Epoch 7/100, Batch 350/1314, Loss: 0.0110\n",
      "Epoch 7/100, Batch 400/1314, Loss: 0.0111\n",
      "Epoch 7/100, Batch 450/1314, Loss: 0.0078\n",
      "Epoch 7/100, Batch 500/1314, Loss: 0.0105\n",
      "Epoch 7/100, Batch 550/1314, Loss: 0.0048\n",
      "Epoch 7/100, Batch 600/1314, Loss: 0.0064\n",
      "Epoch 7/100, Batch 650/1314, Loss: 0.0092\n",
      "Epoch 7/100, Batch 700/1314, Loss: 0.0029\n",
      "Epoch 7/100, Batch 750/1314, Loss: 0.0030\n",
      "Epoch 7/100, Batch 800/1314, Loss: 0.0045\n",
      "Epoch 7/100, Batch 850/1314, Loss: 0.0106\n",
      "Epoch 7/100, Batch 900/1314, Loss: 0.0088\n",
      "Epoch 7/100, Batch 950/1314, Loss: 0.0030\n",
      "Epoch 7/100, Batch 1000/1314, Loss: 0.0031\n",
      "Epoch 7/100, Batch 1050/1314, Loss: 0.0041\n",
      "Epoch 7/100, Batch 1100/1314, Loss: 0.0059\n",
      "Epoch 7/100, Batch 1150/1314, Loss: 0.0077\n",
      "Epoch 7/100, Batch 1200/1314, Loss: 0.0025\n",
      "Epoch 7/100, Batch 1250/1314, Loss: 0.0104\n",
      "Epoch 7/100, Batch 1300/1314, Loss: 0.0069\n",
      "New best model saved! Substance accuracy: 0.8920\n",
      "\n",
      "Epoch 7/100 Results:\n",
      "Train Loss: 0.0081, Val Loss: 0.1923\n",
      "Substance Accuracy: 0.8920\n",
      "Symptom F1: 0.8562, Precision: 0.8981, Recall: 0.8180\n",
      "Learning Rate: 0.000160\n",
      "------------------------------------------------------------\n",
      "Epoch 8/100, Batch 0/1314, Loss: 0.0030\n",
      "Epoch 8/100, Batch 50/1314, Loss: 0.0011\n",
      "Epoch 8/100, Batch 100/1314, Loss: 0.0029\n",
      "Epoch 8/100, Batch 150/1314, Loss: 0.0022\n",
      "Epoch 8/100, Batch 200/1314, Loss: 0.0137\n",
      "Epoch 8/100, Batch 250/1314, Loss: 0.0022\n",
      "Epoch 8/100, Batch 300/1314, Loss: 0.0053\n",
      "Epoch 8/100, Batch 350/1314, Loss: 0.0187\n",
      "Epoch 8/100, Batch 400/1314, Loss: 0.0143\n",
      "Epoch 8/100, Batch 450/1314, Loss: 0.0091\n",
      "Epoch 8/100, Batch 500/1314, Loss: 0.0182\n",
      "Epoch 8/100, Batch 550/1314, Loss: 0.0017\n",
      "Epoch 8/100, Batch 600/1314, Loss: 0.0233\n",
      "Epoch 8/100, Batch 650/1314, Loss: 0.0246\n",
      "Epoch 8/100, Batch 700/1314, Loss: 0.0044\n",
      "Epoch 8/100, Batch 750/1314, Loss: 0.0049\n",
      "Epoch 8/100, Batch 800/1314, Loss: 0.0028\n",
      "Epoch 8/100, Batch 850/1314, Loss: 0.0036\n",
      "Epoch 8/100, Batch 900/1314, Loss: 0.0044\n",
      "Epoch 8/100, Batch 950/1314, Loss: 0.0093\n",
      "Epoch 8/100, Batch 1000/1314, Loss: 0.0096\n",
      "Epoch 8/100, Batch 1050/1314, Loss: 0.0108\n",
      "Epoch 8/100, Batch 1100/1314, Loss: 0.0067\n",
      "Epoch 8/100, Batch 1150/1314, Loss: 0.0020\n",
      "Epoch 8/100, Batch 1200/1314, Loss: 0.0043\n",
      "Epoch 8/100, Batch 1250/1314, Loss: 0.0054\n",
      "Epoch 8/100, Batch 1300/1314, Loss: 0.0040\n",
      "New best model saved! Substance accuracy: 0.8965\n",
      "\n",
      "Epoch 8/100 Results:\n",
      "Train Loss: 0.0073, Val Loss: 0.2105\n",
      "Substance Accuracy: 0.8965\n",
      "Symptom F1: 0.8611, Precision: 0.8989, Recall: 0.8263\n",
      "Learning Rate: 0.000180\n",
      "------------------------------------------------------------\n",
      "Epoch 9/100, Batch 0/1314, Loss: 0.0054\n",
      "Epoch 9/100, Batch 50/1314, Loss: 0.0101\n",
      "Epoch 9/100, Batch 100/1314, Loss: 0.0028\n",
      "Epoch 9/100, Batch 150/1314, Loss: 0.0021\n",
      "Epoch 9/100, Batch 200/1314, Loss: 0.0030\n",
      "Epoch 9/100, Batch 250/1314, Loss: 0.0068\n",
      "Epoch 9/100, Batch 300/1314, Loss: 0.0137\n",
      "Epoch 9/100, Batch 350/1314, Loss: 0.0074\n",
      "Epoch 9/100, Batch 400/1314, Loss: 0.0062\n",
      "Epoch 9/100, Batch 450/1314, Loss: 0.0028\n",
      "Epoch 9/100, Batch 500/1314, Loss: 0.0012\n",
      "Epoch 9/100, Batch 550/1314, Loss: 0.0076\n",
      "Epoch 9/100, Batch 600/1314, Loss: 0.0034\n",
      "Epoch 9/100, Batch 650/1314, Loss: 0.0130\n",
      "Epoch 9/100, Batch 700/1314, Loss: 0.0030\n",
      "Epoch 9/100, Batch 750/1314, Loss: 0.0013\n",
      "Epoch 9/100, Batch 800/1314, Loss: 0.0019\n",
      "Epoch 9/100, Batch 850/1314, Loss: 0.0026\n",
      "Epoch 9/100, Batch 900/1314, Loss: 0.0071\n",
      "Epoch 9/100, Batch 950/1314, Loss: 0.0080\n",
      "Epoch 9/100, Batch 1000/1314, Loss: 0.0084\n",
      "Epoch 9/100, Batch 1050/1314, Loss: 0.0152\n",
      "Epoch 9/100, Batch 1100/1314, Loss: 0.0052\n",
      "Epoch 9/100, Batch 1150/1314, Loss: 0.0092\n",
      "Epoch 9/100, Batch 1200/1314, Loss: 0.0105\n",
      "Epoch 9/100, Batch 1250/1314, Loss: 0.0082\n",
      "Epoch 9/100, Batch 1300/1314, Loss: 0.0098\n",
      "New best model saved! Substance accuracy: 0.9118\n",
      "\n",
      "Epoch 9/100 Results:\n",
      "Train Loss: 0.0066, Val Loss: 0.1562\n",
      "Substance Accuracy: 0.9118\n",
      "Symptom F1: 0.8679, Precision: 0.9001, Recall: 0.8379\n",
      "Learning Rate: 0.000200\n",
      "------------------------------------------------------------\n",
      "Epoch 10/100, Batch 0/1314, Loss: 0.0077\n",
      "Epoch 10/100, Batch 50/1314, Loss: 0.0045\n",
      "Epoch 10/100, Batch 100/1314, Loss: 0.0050\n",
      "Epoch 10/100, Batch 150/1314, Loss: 0.0075\n",
      "Epoch 10/100, Batch 200/1314, Loss: 0.0049\n",
      "Epoch 10/100, Batch 250/1314, Loss: 0.0043\n",
      "Epoch 10/100, Batch 300/1314, Loss: 0.0058\n",
      "Epoch 10/100, Batch 350/1314, Loss: 0.0072\n",
      "Epoch 10/100, Batch 400/1314, Loss: 0.0027\n",
      "Epoch 10/100, Batch 450/1314, Loss: 0.0043\n",
      "Epoch 10/100, Batch 500/1314, Loss: 0.0047\n",
      "Epoch 10/100, Batch 550/1314, Loss: 0.0051\n",
      "Epoch 10/100, Batch 600/1314, Loss: 0.0050\n",
      "Epoch 10/100, Batch 650/1314, Loss: 0.0087\n",
      "Epoch 10/100, Batch 700/1314, Loss: 0.0063\n",
      "Epoch 10/100, Batch 750/1314, Loss: 0.0037\n",
      "Epoch 10/100, Batch 800/1314, Loss: 0.0059\n",
      "Epoch 10/100, Batch 850/1314, Loss: 0.0025\n",
      "Epoch 10/100, Batch 900/1314, Loss: 0.0067\n",
      "Epoch 10/100, Batch 950/1314, Loss: 0.0021\n",
      "Epoch 10/100, Batch 1000/1314, Loss: 0.0051\n",
      "Epoch 10/100, Batch 1050/1314, Loss: 0.0040\n",
      "Epoch 10/100, Batch 1100/1314, Loss: 0.0070\n",
      "Epoch 10/100, Batch 1150/1314, Loss: 0.0026\n",
      "Epoch 10/100, Batch 1200/1314, Loss: 0.0066\n",
      "Epoch 10/100, Batch 1250/1314, Loss: 0.0079\n",
      "Epoch 10/100, Batch 1300/1314, Loss: 0.0029\n",
      "\n",
      "Epoch 10/100 Results:\n",
      "Train Loss: 0.0060, Val Loss: 0.2450\n",
      "Substance Accuracy: 0.8957\n",
      "Symptom F1: 0.8530, Precision: 0.8731, Recall: 0.8339\n",
      "Learning Rate: 0.000200\n",
      "------------------------------------------------------------\n",
      "Epoch 11/100, Batch 0/1314, Loss: 0.0060\n",
      "Epoch 11/100, Batch 50/1314, Loss: 0.0077\n",
      "Epoch 11/100, Batch 100/1314, Loss: 0.0041\n",
      "Epoch 11/100, Batch 150/1314, Loss: 0.0024\n",
      "Epoch 11/100, Batch 200/1314, Loss: 0.0053\n",
      "Epoch 11/100, Batch 250/1314, Loss: 0.0017\n",
      "Epoch 11/100, Batch 300/1314, Loss: 0.0029\n",
      "Epoch 11/100, Batch 350/1314, Loss: 0.0157\n",
      "Epoch 11/100, Batch 400/1314, Loss: 0.0041\n",
      "Epoch 11/100, Batch 450/1314, Loss: 0.0037\n",
      "Epoch 11/100, Batch 500/1314, Loss: 0.0050\n",
      "Epoch 11/100, Batch 550/1314, Loss: 0.0018\n",
      "Epoch 11/100, Batch 600/1314, Loss: 0.0071\n",
      "Epoch 11/100, Batch 650/1314, Loss: 0.0039\n",
      "Epoch 11/100, Batch 700/1314, Loss: 0.0036\n",
      "Epoch 11/100, Batch 750/1314, Loss: 0.0034\n",
      "Epoch 11/100, Batch 800/1314, Loss: 0.0022\n",
      "Epoch 11/100, Batch 850/1314, Loss: 0.0022\n",
      "Epoch 11/100, Batch 900/1314, Loss: 0.0031\n",
      "Epoch 11/100, Batch 950/1314, Loss: 0.0012\n",
      "Epoch 11/100, Batch 1000/1314, Loss: 0.0070\n",
      "Epoch 11/100, Batch 1050/1314, Loss: 0.0043\n",
      "Epoch 11/100, Batch 1100/1314, Loss: 0.0024\n",
      "Epoch 11/100, Batch 1150/1314, Loss: 0.0045\n",
      "Epoch 11/100, Batch 1200/1314, Loss: 0.0012\n",
      "Epoch 11/100, Batch 1250/1314, Loss: 0.0041\n",
      "Epoch 11/100, Batch 1300/1314, Loss: 0.0052\n",
      "\n",
      "Epoch 11/100 Results:\n",
      "Train Loss: 0.0053, Val Loss: 0.2265\n",
      "Substance Accuracy: 0.9089\n",
      "Symptom F1: 0.8585, Precision: 0.8865, Recall: 0.8323\n",
      "Learning Rate: 0.000200\n",
      "------------------------------------------------------------\n",
      "Epoch 12/100, Batch 0/1314, Loss: 0.0040\n",
      "Epoch 12/100, Batch 50/1314, Loss: 0.0017\n",
      "Epoch 12/100, Batch 100/1314, Loss: 0.0072\n",
      "Epoch 12/100, Batch 150/1314, Loss: 0.0024\n",
      "Epoch 12/100, Batch 200/1314, Loss: 0.0021\n",
      "Epoch 12/100, Batch 250/1314, Loss: 0.0068\n",
      "Epoch 12/100, Batch 300/1314, Loss: 0.0037\n",
      "Epoch 12/100, Batch 350/1314, Loss: 0.0056\n",
      "Epoch 12/100, Batch 400/1314, Loss: 0.0038\n",
      "Epoch 12/100, Batch 450/1314, Loss: 0.0019\n",
      "Epoch 12/100, Batch 500/1314, Loss: 0.0039\n",
      "Epoch 12/100, Batch 550/1314, Loss: 0.0036\n",
      "Epoch 12/100, Batch 600/1314, Loss: 0.0027\n",
      "Epoch 12/100, Batch 650/1314, Loss: 0.0061\n",
      "Epoch 12/100, Batch 700/1314, Loss: 0.0049\n",
      "Epoch 12/100, Batch 750/1314, Loss: 0.0179\n",
      "Epoch 12/100, Batch 800/1314, Loss: 0.0175\n",
      "Epoch 12/100, Batch 850/1314, Loss: 0.0010\n",
      "Epoch 12/100, Batch 900/1314, Loss: 0.0068\n",
      "Epoch 12/100, Batch 950/1314, Loss: 0.0041\n",
      "Epoch 12/100, Batch 1000/1314, Loss: 0.0020\n",
      "Epoch 12/100, Batch 1050/1314, Loss: 0.0041\n",
      "Epoch 12/100, Batch 1100/1314, Loss: 0.0075\n",
      "Epoch 12/100, Batch 1150/1314, Loss: 0.0032\n",
      "Epoch 12/100, Batch 1200/1314, Loss: 0.0033\n",
      "Epoch 12/100, Batch 1250/1314, Loss: 0.0020\n",
      "Epoch 12/100, Batch 1300/1314, Loss: 0.0041\n",
      "\n",
      "Epoch 12/100 Results:\n",
      "Train Loss: 0.0048, Val Loss: 0.2246\n",
      "Substance Accuracy: 0.8923\n",
      "Symptom F1: 0.8422, Precision: 0.8663, Recall: 0.8195\n",
      "Learning Rate: 0.000200\n",
      "------------------------------------------------------------\n",
      "Epoch 13/100, Batch 0/1314, Loss: 0.0062\n",
      "Epoch 13/100, Batch 50/1314, Loss: 0.0045\n",
      "Epoch 13/100, Batch 100/1314, Loss: 0.0025\n",
      "Epoch 13/100, Batch 150/1314, Loss: 0.0010\n",
      "Epoch 13/100, Batch 200/1314, Loss: 0.0010\n",
      "Epoch 13/100, Batch 250/1314, Loss: 0.0064\n",
      "Epoch 13/100, Batch 300/1314, Loss: 0.0029\n",
      "Epoch 13/100, Batch 350/1314, Loss: 0.0059\n",
      "Epoch 13/100, Batch 400/1314, Loss: 0.0058\n",
      "Epoch 13/100, Batch 450/1314, Loss: 0.0080\n",
      "Epoch 13/100, Batch 500/1314, Loss: 0.0015\n",
      "Epoch 13/100, Batch 550/1314, Loss: 0.0026\n",
      "Epoch 13/100, Batch 600/1314, Loss: 0.0059\n",
      "Epoch 13/100, Batch 650/1314, Loss: 0.0016\n",
      "Epoch 13/100, Batch 700/1314, Loss: 0.0157\n",
      "Epoch 13/100, Batch 750/1314, Loss: 0.0022\n",
      "Epoch 13/100, Batch 800/1314, Loss: 0.0014\n",
      "Epoch 13/100, Batch 850/1314, Loss: 0.0060\n",
      "Epoch 13/100, Batch 900/1314, Loss: 0.0032\n",
      "Epoch 13/100, Batch 950/1314, Loss: 0.0006\n",
      "Epoch 13/100, Batch 1000/1314, Loss: 0.0007\n",
      "Epoch 13/100, Batch 1050/1314, Loss: 0.0040\n",
      "Epoch 13/100, Batch 1100/1314, Loss: 0.0024\n",
      "Epoch 13/100, Batch 1150/1314, Loss: 0.0016\n",
      "Epoch 13/100, Batch 1200/1314, Loss: 0.0025\n",
      "Epoch 13/100, Batch 1250/1314, Loss: 0.0065\n",
      "Epoch 13/100, Batch 1300/1314, Loss: 0.0071\n",
      "\n",
      "Epoch 13/100 Results:\n",
      "Train Loss: 0.0043, Val Loss: 0.2578\n",
      "Substance Accuracy: 0.8856\n",
      "Symptom F1: 0.8552, Precision: 0.8796, Recall: 0.8321\n",
      "Learning Rate: 0.000199\n",
      "------------------------------------------------------------\n",
      "Epoch 14/100, Batch 0/1314, Loss: 0.0088\n",
      "Epoch 14/100, Batch 50/1314, Loss: 0.0074\n",
      "Epoch 14/100, Batch 100/1314, Loss: 0.0030\n",
      "Epoch 14/100, Batch 150/1314, Loss: 0.0030\n",
      "Epoch 14/100, Batch 200/1314, Loss: 0.0022\n",
      "Epoch 14/100, Batch 250/1314, Loss: 0.0019\n",
      "Epoch 14/100, Batch 300/1314, Loss: 0.0005\n",
      "Epoch 14/100, Batch 350/1314, Loss: 0.0010\n",
      "Epoch 14/100, Batch 400/1314, Loss: 0.0060\n",
      "Epoch 14/100, Batch 450/1314, Loss: 0.0031\n",
      "Epoch 14/100, Batch 500/1314, Loss: 0.0045\n",
      "Epoch 14/100, Batch 550/1314, Loss: 0.0025\n",
      "Epoch 14/100, Batch 600/1314, Loss: 0.0057\n",
      "Epoch 14/100, Batch 650/1314, Loss: 0.0008\n",
      "Epoch 14/100, Batch 700/1314, Loss: 0.0040\n",
      "Epoch 14/100, Batch 750/1314, Loss: 0.0027\n",
      "Epoch 14/100, Batch 800/1314, Loss: 0.0020\n",
      "Epoch 14/100, Batch 850/1314, Loss: 0.0003\n",
      "Epoch 14/100, Batch 900/1314, Loss: 0.0013\n",
      "Epoch 14/100, Batch 950/1314, Loss: 0.0031\n",
      "Epoch 14/100, Batch 1000/1314, Loss: 0.0030\n",
      "Epoch 14/100, Batch 1050/1314, Loss: 0.0022\n",
      "Epoch 14/100, Batch 1100/1314, Loss: 0.0046\n",
      "Epoch 14/100, Batch 1150/1314, Loss: 0.0083\n",
      "Epoch 14/100, Batch 1200/1314, Loss: 0.0043\n",
      "Epoch 14/100, Batch 1250/1314, Loss: 0.0003\n",
      "Epoch 14/100, Batch 1300/1314, Loss: 0.0018\n",
      "\n",
      "Epoch 14/100 Results:\n",
      "Train Loss: 0.0038, Val Loss: 0.3181\n",
      "Substance Accuracy: 0.8736\n",
      "Symptom F1: 0.8419, Precision: 0.8727, Recall: 0.8132\n",
      "Learning Rate: 0.000199\n",
      "------------------------------------------------------------\n",
      "Epoch 15/100, Batch 0/1314, Loss: 0.0106\n",
      "Epoch 15/100, Batch 50/1314, Loss: 0.0019\n",
      "Epoch 15/100, Batch 100/1314, Loss: 0.0059\n",
      "Epoch 15/100, Batch 150/1314, Loss: 0.0008\n",
      "Epoch 15/100, Batch 200/1314, Loss: 0.0279\n",
      "Epoch 15/100, Batch 250/1314, Loss: 0.0013\n",
      "Epoch 15/100, Batch 300/1314, Loss: 0.0070\n",
      "Epoch 15/100, Batch 350/1314, Loss: 0.0048\n",
      "Epoch 15/100, Batch 400/1314, Loss: 0.0035\n",
      "Epoch 15/100, Batch 450/1314, Loss: 0.0017\n",
      "Epoch 15/100, Batch 500/1314, Loss: 0.0009\n",
      "Epoch 15/100, Batch 550/1314, Loss: 0.0025\n",
      "Epoch 15/100, Batch 600/1314, Loss: 0.0050\n",
      "Epoch 15/100, Batch 650/1314, Loss: 0.0011\n",
      "Epoch 15/100, Batch 700/1314, Loss: 0.0008\n",
      "Epoch 15/100, Batch 750/1314, Loss: 0.0056\n",
      "Epoch 15/100, Batch 800/1314, Loss: 0.0056\n",
      "Epoch 15/100, Batch 850/1314, Loss: 0.0040\n",
      "Epoch 15/100, Batch 900/1314, Loss: 0.0044\n",
      "Epoch 15/100, Batch 950/1314, Loss: 0.0017\n",
      "Epoch 15/100, Batch 1000/1314, Loss: 0.0007\n",
      "Epoch 15/100, Batch 1050/1314, Loss: 0.0025\n",
      "Epoch 15/100, Batch 1100/1314, Loss: 0.0023\n",
      "Epoch 15/100, Batch 1150/1314, Loss: 0.0012\n",
      "Epoch 15/100, Batch 1200/1314, Loss: 0.0010\n",
      "Epoch 15/100, Batch 1250/1314, Loss: 0.0013\n",
      "Epoch 15/100, Batch 1300/1314, Loss: 0.0026\n",
      "\n",
      "Epoch 15/100 Results:\n",
      "Train Loss: 0.0034, Val Loss: 0.2660\n",
      "Substance Accuracy: 0.8967\n",
      "Symptom F1: 0.8462, Precision: 0.8741, Recall: 0.8201\n",
      "Learning Rate: 0.000198\n",
      "------------------------------------------------------------\n",
      "Epoch 16/100, Batch 0/1314, Loss: 0.0031\n",
      "Epoch 16/100, Batch 50/1314, Loss: 0.0013\n",
      "Epoch 16/100, Batch 100/1314, Loss: 0.0021\n",
      "Epoch 16/100, Batch 150/1314, Loss: 0.0006\n",
      "Epoch 16/100, Batch 200/1314, Loss: 0.0048\n",
      "Epoch 16/100, Batch 250/1314, Loss: 0.0002\n",
      "Epoch 16/100, Batch 300/1314, Loss: 0.0044\n",
      "Epoch 16/100, Batch 350/1314, Loss: 0.0038\n",
      "Epoch 16/100, Batch 400/1314, Loss: 0.0032\n",
      "Epoch 16/100, Batch 450/1314, Loss: 0.0047\n",
      "Epoch 16/100, Batch 500/1314, Loss: 0.0009\n",
      "Epoch 16/100, Batch 550/1314, Loss: 0.0010\n",
      "Epoch 16/100, Batch 600/1314, Loss: 0.0038\n",
      "Epoch 16/100, Batch 650/1314, Loss: 0.0007\n",
      "Epoch 16/100, Batch 700/1314, Loss: 0.0028\n",
      "Epoch 16/100, Batch 750/1314, Loss: 0.0006\n",
      "Epoch 16/100, Batch 800/1314, Loss: 0.0060\n",
      "Epoch 16/100, Batch 850/1314, Loss: 0.0027\n",
      "Epoch 16/100, Batch 900/1314, Loss: 0.0009\n",
      "Epoch 16/100, Batch 950/1314, Loss: 0.0049\n",
      "Epoch 16/100, Batch 1000/1314, Loss: 0.0039\n",
      "Epoch 16/100, Batch 1050/1314, Loss: 0.0137\n",
      "Epoch 16/100, Batch 1100/1314, Loss: 0.0014\n",
      "Epoch 16/100, Batch 1150/1314, Loss: 0.0010\n",
      "Epoch 16/100, Batch 1200/1314, Loss: 0.0026\n",
      "Epoch 16/100, Batch 1250/1314, Loss: 0.0027\n",
      "Epoch 16/100, Batch 1300/1314, Loss: 0.0072\n",
      "\n",
      "Epoch 16/100 Results:\n",
      "Train Loss: 0.0031, Val Loss: 0.3236\n",
      "Substance Accuracy: 0.8732\n",
      "Symptom F1: 0.8494, Precision: 0.8719, Recall: 0.8279\n",
      "Learning Rate: 0.000198\n",
      "------------------------------------------------------------\n",
      "Epoch 17/100, Batch 0/1314, Loss: 0.0026\n",
      "Epoch 17/100, Batch 50/1314, Loss: 0.0005\n",
      "Epoch 17/100, Batch 100/1314, Loss: 0.0001\n",
      "Epoch 17/100, Batch 150/1314, Loss: 0.0045\n",
      "Epoch 17/100, Batch 200/1314, Loss: 0.0018\n",
      "Epoch 17/100, Batch 250/1314, Loss: 0.0015\n",
      "Epoch 17/100, Batch 300/1314, Loss: 0.0051\n",
      "Epoch 17/100, Batch 350/1314, Loss: 0.0034\n",
      "Epoch 17/100, Batch 400/1314, Loss: 0.0013\n",
      "Epoch 17/100, Batch 450/1314, Loss: 0.0037\n",
      "Epoch 17/100, Batch 500/1314, Loss: 0.0025\n",
      "Epoch 17/100, Batch 550/1314, Loss: 0.0011\n",
      "Epoch 17/100, Batch 600/1314, Loss: 0.0030\n",
      "Epoch 17/100, Batch 650/1314, Loss: 0.0042\n",
      "Epoch 17/100, Batch 700/1314, Loss: 0.0033\n",
      "Epoch 17/100, Batch 750/1314, Loss: 0.0024\n",
      "Epoch 17/100, Batch 800/1314, Loss: 0.0017\n",
      "Epoch 17/100, Batch 850/1314, Loss: 0.0003\n",
      "Epoch 17/100, Batch 900/1314, Loss: 0.0026\n",
      "Epoch 17/100, Batch 950/1314, Loss: 0.0004\n",
      "Epoch 17/100, Batch 1000/1314, Loss: 0.0015\n",
      "Epoch 17/100, Batch 1050/1314, Loss: 0.0045\n",
      "Epoch 17/100, Batch 1100/1314, Loss: 0.0048\n",
      "Epoch 17/100, Batch 1150/1314, Loss: 0.0023\n",
      "Epoch 17/100, Batch 1200/1314, Loss: 0.0007\n",
      "Epoch 17/100, Batch 1250/1314, Loss: 0.0047\n",
      "Epoch 17/100, Batch 1300/1314, Loss: 0.0020\n",
      "\n",
      "Epoch 17/100 Results:\n",
      "Train Loss: 0.0027, Val Loss: 0.3287\n",
      "Substance Accuracy: 0.8969\n",
      "Symptom F1: 0.8424, Precision: 0.8735, Recall: 0.8134\n",
      "Learning Rate: 0.000197\n",
      "------------------------------------------------------------\n",
      "Epoch 18/100, Batch 0/1314, Loss: 0.0047\n",
      "Epoch 18/100, Batch 50/1314, Loss: 0.0013\n",
      "Epoch 18/100, Batch 100/1314, Loss: 0.0015\n",
      "Epoch 18/100, Batch 150/1314, Loss: 0.0055\n",
      "Epoch 18/100, Batch 200/1314, Loss: 0.0003\n",
      "Epoch 18/100, Batch 250/1314, Loss: 0.0010\n",
      "Epoch 18/100, Batch 300/1314, Loss: 0.0006\n",
      "Epoch 18/100, Batch 350/1314, Loss: 0.0001\n",
      "Epoch 18/100, Batch 400/1314, Loss: 0.0002\n",
      "Epoch 18/100, Batch 450/1314, Loss: 0.0010\n",
      "Epoch 18/100, Batch 500/1314, Loss: 0.0005\n",
      "Epoch 18/100, Batch 550/1314, Loss: 0.0016\n",
      "Epoch 18/100, Batch 600/1314, Loss: 0.0015\n",
      "Epoch 18/100, Batch 650/1314, Loss: 0.0098\n",
      "Epoch 18/100, Batch 700/1314, Loss: 0.0026\n",
      "Epoch 18/100, Batch 750/1314, Loss: 0.0068\n",
      "Epoch 18/100, Batch 800/1314, Loss: 0.0027\n",
      "Epoch 18/100, Batch 850/1314, Loss: 0.0011\n",
      "Epoch 18/100, Batch 900/1314, Loss: 0.0015\n",
      "Epoch 18/100, Batch 950/1314, Loss: 0.0071\n",
      "Epoch 18/100, Batch 1000/1314, Loss: 0.0046\n",
      "Epoch 18/100, Batch 1050/1314, Loss: 0.0006\n",
      "Epoch 18/100, Batch 1100/1314, Loss: 0.0005\n",
      "Epoch 18/100, Batch 1150/1314, Loss: 0.0004\n",
      "Epoch 18/100, Batch 1200/1314, Loss: 0.0037\n",
      "Epoch 18/100, Batch 1250/1314, Loss: 0.0028\n",
      "Epoch 18/100, Batch 1300/1314, Loss: 0.0019\n",
      "New best model saved! Substance accuracy: 0.9189\n",
      "\n",
      "Epoch 18/100 Results:\n",
      "Train Loss: 0.0025, Val Loss: 0.2399\n",
      "Substance Accuracy: 0.9189\n",
      "Symptom F1: 0.8510, Precision: 0.8796, Recall: 0.8241\n",
      "Learning Rate: 0.000196\n",
      "------------------------------------------------------------\n",
      "Epoch 19/100, Batch 0/1314, Loss: 0.0016\n",
      "Epoch 19/100, Batch 50/1314, Loss: 0.0006\n",
      "Epoch 19/100, Batch 100/1314, Loss: 0.0017\n",
      "Epoch 19/100, Batch 150/1314, Loss: 0.0021\n",
      "Epoch 19/100, Batch 200/1314, Loss: 0.0037\n",
      "Epoch 19/100, Batch 250/1314, Loss: 0.0029\n",
      "Epoch 19/100, Batch 300/1314, Loss: 0.0012\n",
      "Epoch 19/100, Batch 350/1314, Loss: 0.0009\n",
      "Epoch 19/100, Batch 400/1314, Loss: 0.0006\n",
      "Epoch 19/100, Batch 450/1314, Loss: 0.0039\n",
      "Epoch 19/100, Batch 500/1314, Loss: 0.0002\n",
      "Epoch 19/100, Batch 550/1314, Loss: 0.0011\n",
      "Epoch 19/100, Batch 600/1314, Loss: 0.0005\n",
      "Epoch 19/100, Batch 650/1314, Loss: 0.0009\n",
      "Epoch 19/100, Batch 700/1314, Loss: 0.0005\n",
      "Epoch 19/100, Batch 750/1314, Loss: 0.0067\n",
      "Epoch 19/100, Batch 800/1314, Loss: 0.0004\n",
      "Epoch 19/100, Batch 850/1314, Loss: 0.0012\n",
      "Epoch 19/100, Batch 900/1314, Loss: 0.0145\n",
      "Epoch 19/100, Batch 950/1314, Loss: 0.0006\n",
      "Epoch 19/100, Batch 1000/1314, Loss: 0.0060\n",
      "Epoch 19/100, Batch 1050/1314, Loss: 0.0009\n",
      "Epoch 19/100, Batch 1100/1314, Loss: 0.0014\n",
      "Epoch 19/100, Batch 1150/1314, Loss: 0.0018\n",
      "Epoch 19/100, Batch 1200/1314, Loss: 0.0014\n",
      "Epoch 19/100, Batch 1250/1314, Loss: 0.0010\n",
      "Epoch 19/100, Batch 1300/1314, Loss: 0.0003\n",
      "\n",
      "Epoch 19/100 Results:\n",
      "Train Loss: 0.0024, Val Loss: 0.3240\n",
      "Substance Accuracy: 0.8935\n",
      "Symptom F1: 0.8545, Precision: 0.8849, Recall: 0.8262\n",
      "Learning Rate: 0.000195\n",
      "------------------------------------------------------------\n",
      "Epoch 20/100, Batch 0/1314, Loss: 0.0053\n",
      "Epoch 20/100, Batch 50/1314, Loss: 0.0005\n",
      "Epoch 20/100, Batch 100/1314, Loss: 0.0013\n",
      "Epoch 20/100, Batch 150/1314, Loss: 0.0007\n",
      "Epoch 20/100, Batch 200/1314, Loss: 0.0006\n",
      "Epoch 20/100, Batch 250/1314, Loss: 0.0004\n",
      "Epoch 20/100, Batch 300/1314, Loss: 0.0003\n",
      "Epoch 20/100, Batch 350/1314, Loss: 0.0000\n",
      "Epoch 20/100, Batch 400/1314, Loss: 0.0024\n",
      "Epoch 20/100, Batch 450/1314, Loss: 0.0024\n",
      "Epoch 20/100, Batch 500/1314, Loss: 0.0041\n",
      "Epoch 20/100, Batch 550/1314, Loss: 0.0025\n",
      "Epoch 20/100, Batch 600/1314, Loss: 0.0018\n",
      "Epoch 20/100, Batch 650/1314, Loss: 0.0009\n",
      "Epoch 20/100, Batch 700/1314, Loss: 0.0000\n",
      "Epoch 20/100, Batch 750/1314, Loss: 0.0001\n",
      "Epoch 20/100, Batch 800/1314, Loss: 0.0005\n",
      "Epoch 20/100, Batch 850/1314, Loss: 0.0029\n",
      "Epoch 20/100, Batch 900/1314, Loss: 0.0004\n",
      "Epoch 20/100, Batch 950/1314, Loss: 0.0009\n",
      "Epoch 20/100, Batch 1000/1314, Loss: 0.0132\n",
      "Epoch 20/100, Batch 1050/1314, Loss: 0.0007\n",
      "Epoch 20/100, Batch 1100/1314, Loss: 0.0025\n",
      "Epoch 20/100, Batch 1150/1314, Loss: 0.0021\n",
      "Epoch 20/100, Batch 1200/1314, Loss: 0.0008\n",
      "Epoch 20/100, Batch 1250/1314, Loss: 0.0005\n",
      "Epoch 20/100, Batch 1300/1314, Loss: 0.0014\n",
      "\n",
      "Epoch 20/100 Results:\n",
      "Train Loss: 0.0022, Val Loss: 0.3097\n",
      "Substance Accuracy: 0.9022\n",
      "Symptom F1: 0.8532, Precision: 0.8812, Recall: 0.8269\n",
      "Learning Rate: 0.000194\n",
      "------------------------------------------------------------\n",
      "Epoch 21/100, Batch 0/1314, Loss: 0.0013\n",
      "Epoch 21/100, Batch 50/1314, Loss: 0.0042\n",
      "Epoch 21/100, Batch 100/1314, Loss: 0.0011\n",
      "Epoch 21/100, Batch 150/1314, Loss: 0.0013\n",
      "Epoch 21/100, Batch 200/1314, Loss: 0.0010\n",
      "Epoch 21/100, Batch 250/1314, Loss: 0.0038\n",
      "Epoch 21/100, Batch 300/1314, Loss: 0.0080\n",
      "Epoch 21/100, Batch 350/1314, Loss: 0.0058\n",
      "Epoch 21/100, Batch 400/1314, Loss: 0.0009\n",
      "Epoch 21/100, Batch 450/1314, Loss: 0.0014\n",
      "Epoch 21/100, Batch 500/1314, Loss: 0.0026\n",
      "Epoch 21/100, Batch 550/1314, Loss: 0.0005\n",
      "Epoch 21/100, Batch 600/1314, Loss: 0.0014\n",
      "Epoch 21/100, Batch 650/1314, Loss: 0.0013\n",
      "Epoch 21/100, Batch 700/1314, Loss: 0.0001\n",
      "Epoch 21/100, Batch 750/1314, Loss: 0.0012\n",
      "Epoch 21/100, Batch 800/1314, Loss: 0.0036\n",
      "Epoch 21/100, Batch 850/1314, Loss: 0.0025\n",
      "Epoch 21/100, Batch 900/1314, Loss: 0.0012\n",
      "Epoch 21/100, Batch 950/1314, Loss: 0.0017\n",
      "Epoch 21/100, Batch 1000/1314, Loss: 0.0031\n",
      "Epoch 21/100, Batch 1050/1314, Loss: 0.0019\n",
      "Epoch 21/100, Batch 1100/1314, Loss: 0.0027\n",
      "Epoch 21/100, Batch 1150/1314, Loss: 0.0002\n",
      "Epoch 21/100, Batch 1200/1314, Loss: 0.0014\n",
      "Epoch 21/100, Batch 1250/1314, Loss: 0.0014\n",
      "Epoch 21/100, Batch 1300/1314, Loss: 0.0001\n",
      "\n",
      "Epoch 21/100 Results:\n",
      "Train Loss: 0.0020, Val Loss: 0.3142\n",
      "Substance Accuracy: 0.8995\n",
      "Symptom F1: 0.8476, Precision: 0.8757, Recall: 0.8213\n",
      "Learning Rate: 0.000193\n",
      "------------------------------------------------------------\n",
      "Epoch 22/100, Batch 0/1314, Loss: 0.0007\n",
      "Epoch 22/100, Batch 50/1314, Loss: 0.0002\n",
      "Epoch 22/100, Batch 100/1314, Loss: 0.0003\n",
      "Epoch 22/100, Batch 150/1314, Loss: 0.0008\n",
      "Epoch 22/100, Batch 200/1314, Loss: 0.0004\n",
      "Epoch 22/100, Batch 250/1314, Loss: 0.0028\n",
      "Epoch 22/100, Batch 300/1314, Loss: 0.0035\n",
      "Epoch 22/100, Batch 350/1314, Loss: 0.0019\n",
      "Epoch 22/100, Batch 400/1314, Loss: 0.0040\n",
      "Epoch 22/100, Batch 450/1314, Loss: 0.0006\n",
      "Epoch 22/100, Batch 500/1314, Loss: 0.0013\n",
      "Epoch 22/100, Batch 550/1314, Loss: 0.0014\n",
      "Epoch 22/100, Batch 600/1314, Loss: 0.0069\n",
      "Epoch 22/100, Batch 650/1314, Loss: 0.0003\n",
      "Epoch 22/100, Batch 700/1314, Loss: 0.0022\n",
      "Epoch 22/100, Batch 750/1314, Loss: 0.0013\n",
      "Epoch 22/100, Batch 800/1314, Loss: 0.0007\n",
      "Epoch 22/100, Batch 850/1314, Loss: 0.0002\n",
      "Epoch 22/100, Batch 900/1314, Loss: 0.0006\n",
      "Epoch 22/100, Batch 950/1314, Loss: 0.0000\n",
      "Epoch 22/100, Batch 1000/1314, Loss: 0.0019\n",
      "Epoch 22/100, Batch 1050/1314, Loss: 0.0005\n",
      "Epoch 22/100, Batch 1100/1314, Loss: 0.0002\n",
      "Epoch 22/100, Batch 1150/1314, Loss: 0.0007\n",
      "Epoch 22/100, Batch 1200/1314, Loss: 0.0026\n",
      "Epoch 22/100, Batch 1250/1314, Loss: 0.0017\n",
      "Epoch 22/100, Batch 1300/1314, Loss: 0.0045\n",
      "\n",
      "Epoch 22/100 Results:\n",
      "Train Loss: 0.0019, Val Loss: 0.3679\n",
      "Substance Accuracy: 0.8976\n",
      "Symptom F1: 0.8420, Precision: 0.8727, Recall: 0.8134\n",
      "Learning Rate: 0.000191\n",
      "------------------------------------------------------------\n",
      "Epoch 23/100, Batch 0/1314, Loss: 0.0004\n",
      "Epoch 23/100, Batch 50/1314, Loss: 0.0013\n",
      "Epoch 23/100, Batch 100/1314, Loss: 0.0005\n",
      "Epoch 23/100, Batch 150/1314, Loss: 0.0001\n",
      "Epoch 23/100, Batch 200/1314, Loss: 0.0046\n",
      "Epoch 23/100, Batch 250/1314, Loss: 0.0004\n",
      "Epoch 23/100, Batch 300/1314, Loss: 0.0001\n",
      "Epoch 23/100, Batch 350/1314, Loss: 0.0037\n",
      "Epoch 23/100, Batch 400/1314, Loss: 0.0009\n",
      "Epoch 23/100, Batch 450/1314, Loss: 0.0000\n",
      "Epoch 23/100, Batch 500/1314, Loss: 0.0007\n",
      "Epoch 23/100, Batch 550/1314, Loss: 0.0000\n",
      "Epoch 23/100, Batch 600/1314, Loss: 0.0008\n",
      "Epoch 23/100, Batch 650/1314, Loss: 0.0004\n",
      "Epoch 23/100, Batch 700/1314, Loss: 0.0002\n",
      "Epoch 23/100, Batch 750/1314, Loss: 0.0018\n",
      "Epoch 23/100, Batch 800/1314, Loss: 0.0004\n",
      "Epoch 23/100, Batch 850/1314, Loss: 0.0011\n",
      "Epoch 23/100, Batch 900/1314, Loss: 0.0029\n",
      "Epoch 23/100, Batch 950/1314, Loss: 0.0002\n",
      "Epoch 23/100, Batch 1000/1314, Loss: 0.0012\n",
      "Epoch 23/100, Batch 1050/1314, Loss: 0.0088\n",
      "Epoch 23/100, Batch 1100/1314, Loss: 0.0033\n",
      "Epoch 23/100, Batch 1150/1314, Loss: 0.0028\n",
      "Epoch 23/100, Batch 1200/1314, Loss: 0.0019\n",
      "Epoch 23/100, Batch 1250/1314, Loss: 0.0005\n",
      "Epoch 23/100, Batch 1300/1314, Loss: 0.0015\n",
      "\n",
      "Epoch 23/100 Results:\n",
      "Train Loss: 0.0017, Val Loss: 0.3419\n",
      "Substance Accuracy: 0.8927\n",
      "Symptom F1: 0.8517, Precision: 0.8867, Recall: 0.8194\n",
      "Learning Rate: 0.000190\n",
      "------------------------------------------------------------\n",
      "Epoch 24/100, Batch 0/1314, Loss: 0.0010\n",
      "Epoch 24/100, Batch 50/1314, Loss: 0.0005\n",
      "Epoch 24/100, Batch 100/1314, Loss: 0.0006\n",
      "Epoch 24/100, Batch 150/1314, Loss: 0.0009\n",
      "Epoch 24/100, Batch 200/1314, Loss: 0.0029\n",
      "Epoch 24/100, Batch 250/1314, Loss: 0.0005\n",
      "Epoch 24/100, Batch 300/1314, Loss: 0.0006\n",
      "Epoch 24/100, Batch 350/1314, Loss: 0.0014\n",
      "Epoch 24/100, Batch 400/1314, Loss: 0.0009\n",
      "Epoch 24/100, Batch 450/1314, Loss: 0.0014\n",
      "Epoch 24/100, Batch 500/1314, Loss: 0.0004\n",
      "Epoch 24/100, Batch 550/1314, Loss: 0.0005\n",
      "Epoch 24/100, Batch 600/1314, Loss: 0.0001\n",
      "Epoch 24/100, Batch 650/1314, Loss: 0.0008\n",
      "Epoch 24/100, Batch 700/1314, Loss: 0.0015\n",
      "Epoch 24/100, Batch 750/1314, Loss: 0.0020\n",
      "Epoch 24/100, Batch 800/1314, Loss: 0.0006\n",
      "Epoch 24/100, Batch 850/1314, Loss: 0.0005\n",
      "Epoch 24/100, Batch 900/1314, Loss: 0.0054\n",
      "Epoch 24/100, Batch 950/1314, Loss: 0.0054\n",
      "Epoch 24/100, Batch 1000/1314, Loss: 0.0018\n",
      "Epoch 24/100, Batch 1050/1314, Loss: 0.0041\n",
      "Epoch 24/100, Batch 1100/1314, Loss: 0.0050\n",
      "Epoch 24/100, Batch 1150/1314, Loss: 0.0013\n",
      "Epoch 24/100, Batch 1200/1314, Loss: 0.0019\n",
      "Epoch 24/100, Batch 1250/1314, Loss: 0.0013\n",
      "Epoch 24/100, Batch 1300/1314, Loss: 0.0075\n",
      "\n",
      "Epoch 24/100 Results:\n",
      "Train Loss: 0.0018, Val Loss: 0.2625\n",
      "Substance Accuracy: 0.9115\n",
      "Symptom F1: 0.8560, Precision: 0.8949, Recall: 0.8204\n",
      "Learning Rate: 0.000188\n",
      "------------------------------------------------------------\n",
      "Epoch 25/100, Batch 0/1314, Loss: 0.0032\n",
      "Epoch 25/100, Batch 50/1314, Loss: 0.0003\n",
      "Epoch 25/100, Batch 100/1314, Loss: 0.0046\n",
      "Epoch 25/100, Batch 150/1314, Loss: 0.0002\n",
      "Epoch 25/100, Batch 200/1314, Loss: 0.0006\n",
      "Epoch 25/100, Batch 250/1314, Loss: 0.0016\n",
      "Epoch 25/100, Batch 300/1314, Loss: 0.0000\n",
      "Epoch 25/100, Batch 350/1314, Loss: 0.0001\n",
      "Epoch 25/100, Batch 400/1314, Loss: 0.0006\n",
      "Epoch 25/100, Batch 450/1314, Loss: 0.0011\n",
      "Epoch 25/100, Batch 500/1314, Loss: 0.0008\n",
      "Epoch 25/100, Batch 550/1314, Loss: 0.0000\n",
      "Epoch 25/100, Batch 600/1314, Loss: 0.0143\n",
      "Epoch 25/100, Batch 650/1314, Loss: 0.0006\n",
      "Epoch 25/100, Batch 700/1314, Loss: 0.0001\n",
      "Epoch 25/100, Batch 750/1314, Loss: 0.0003\n",
      "Epoch 25/100, Batch 800/1314, Loss: 0.0016\n",
      "Epoch 25/100, Batch 850/1314, Loss: 0.0005\n",
      "Epoch 25/100, Batch 900/1314, Loss: 0.0015\n",
      "Epoch 25/100, Batch 950/1314, Loss: 0.0002\n",
      "Epoch 25/100, Batch 1000/1314, Loss: 0.0014\n",
      "Epoch 25/100, Batch 1050/1314, Loss: 0.0106\n",
      "Epoch 25/100, Batch 1100/1314, Loss: 0.0002\n",
      "Epoch 25/100, Batch 1150/1314, Loss: 0.0011\n",
      "Epoch 25/100, Batch 1200/1314, Loss: 0.0029\n",
      "Epoch 25/100, Batch 1250/1314, Loss: 0.0009\n",
      "Epoch 25/100, Batch 1300/1314, Loss: 0.0004\n",
      "New best model saved! Substance accuracy: 0.9221\n",
      "\n",
      "Epoch 25/100 Results:\n",
      "Train Loss: 0.0016, Val Loss: 0.2728\n",
      "Substance Accuracy: 0.9221\n",
      "Symptom F1: 0.8580, Precision: 0.8906, Recall: 0.8277\n",
      "Learning Rate: 0.000187\n",
      "------------------------------------------------------------\n",
      "Epoch 26/100, Batch 0/1314, Loss: 0.0037\n",
      "Epoch 26/100, Batch 50/1314, Loss: 0.0002\n",
      "Epoch 26/100, Batch 100/1314, Loss: 0.0001\n",
      "Epoch 26/100, Batch 150/1314, Loss: 0.0051\n",
      "Epoch 26/100, Batch 200/1314, Loss: 0.0028\n",
      "Epoch 26/100, Batch 250/1314, Loss: 0.0006\n",
      "Epoch 26/100, Batch 300/1314, Loss: 0.0032\n",
      "Epoch 26/100, Batch 350/1314, Loss: 0.0003\n",
      "Epoch 26/100, Batch 400/1314, Loss: 0.0004\n",
      "Epoch 26/100, Batch 450/1314, Loss: 0.0015\n",
      "Epoch 26/100, Batch 500/1314, Loss: 0.0082\n",
      "Epoch 26/100, Batch 550/1314, Loss: 0.0001\n",
      "Epoch 26/100, Batch 600/1314, Loss: 0.0001\n",
      "Epoch 26/100, Batch 650/1314, Loss: 0.0003\n",
      "Epoch 26/100, Batch 700/1314, Loss: 0.0001\n",
      "Epoch 26/100, Batch 750/1314, Loss: 0.0002\n",
      "Epoch 26/100, Batch 800/1314, Loss: 0.0022\n",
      "Epoch 26/100, Batch 850/1314, Loss: 0.0002\n",
      "Epoch 26/100, Batch 900/1314, Loss: 0.0032\n",
      "Epoch 26/100, Batch 950/1314, Loss: 0.0004\n",
      "Epoch 26/100, Batch 1000/1314, Loss: 0.0013\n",
      "Epoch 26/100, Batch 1050/1314, Loss: 0.0000\n",
      "Epoch 26/100, Batch 1100/1314, Loss: 0.0001\n",
      "Epoch 26/100, Batch 1150/1314, Loss: 0.0000\n",
      "Epoch 26/100, Batch 1200/1314, Loss: 0.0171\n",
      "Epoch 26/100, Batch 1250/1314, Loss: 0.0035\n",
      "Epoch 26/100, Batch 1300/1314, Loss: 0.0009\n",
      "New best model saved! Substance accuracy: 0.9342\n",
      "\n",
      "Epoch 26/100 Results:\n",
      "Train Loss: 0.0015, Val Loss: 0.2329\n",
      "Substance Accuracy: 0.9342\n",
      "Symptom F1: 0.8617, Precision: 0.8934, Recall: 0.8322\n",
      "Learning Rate: 0.000185\n",
      "------------------------------------------------------------\n",
      "Epoch 27/100, Batch 0/1314, Loss: 0.0000\n",
      "Epoch 27/100, Batch 50/1314, Loss: 0.0005\n",
      "Epoch 27/100, Batch 100/1314, Loss: 0.0031\n",
      "Epoch 27/100, Batch 150/1314, Loss: 0.0011\n",
      "Epoch 27/100, Batch 200/1314, Loss: 0.0074\n",
      "Epoch 27/100, Batch 250/1314, Loss: 0.0013\n",
      "Epoch 27/100, Batch 300/1314, Loss: 0.0043\n",
      "Epoch 27/100, Batch 350/1314, Loss: 0.0008\n",
      "Epoch 27/100, Batch 400/1314, Loss: 0.0009\n",
      "Epoch 27/100, Batch 450/1314, Loss: 0.0021\n",
      "Epoch 27/100, Batch 500/1314, Loss: 0.0002\n",
      "Epoch 27/100, Batch 550/1314, Loss: 0.0005\n",
      "Epoch 27/100, Batch 600/1314, Loss: 0.0012\n",
      "Epoch 27/100, Batch 650/1314, Loss: 0.0039\n",
      "Epoch 27/100, Batch 700/1314, Loss: 0.0015\n",
      "Epoch 27/100, Batch 750/1314, Loss: 0.0005\n",
      "Epoch 27/100, Batch 800/1314, Loss: 0.0001\n",
      "Epoch 27/100, Batch 850/1314, Loss: 0.0029\n",
      "Epoch 27/100, Batch 900/1314, Loss: 0.0002\n",
      "Epoch 27/100, Batch 950/1314, Loss: 0.0001\n",
      "Epoch 27/100, Batch 1000/1314, Loss: 0.0045\n",
      "Epoch 27/100, Batch 1050/1314, Loss: 0.0002\n",
      "Epoch 27/100, Batch 1100/1314, Loss: 0.0001\n",
      "Epoch 27/100, Batch 1150/1314, Loss: 0.0001\n",
      "Epoch 27/100, Batch 1200/1314, Loss: 0.0015\n",
      "Epoch 27/100, Batch 1250/1314, Loss: 0.0031\n",
      "Epoch 27/100, Batch 1300/1314, Loss: 0.0009\n",
      "\n",
      "Epoch 27/100 Results:\n",
      "Train Loss: 0.0015, Val Loss: 0.3144\n",
      "Substance Accuracy: 0.9147\n",
      "Symptom F1: 0.8496, Precision: 0.8843, Recall: 0.8175\n",
      "Learning Rate: 0.000183\n",
      "------------------------------------------------------------\n",
      "Epoch 28/100, Batch 0/1314, Loss: 0.0006\n",
      "Epoch 28/100, Batch 50/1314, Loss: 0.0016\n",
      "Epoch 28/100, Batch 100/1314, Loss: 0.0001\n",
      "Epoch 28/100, Batch 150/1314, Loss: 0.0012\n",
      "Epoch 28/100, Batch 200/1314, Loss: 0.0009\n",
      "Epoch 28/100, Batch 250/1314, Loss: 0.0007\n",
      "Epoch 28/100, Batch 300/1314, Loss: 0.0002\n",
      "Epoch 28/100, Batch 350/1314, Loss: 0.0004\n",
      "Epoch 28/100, Batch 400/1314, Loss: 0.0004\n",
      "Epoch 28/100, Batch 450/1314, Loss: 0.0030\n",
      "Epoch 28/100, Batch 500/1314, Loss: 0.0001\n",
      "Epoch 28/100, Batch 550/1314, Loss: 0.0067\n",
      "Epoch 28/100, Batch 600/1314, Loss: 0.0101\n",
      "Epoch 28/100, Batch 650/1314, Loss: 0.0013\n",
      "Epoch 28/100, Batch 700/1314, Loss: 0.0011\n",
      "Epoch 28/100, Batch 750/1314, Loss: 0.0011\n",
      "Epoch 28/100, Batch 800/1314, Loss: 0.0002\n",
      "Epoch 28/100, Batch 850/1314, Loss: 0.0015\n",
      "Epoch 28/100, Batch 900/1314, Loss: 0.0006\n",
      "Epoch 28/100, Batch 950/1314, Loss: 0.0005\n",
      "Epoch 28/100, Batch 1000/1314, Loss: 0.0023\n",
      "Epoch 28/100, Batch 1050/1314, Loss: 0.0002\n",
      "Epoch 28/100, Batch 1100/1314, Loss: 0.0002\n",
      "Epoch 28/100, Batch 1150/1314, Loss: 0.0067\n",
      "Epoch 28/100, Batch 1200/1314, Loss: 0.0002\n",
      "Epoch 28/100, Batch 1250/1314, Loss: 0.0004\n",
      "Epoch 28/100, Batch 1300/1314, Loss: 0.0004\n",
      "\n",
      "Epoch 28/100 Results:\n",
      "Train Loss: 0.0012, Val Loss: 0.2325\n",
      "Substance Accuracy: 0.9339\n",
      "Symptom F1: 0.8662, Precision: 0.8968, Recall: 0.8377\n",
      "Learning Rate: 0.000181\n",
      "------------------------------------------------------------\n",
      "Epoch 29/100, Batch 0/1314, Loss: 0.0001\n",
      "Epoch 29/100, Batch 50/1314, Loss: 0.0005\n",
      "Epoch 29/100, Batch 100/1314, Loss: 0.0013\n",
      "Epoch 29/100, Batch 150/1314, Loss: 0.0004\n",
      "Epoch 29/100, Batch 200/1314, Loss: 0.0004\n",
      "Epoch 29/100, Batch 250/1314, Loss: 0.0025\n",
      "Epoch 29/100, Batch 300/1314, Loss: 0.0012\n",
      "Epoch 29/100, Batch 350/1314, Loss: 0.0001\n",
      "Epoch 29/100, Batch 400/1314, Loss: 0.0000\n",
      "Epoch 29/100, Batch 450/1314, Loss: 0.0036\n",
      "Epoch 29/100, Batch 500/1314, Loss: 0.0033\n",
      "Epoch 29/100, Batch 550/1314, Loss: 0.0011\n",
      "Epoch 29/100, Batch 600/1314, Loss: 0.0001\n",
      "Epoch 29/100, Batch 650/1314, Loss: 0.0010\n",
      "Epoch 29/100, Batch 700/1314, Loss: 0.0001\n",
      "Epoch 29/100, Batch 750/1314, Loss: 0.0000\n",
      "Epoch 29/100, Batch 800/1314, Loss: 0.0001\n",
      "Epoch 29/100, Batch 850/1314, Loss: 0.0003\n",
      "Epoch 29/100, Batch 900/1314, Loss: 0.0034\n",
      "Epoch 29/100, Batch 950/1314, Loss: 0.0004\n",
      "Epoch 29/100, Batch 1000/1314, Loss: 0.0007\n",
      "Epoch 29/100, Batch 1050/1314, Loss: 0.0009\n",
      "Epoch 29/100, Batch 1100/1314, Loss: 0.0012\n",
      "Epoch 29/100, Batch 1150/1314, Loss: 0.0004\n",
      "Epoch 29/100, Batch 1200/1314, Loss: 0.0007\n",
      "Epoch 29/100, Batch 1250/1314, Loss: 0.0005\n",
      "Epoch 29/100, Batch 1300/1314, Loss: 0.0001\n",
      "\n",
      "Epoch 29/100 Results:\n",
      "Train Loss: 0.0012, Val Loss: 0.2417\n",
      "Substance Accuracy: 0.9319\n",
      "Symptom F1: 0.8692, Precision: 0.8998, Recall: 0.8407\n",
      "Learning Rate: 0.000179\n",
      "------------------------------------------------------------\n",
      "Epoch 30/100, Batch 0/1314, Loss: 0.0009\n",
      "Epoch 30/100, Batch 50/1314, Loss: 0.0015\n",
      "Epoch 30/100, Batch 100/1314, Loss: 0.0020\n",
      "Epoch 30/100, Batch 150/1314, Loss: 0.0000\n",
      "Epoch 30/100, Batch 200/1314, Loss: 0.0001\n",
      "Epoch 30/100, Batch 250/1314, Loss: 0.0028\n",
      "Epoch 30/100, Batch 300/1314, Loss: 0.0001\n",
      "Epoch 30/100, Batch 350/1314, Loss: 0.0042\n",
      "Epoch 30/100, Batch 400/1314, Loss: 0.0001\n",
      "Epoch 30/100, Batch 450/1314, Loss: 0.0001\n",
      "Epoch 30/100, Batch 500/1314, Loss: 0.0006\n",
      "Epoch 30/100, Batch 550/1314, Loss: 0.0005\n",
      "Epoch 30/100, Batch 600/1314, Loss: 0.0001\n",
      "Epoch 30/100, Batch 650/1314, Loss: 0.0006\n",
      "Epoch 30/100, Batch 700/1314, Loss: 0.0018\n",
      "Epoch 30/100, Batch 750/1314, Loss: 0.0009\n",
      "Epoch 30/100, Batch 800/1314, Loss: 0.0006\n",
      "Epoch 30/100, Batch 850/1314, Loss: 0.0044\n",
      "Epoch 30/100, Batch 900/1314, Loss: 0.0008\n",
      "Epoch 30/100, Batch 950/1314, Loss: 0.0017\n",
      "Epoch 30/100, Batch 1000/1314, Loss: 0.0038\n",
      "Epoch 30/100, Batch 1050/1314, Loss: 0.0019\n",
      "Epoch 30/100, Batch 1100/1314, Loss: 0.0010\n",
      "Epoch 30/100, Batch 1150/1314, Loss: 0.0010\n",
      "Epoch 30/100, Batch 1200/1314, Loss: 0.0003\n",
      "Epoch 30/100, Batch 1250/1314, Loss: 0.0011\n",
      "Epoch 30/100, Batch 1300/1314, Loss: 0.0001\n",
      "New best model saved! Substance accuracy: 0.9447\n",
      "\n",
      "Epoch 30/100 Results:\n",
      "Train Loss: 0.0013, Val Loss: 0.2159\n",
      "Substance Accuracy: 0.9447\n",
      "Symptom F1: 0.8702, Precision: 0.9072, Recall: 0.8361\n",
      "Learning Rate: 0.000177\n",
      "------------------------------------------------------------\n",
      "Epoch 31/100, Batch 0/1314, Loss: 0.0002\n",
      "Epoch 31/100, Batch 50/1314, Loss: 0.0000\n",
      "Epoch 31/100, Batch 100/1314, Loss: 0.0005\n",
      "Epoch 31/100, Batch 150/1314, Loss: 0.0002\n",
      "Epoch 31/100, Batch 200/1314, Loss: 0.0058\n",
      "Epoch 31/100, Batch 250/1314, Loss: 0.0002\n",
      "Epoch 31/100, Batch 300/1314, Loss: 0.0012\n",
      "Epoch 31/100, Batch 350/1314, Loss: 0.0007\n",
      "Epoch 31/100, Batch 400/1314, Loss: 0.0008\n",
      "Epoch 31/100, Batch 450/1314, Loss: 0.0000\n",
      "Epoch 31/100, Batch 500/1314, Loss: 0.0000\n",
      "Epoch 31/100, Batch 550/1314, Loss: 0.0021\n",
      "Epoch 31/100, Batch 600/1314, Loss: 0.0061\n",
      "Epoch 31/100, Batch 650/1314, Loss: 0.0000\n",
      "Epoch 31/100, Batch 700/1314, Loss: 0.0009\n",
      "Epoch 31/100, Batch 750/1314, Loss: 0.0000\n",
      "Epoch 31/100, Batch 800/1314, Loss: 0.0030\n",
      "Epoch 31/100, Batch 850/1314, Loss: 0.0000\n",
      "Epoch 31/100, Batch 900/1314, Loss: 0.0000\n",
      "Epoch 31/100, Batch 950/1314, Loss: 0.0011\n",
      "Epoch 31/100, Batch 1000/1314, Loss: 0.0000\n",
      "Epoch 31/100, Batch 1050/1314, Loss: 0.0020\n",
      "Epoch 31/100, Batch 1100/1314, Loss: 0.0038\n",
      "Epoch 31/100, Batch 1150/1314, Loss: 0.0000\n",
      "Epoch 31/100, Batch 1200/1314, Loss: 0.0001\n",
      "Epoch 31/100, Batch 1250/1314, Loss: 0.0002\n",
      "Epoch 31/100, Batch 1300/1314, Loss: 0.0000\n",
      "New best model saved! Substance accuracy: 0.9459\n",
      "\n",
      "Epoch 31/100 Results:\n",
      "Train Loss: 0.0011, Val Loss: 0.2270\n",
      "Substance Accuracy: 0.9459\n",
      "Symptom F1: 0.8665, Precision: 0.8982, Recall: 0.8369\n",
      "Learning Rate: 0.000174\n",
      "------------------------------------------------------------\n",
      "Epoch 32/100, Batch 0/1314, Loss: 0.0013\n",
      "Epoch 32/100, Batch 50/1314, Loss: 0.0028\n",
      "Epoch 32/100, Batch 100/1314, Loss: 0.0027\n",
      "Epoch 32/100, Batch 150/1314, Loss: 0.0001\n",
      "Epoch 32/100, Batch 200/1314, Loss: 0.0001\n",
      "Epoch 32/100, Batch 250/1314, Loss: 0.0003\n",
      "Epoch 32/100, Batch 300/1314, Loss: 0.0000\n",
      "Epoch 32/100, Batch 350/1314, Loss: 0.0002\n",
      "Epoch 32/100, Batch 400/1314, Loss: 0.0002\n",
      "Epoch 32/100, Batch 450/1314, Loss: 0.0035\n",
      "Epoch 32/100, Batch 500/1314, Loss: 0.0008\n",
      "Epoch 32/100, Batch 550/1314, Loss: 0.0003\n",
      "Epoch 32/100, Batch 600/1314, Loss: 0.0009\n",
      "Epoch 32/100, Batch 650/1314, Loss: 0.0011\n",
      "Epoch 32/100, Batch 700/1314, Loss: 0.0007\n",
      "Epoch 32/100, Batch 750/1314, Loss: 0.0015\n",
      "Epoch 32/100, Batch 800/1314, Loss: 0.0001\n",
      "Epoch 32/100, Batch 850/1314, Loss: 0.0001\n",
      "Epoch 32/100, Batch 900/1314, Loss: 0.0001\n",
      "Epoch 32/100, Batch 950/1314, Loss: 0.0006\n",
      "Epoch 32/100, Batch 1000/1314, Loss: 0.0002\n",
      "Epoch 32/100, Batch 1050/1314, Loss: 0.0003\n",
      "Epoch 32/100, Batch 1100/1314, Loss: 0.0009\n",
      "Epoch 32/100, Batch 1150/1314, Loss: 0.0000\n",
      "Epoch 32/100, Batch 1200/1314, Loss: 0.0002\n",
      "Epoch 32/100, Batch 1250/1314, Loss: 0.0002\n",
      "Epoch 32/100, Batch 1300/1314, Loss: 0.0005\n",
      "New best model saved! Substance accuracy: 0.9606\n",
      "\n",
      "Epoch 32/100 Results:\n",
      "Train Loss: 0.0011, Val Loss: 0.1637\n",
      "Substance Accuracy: 0.9606\n",
      "Symptom F1: 0.8670, Precision: 0.8985, Recall: 0.8377\n",
      "Learning Rate: 0.000172\n",
      "------------------------------------------------------------\n",
      "Target accuracy of 95% reached at epoch 32!\n",
      "\n",
      "Loaded best model with substance accuracy: 0.9606\n",
      "\n",
      "Training completed!\n",
      "\n",
      "============================================================\n",
      "TRAINING SUMMARY\n",
      "============================================================\n",
      "Total Epochs: 32\n",
      "Final Train Loss: 0.0011\n",
      "Final Val Loss: 0.1637\n",
      "Best Substance Accuracy: 0.9606\n",
      "Final Symptom F1: 0.8670\n",
      "\n",
      "Epoch-by-Epoch Progress:\n",
      "Epoch | Train Loss | Val Loss | Substance Acc | Symptom F1\n",
      "------------------------------------------------------------\n",
      "    1 |     0.1628 |   0.4193 |        0.7129 |     0.7176\n",
      "    2 |     0.0359 |   0.3822 |        0.7509 |     0.8285\n",
      "    3 |     0.0199 |   0.3675 |        0.7848 |     0.8554\n",
      "    4 |     0.0144 |   0.3085 |        0.8121 |     0.8569\n",
      "    5 |     0.0112 |   0.2563 |        0.8392 |     0.8642\n",
      "    6 |     0.0094 |   0.2105 |        0.8871 |     0.8665\n",
      "    7 |     0.0081 |   0.1923 |        0.8920 |     0.8562\n",
      "    8 |     0.0073 |   0.2105 |        0.8965 |     0.8611\n",
      "    9 |     0.0066 |   0.1562 |        0.9118 |     0.8679\n",
      "   10 |     0.0060 |   0.2450 |        0.8957 |     0.8530\n",
      "   11 |     0.0053 |   0.2265 |        0.9089 |     0.8585\n",
      "   12 |     0.0048 |   0.2246 |        0.8923 |     0.8422\n",
      "   13 |     0.0043 |   0.2578 |        0.8856 |     0.8552\n",
      "   14 |     0.0038 |   0.3181 |        0.8736 |     0.8419\n",
      "   15 |     0.0034 |   0.2660 |        0.8967 |     0.8462\n",
      "   16 |     0.0031 |   0.3236 |        0.8732 |     0.8494\n",
      "   17 |     0.0027 |   0.3287 |        0.8969 |     0.8424\n",
      "   18 |     0.0025 |   0.2399 |        0.9189 |     0.8510\n",
      "   19 |     0.0024 |   0.3240 |        0.8935 |     0.8545\n",
      "   20 |     0.0022 |   0.3097 |        0.9022 |     0.8532\n",
      "   21 |     0.0020 |   0.3142 |        0.8995 |     0.8476\n",
      "   22 |     0.0019 |   0.3679 |        0.8976 |     0.8420\n",
      "   23 |     0.0017 |   0.3419 |        0.8927 |     0.8517\n",
      "   24 |     0.0018 |   0.2625 |        0.9115 |     0.8560\n",
      "   25 |     0.0016 |   0.2728 |        0.9221 |     0.8580\n",
      "   26 |     0.0015 |   0.2329 |        0.9342 |     0.8617\n",
      "   27 |     0.0015 |   0.3144 |        0.9147 |     0.8496\n",
      "   28 |     0.0012 |   0.2325 |        0.9339 |     0.8662\n",
      "   29 |     0.0012 |   0.2417 |        0.9319 |     0.8692\n",
      "   30 |     0.0013 |   0.2159 |        0.9447 |     0.8702\n",
      "   31 |     0.0011 |   0.2270 |        0.9459 |     0.8665\n",
      "   32 |     0.0011 |   0.1637 |        0.9606 |     0.8670\n",
      "Training history saved to training_history.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score\n",
    "import numpy as np\n",
    "import gc\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "\n",
    "# Custom training function with enhancements for >95% accuracy\n",
    "def train_model(model, train_dataset, test_dataset, num_epochs=100, batch_size=32, learning_rate=2e-4):\n",
    "    # Create data loaders with larger batch size for stability\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, pin_memory=True)\n",
    "    \n",
    "    # Optimizer with weight decay and gradient clipping\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "    \n",
    "    # Learning rate scheduler with warmup\n",
    "    warmup_epochs = 10\n",
    "    def lr_lambda(epoch):\n",
    "        if epoch < warmup_epochs:\n",
    "            return float(epoch + 1) / warmup_epochs  # Linear warmup\n",
    "        return 0.5 * (1 + np.cos(np.pi * (epoch - warmup_epochs) / (num_epochs - warmup_epochs)))  # Cosine decay\n",
    "    scheduler = LambdaLR(optimizer, lr_lambda)\n",
    "    \n",
    "    # Training history\n",
    "    history = {\n",
    "        'train_loss': [], 'val_loss': [],\n",
    "        'substance_acc': [], 'symptom_f1': [],\n",
    "        'symptom_precision': [], 'symptom_recall': []\n",
    "    }\n",
    "    \n",
    "    best_substance_acc = 0.0\n",
    "    best_model_state = None\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    print(f\"Total epochs: {num_epochs}\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"Initial learning rate: {learning_rate}\")\n",
    "    print(f\"Total training batches: {len(train_loader)}\")\n",
    "    print(f\"Total validation batches: {len(test_loader)}\")\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_batches = 0\n",
    "        \n",
    "        for batch_idx, batch in enumerate(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Move data to device\n",
    "            x = batch['x'].to(device)\n",
    "            substance_labels = batch['substance_labels'].to(device)\n",
    "            symptom_labels = batch['symptom_labels'].to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(x, substance_labels=substance_labels, symptom_labels=symptom_labels)\n",
    "            loss = outputs['loss']\n",
    "            \n",
    "            # Backward pass with gradient clipping\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            train_batches += 1\n",
    "            \n",
    "            # Log progress every 50 batches\n",
    "            if batch_idx % 50 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, \"\n",
    "                      f\"Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        avg_train_loss = train_loss / train_batches\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batches = 0\n",
    "        all_substance_preds = []\n",
    "        all_substance_labels = []\n",
    "        all_symptom_preds = []\n",
    "        all_symptom_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                x = batch['x'].to(device)\n",
    "                substance_labels = batch['substance_labels'].to(device)\n",
    "                symptom_labels = batch['symptom_labels'].to(device)\n",
    "                \n",
    "                outputs = model(x, substance_labels=substance_labels, symptom_labels=symptom_labels)\n",
    "                loss = outputs['loss']\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "                val_batches += 1\n",
    "                \n",
    "                # Collect predictions\n",
    "                substance_preds = torch.argmax(outputs['substance_logits'], dim=1)\n",
    "                symptom_preds = (torch.sigmoid(outputs['symptom_logits']) > 0.5).float()\n",
    "                \n",
    "                all_substance_preds.extend(substance_preds.cpu().numpy())\n",
    "                all_substance_labels.extend(substance_labels.cpu().numpy())\n",
    "                all_symptom_preds.extend(symptom_preds.cpu().numpy())\n",
    "                all_symptom_labels.extend(symptom_labels.cpu().numpy())\n",
    "        \n",
    "        avg_val_loss = val_loss / val_batches\n",
    "        \n",
    "        # Calculate metrics\n",
    "        all_substance_preds = np.array(all_substance_preds)\n",
    "        all_substance_labels = np.array(all_substance_labels)\n",
    "        all_symptom_preds = np.array(all_symptom_preds)\n",
    "        all_symptom_labels = np.array(all_symptom_labels)\n",
    "        \n",
    "        substance_accuracy = accuracy_score(all_substance_labels, all_substance_preds)\n",
    "        symptom_f1 = f1_score(all_symptom_labels, all_symptom_preds, average='micro', zero_division=0)\n",
    "        symptom_precision = precision_score(all_symptom_labels, all_symptom_preds, average='micro', zero_division=0)\n",
    "        symptom_recall = recall_score(all_symptom_labels, all_symptom_preds, average='micro', zero_division=0)\n",
    "        \n",
    "        # Update learning rate\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save best model\n",
    "        if substance_accuracy > best_substance_acc:\n",
    "            best_substance_acc = substance_accuracy\n",
    "            best_model_state = model.state_dict().copy()\n",
    "            torch.save(best_model_state, 'best_model.pth')\n",
    "            print(f\"New best model saved! Substance accuracy: {best_substance_acc:.4f}\")\n",
    "        \n",
    "        # Store history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(avg_val_loss)\n",
    "        history['substance_acc'].append(substance_accuracy)\n",
    "        history['symptom_f1'].append(symptom_f1)\n",
    "        history['symptom_precision'].append(symptom_precision)\n",
    "        history['symptom_recall'].append(symptom_recall)\n",
    "        \n",
    "        # Print epoch results\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs} Results:\")\n",
    "        print(f\"Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"Substance Accuracy: {substance_accuracy:.4f}\")\n",
    "        print(f\"Symptom F1: {symptom_f1:.4f}, Precision: {symptom_precision:.4f}, Recall: {symptom_recall:.4f}\")\n",
    "        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        # Early stopping if accuracy target is met\n",
    "        if substance_accuracy >= 0.95:\n",
    "            print(f\"Target accuracy of 95% reached at epoch {epoch+1}!\")\n",
    "            break\n",
    "        \n",
    "        # Clear cache\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "    # Load best model\n",
    "    if best_model_state is not None:\n",
    "        model.load_state_dict(best_model_state)\n",
    "        print(f\"\\nLoaded best model with substance accuracy: {best_substance_acc:.4f}\")\n",
    "\n",
    "    return model, history\n",
    "\n",
    "# Enhanced evaluation function\n",
    "def evaluate_model(model, test_dataset, batch_size=32):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    all_substance_preds = []\n",
    "    all_substance_labels = []\n",
    "    all_symptom_preds = []\n",
    "    all_symptom_labels = []\n",
    "    all_substance_probs = []\n",
    "    all_symptom_probs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            x = batch['x'].to(device)\n",
    "            substance_labels = batch['substance_labels'].to(device)\n",
    "            symptom_labels = batch['symptom_labels'].to(device)\n",
    "            \n",
    "            outputs = model(x)\n",
    "            \n",
    "            substance_preds = torch.argmax(outputs['substance_logits'], dim=1)\n",
    "            symptom_preds = (torch.sigmoid(outputs['symptom_logits']) > 0.5).float()\n",
    "            \n",
    "            all_substance_preds.extend(substance_preds.cpu().numpy())\n",
    "            all_substance_labels.extend(substance_labels.cpu().numpy())\n",
    "            all_symptom_preds.extend(symptom_preds.cpu().numpy())\n",
    "            all_symptom_labels.extend(symptom_labels.cpu().numpy())\n",
    "            all_substance_probs.extend(torch.softmax(outputs['substance_logits'], dim=1).cpu().numpy())\n",
    "            all_symptom_probs.extend(torch.sigmoid(outputs['symptom_logits']).cpu().numpy())\n",
    "\n",
    "    return {\n",
    "        'substance_preds': np.array(all_substance_preds),\n",
    "        'substance_labels': np.array(all_substance_labels),\n",
    "        'symptom_preds': np.array(all_symptom_preds),\n",
    "        'symptom_labels': np.array(all_symptom_labels),\n",
    "        'substance_probs': np.array(all_substance_probs),\n",
    "        'symptom_probs': np.array(all_symptom_probs)\n",
    "    }\n",
    "\n",
    "# Simple text-based visualization function\n",
    "def print_training_summary(history):\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRAINING SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    epochs = len(history['train_loss'])\n",
    "    \n",
    "    print(f\"Total Epochs: {epochs}\")\n",
    "    print(f\"Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"Best Substance Accuracy: {max(history['substance_acc']):.4f}\")\n",
    "    print(f\"Final Symptom F1: {history['symptom_f1'][-1]:.4f}\")\n",
    "    \n",
    "    print(\"\\nEpoch-by-Epoch Progress:\")\n",
    "    print(\"Epoch | Train Loss | Val Loss | Substance Acc | Symptom F1\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        print(f\"{i+1:5d} | {history['train_loss'][i]:10.4f} | {history['val_loss'][i]:8.4f} | \"\n",
    "              f\"{history['substance_acc'][i]:13.4f} | {history['symptom_f1'][i]:10.4f}\")\n",
    "\n",
    "# Save training history to CSV\n",
    "def save_training_history(history, filename='training_history.csv'):\n",
    "    import csv\n",
    "    \n",
    "    with open(filename, 'w', newline='') as csvfile:\n",
    "        fieldnames = ['epoch'] + list(history.keys())\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        \n",
    "        writer.writeheader()\n",
    "        for i in range(len(history['train_loss'])):\n",
    "            row = {'epoch': i+1}\n",
    "            for key in history.keys():\n",
    "                row[key] = history[key][i]\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    print(f\"Training history saved to {filename}\")\n",
    "\n",
    "# Clear memory before training\n",
    "torch. cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "gc.collect()\n",
    "\n",
    "# Train the model with optimized parameters\n",
    "print(\"Starting model training...\")\n",
    "trained_model, training_history = train_model(\n",
    "    model=model,\n",
    "    train_dataset=train_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    num_epochs=100,  # Set to 100 as requested\n",
    "    batch_size=32,   # Increased for better gradient estimates\n",
    "    learning_rate=2e-4  # Lowered for more stable convergence\n",
    ")\n",
    "\n",
    "print(\"\\nTraining completed!\")\n",
    "print_training_summary(training_history)\n",
    "save_training_history(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af5bdb3c",
   "metadata": {},
   "source": [
    "6. Evaluate Model\n",
    "\n",
    "Evaluate and print results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "33040f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluating trained model...\n",
      "Final Evaluation Results:\n",
      "Substance Accuracy: 0.9606\n",
      "Symptom F1 Score: 0.8670\n",
      "Symptom Precision: 0.8985\n",
      "Symptom Recall: 0.8377\n",
      "\n",
      "Substance Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        none       1.00      0.89      0.94      3676\n",
      "      opioid       0.90      1.00      0.95      3502\n",
      "   stimulant       1.00      1.00      1.00      3328\n",
      "\n",
      "    accuracy                           0.96     10506\n",
      "   macro avg       0.96      0.96      0.96     10506\n",
      "weighted avg       0.96      0.96      0.96     10506\n",
      "\n",
      "\n",
      "Symptom Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "adverse_event       0.90      0.95      0.92      4559\n",
      "      anxiety       0.04      0.20      0.06         5\n",
      "    confusion       0.99      0.99      0.99       628\n",
      " constipation       0.00      0.00      0.00         2\n",
      "    dizziness       0.67      1.00      0.80        35\n",
      "   drowsiness       0.40      0.67      0.50         3\n",
      "      dyspnea       0.17      0.50      0.25        14\n",
      "      fatigue       0.15      1.00      0.26         4\n",
      "     headache       0.25      0.89      0.40        19\n",
      "     hematoma       0.06      0.17      0.08         6\n",
      "       nausea       0.91      0.94      0.93       102\n",
      "         none       0.98      0.66      0.79      4326\n",
      "     overdose       0.98      0.98      0.98       244\n",
      "         pain       0.72      0.98      0.83       530\n",
      "     pruritus       0.21      0.80      0.33         5\n",
      "         rash       0.56      0.97      0.71        30\n",
      "      seizure       0.93      0.97      0.95        79\n",
      "     vomiting       0.65      0.94      0.77        16\n",
      "\n",
      "    micro avg       0.90      0.84      0.87     10607\n",
      "    macro avg       0.53      0.76      0.59     10607\n",
      " weighted avg       0.92      0.84      0.87     10607\n",
      "  samples avg       0.84      0.84      0.84     10607\n",
      "\n",
      "\n",
      "================================================================================\n",
      "TRAINING HISTORY VISUALIZATION\n",
      "================================================================================\n",
      "\n",
      "📉 LOSS TRENDS:\n",
      "--------------------------------------------------\n",
      "E 1 Train: ███████████                    0.1628\n",
      "    Val  : ██████████████████████████████ 0.4193\n",
      "\n",
      "E 2 Train: ██                             0.0359\n",
      "    Val  : ███████████████████████████    0.3822\n",
      "\n",
      "E 3 Train: █                              0.0199\n",
      "    Val  : ██████████████████████████     0.3675\n",
      "\n",
      "E 4 Train: █                              0.0144\n",
      "    Val  : ██████████████████████         0.3085\n",
      "\n",
      "E 5 Train:                                0.0112\n",
      "    Val  : ██████████████████             0.2563\n",
      "\n",
      "E 6 Train:                                0.0094\n",
      "    Val  : ███████████████                0.2105\n",
      "\n",
      "E 7 Train:                                0.0081\n",
      "    Val  : █████████████                  0.1923\n",
      "\n",
      "E 8 Train:                                0.0073\n",
      "    Val  : ███████████████                0.2105\n",
      "\n",
      "E 9 Train:                                0.0066\n",
      "    Val  : ███████████                    0.1562\n",
      "\n",
      "E10 Train:                                0.0060\n",
      "    Val  : █████████████████              0.2450\n",
      "\n",
      "E11 Train:                                0.0053\n",
      "    Val  : ████████████████               0.2265\n",
      "\n",
      "E12 Train:                                0.0048\n",
      "    Val  : ████████████████               0.2246\n",
      "\n",
      "E13 Train:                                0.0043\n",
      "    Val  : ██████████████████             0.2578\n",
      "\n",
      "E14 Train:                                0.0038\n",
      "    Val  : ██████████████████████         0.3181\n",
      "\n",
      "E15 Train:                                0.0034\n",
      "    Val  : ███████████████████            0.2660\n",
      "\n",
      "E16 Train:                                0.0031\n",
      "    Val  : ███████████████████████        0.3236\n",
      "\n",
      "E17 Train:                                0.0027\n",
      "    Val  : ███████████████████████        0.3287\n",
      "\n",
      "E18 Train:                                0.0025\n",
      "    Val  : █████████████████              0.2399\n",
      "\n",
      "E19 Train:                                0.0024\n",
      "    Val  : ███████████████████████        0.3240\n",
      "\n",
      "E20 Train:                                0.0022\n",
      "    Val  : ██████████████████████         0.3097\n",
      "\n",
      "E21 Train:                                0.0020\n",
      "    Val  : ██████████████████████         0.3142\n",
      "\n",
      "E22 Train:                                0.0019\n",
      "    Val  : ██████████████████████████     0.3679\n",
      "\n",
      "E23 Train:                                0.0017\n",
      "    Val  : ████████████████████████       0.3419\n",
      "\n",
      "E24 Train:                                0.0018\n",
      "    Val  : ██████████████████             0.2625\n",
      "\n",
      "E25 Train:                                0.0016\n",
      "    Val  : ███████████████████            0.2728\n",
      "\n",
      "E26 Train:                                0.0015\n",
      "    Val  : ████████████████               0.2329\n",
      "\n",
      "E27 Train:                                0.0015\n",
      "    Val  : ██████████████████████         0.3144\n",
      "\n",
      "E28 Train:                                0.0012\n",
      "    Val  : ████████████████               0.2325\n",
      "\n",
      "E29 Train:                                0.0012\n",
      "    Val  : █████████████████              0.2417\n",
      "\n",
      "E30 Train:                                0.0013\n",
      "    Val  : ███████████████                0.2159\n",
      "\n",
      "E31 Train:                                0.0011\n",
      "    Val  : ████████████████               0.2270\n",
      "\n",
      "E32 Train:                                0.0011\n",
      "    Val  : ███████████                    0.1637\n",
      "\n",
      "\n",
      "📊 SUBSTANCE ACCURACY TRENDS:\n",
      "--------------------------------------------------\n",
      "E 1: ████████████████████████████             0.7129\n",
      "E 2: ██████████████████████████████           0.7509\n",
      "E 3: ███████████████████████████████          0.7848\n",
      "E 4: ████████████████████████████████         0.8121\n",
      "E 5: █████████████████████████████████        0.8392\n",
      "E 6: ███████████████████████████████████      0.8871\n",
      "E 7: ███████████████████████████████████      0.8920\n",
      "E 8: ███████████████████████████████████      0.8965\n",
      "E 9: ████████████████████████████████████     0.9118\n",
      "E10: ███████████████████████████████████      0.8957\n",
      "E11: ████████████████████████████████████     0.9089\n",
      "E12: ███████████████████████████████████      0.8923\n",
      "E13: ███████████████████████████████████      0.8856\n",
      "E14: ██████████████████████████████████       0.8736\n",
      "E15: ███████████████████████████████████      0.8967\n",
      "E16: ██████████████████████████████████       0.8732\n",
      "E17: ███████████████████████████████████      0.8969\n",
      "E18: ████████████████████████████████████     0.9189\n",
      "E19: ███████████████████████████████████      0.8935\n",
      "E20: ████████████████████████████████████     0.9022\n",
      "E21: ███████████████████████████████████      0.8995\n",
      "E22: ███████████████████████████████████      0.8976\n",
      "E23: ███████████████████████████████████      0.8927\n",
      "E24: ████████████████████████████████████     0.9115\n",
      "E25: ████████████████████████████████████     0.9221\n",
      "E26: █████████████████████████████████████    0.9342\n",
      "E27: ████████████████████████████████████     0.9147\n",
      "E28: █████████████████████████████████████    0.9339\n",
      "E29: █████████████████████████████████████    0.9319\n",
      "E30: █████████████████████████████████████    0.9447\n",
      "E31: █████████████████████████████████████    0.9459\n",
      "E32: ██████████████████████████████████████   0.9606\n",
      "\n",
      "🎯 SYMPTOM F1 SCORE TRENDS:\n",
      "--------------------------------------------------\n",
      "E 1: ████████████████████████████████         0.7176\n",
      "E 2: ██████████████████████████████████████   0.8285\n",
      "E 3: ███████████████████████████████████████  0.8554\n",
      "E 4: ███████████████████████████████████████  0.8569\n",
      "E 5: ███████████████████████████████████████  0.8642\n",
      "E 6: ███████████████████████████████████████  0.8665\n",
      "E 7: ███████████████████████████████████████  0.8562\n",
      "E 8: ███████████████████████████████████████  0.8611\n",
      "E 9: ███████████████████████████████████████  0.8679\n",
      "E10: ███████████████████████████████████████  0.8530\n",
      "E11: ███████████████████████████████████████  0.8585\n",
      "E12: ██████████████████████████████████████   0.8422\n",
      "E13: ███████████████████████████████████████  0.8552\n",
      "E14: ██████████████████████████████████████   0.8419\n",
      "E15: ██████████████████████████████████████   0.8462\n",
      "E16: ███████████████████████████████████████  0.8494\n",
      "E17: ██████████████████████████████████████   0.8424\n",
      "E18: ███████████████████████████████████████  0.8510\n",
      "E19: ███████████████████████████████████████  0.8545\n",
      "E20: ███████████████████████████████████████  0.8532\n",
      "E21: ██████████████████████████████████████   0.8476\n",
      "E22: ██████████████████████████████████████   0.8420\n",
      "E23: ███████████████████████████████████████  0.8517\n",
      "E24: ███████████████████████████████████████  0.8560\n",
      "E25: ███████████████████████████████████████  0.8580\n",
      "E26: ███████████████████████████████████████  0.8617\n",
      "E27: ███████████████████████████████████████  0.8496\n",
      "E28: ███████████████████████████████████████  0.8662\n",
      "E29: ███████████████████████████████████████  0.8692\n",
      "E30: ████████████████████████████████████████ 0.8702\n",
      "E31: ███████████████████████████████████████  0.8665\n",
      "E32: ███████████████████████████████████████  0.8670\n",
      "\n",
      "📈 TRAINING SUMMARY STATISTICS:\n",
      "--------------------------------------------------\n",
      "Best Substance Accuracy: 0.9606 (Epoch 32)\n",
      "Best Symptom F1 Score: 0.8702 (Epoch 30)\n",
      "Final Train Loss: 0.0011\n",
      "Final Val Loss: 0.1637\n",
      "Loss Improvement: 99.3%\n",
      "\n",
      "📊 Interactive charts saved to training_charts.html\n",
      "Open this file in your web browser to view the charts!\n",
      "📁 Detailed results saved to training_detailed_results.csv and evaluation_detailed_results.csv\n",
      "💾 Model saved as 'best_model.pth'\n",
      "\n",
      "🎉 Evaluation completed! Check the generated files for detailed results.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report\n",
    "import torch\n",
    "\n",
    "# Replace the trainer evaluation code with this:\n",
    "\n",
    "# Evaluate the trained model\n",
    "print(\"\\nEvaluating trained model...\")\n",
    "eval_results = evaluate_model(trained_model, test_dataset, batch_size=16)\n",
    "\n",
    "# Print evaluation metrics\n",
    "substance_accuracy = accuracy_score(eval_results['substance_labels'], eval_results['substance_preds'])\n",
    "symptom_f1 = f1_score(eval_results['symptom_labels'], eval_results['symptom_preds'], average='micro', zero_division=0)\n",
    "symptom_precision = precision_score(eval_results['symptom_labels'], eval_results['symptom_preds'], average='micro', zero_division=0)\n",
    "symptom_recall = recall_score(eval_results['symptom_labels'], eval_results['symptom_preds'], average='micro', zero_division=0)\n",
    "\n",
    "print(f'Final Evaluation Results:')\n",
    "print(f'Substance Accuracy: {substance_accuracy:.4f}')\n",
    "print(f'Symptom F1 Score: {symptom_f1:.4f}')\n",
    "print(f'Symptom Precision: {symptom_precision:.4f}')\n",
    "print(f'Symptom Recall: {symptom_recall:.4f}')\n",
    "\n",
    "# Get predictions for classification reports\n",
    "substance_preds = eval_results['substance_preds']\n",
    "symptom_preds = eval_results['symptom_preds']\n",
    "\n",
    "# Make sure you have these variables defined (they should be from your data preprocessing)\n",
    "# If not, you'll need to extract them from your datasets\n",
    "print('\\nSubstance Classification Report:')\n",
    "print(classification_report(eval_results['substance_labels'], substance_preds,\n",
    "                           target_names=substance_classes, zero_division=0))\n",
    "\n",
    "print('\\nSymptom Classification Report:')\n",
    "print(classification_report(eval_results['symptom_labels'], symptom_preds,\n",
    "                           target_names=symptom_columns, zero_division=0))\n",
    "\n",
    "# Text-based training history visualization (alternative to matplotlib)\n",
    "def print_training_charts(history):\n",
    "    \"\"\"\n",
    "    Create text-based charts for training history\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"TRAINING HISTORY VISUALIZATION\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    epochs = len(history['train_loss'])\n",
    "    \n",
    "    # Loss chart\n",
    "    print(\"\\n📉 LOSS TRENDS:\")\n",
    "    print(\"-\" * 50)\n",
    "    max_train_loss = max(history['train_loss'])\n",
    "    max_val_loss = max(history['val_loss'])\n",
    "    max_loss = max(max_train_loss, max_val_loss)\n",
    "    \n",
    "    for i in range(epochs):\n",
    "        train_bar = int((history['train_loss'][i] / max_loss) * 30)\n",
    "        val_bar = int((history['val_loss'][i] / max_loss) * 30)\n",
    "        \n",
    "        print(f\"E{i+1:2d} Train: {'█' * train_bar:<30} {history['train_loss'][i]:.4f}\")\n",
    "        print(f\"    Val  : {'█' * val_bar:<30} {history['val_loss'][i]:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    # Accuracy chart\n",
    "    print(\"\\n📊 SUBSTANCE ACCURACY TRENDS:\")\n",
    "    print(\"-\" * 50)\n",
    "    for i in range(epochs):\n",
    "        acc_bar = int(history['substance_acc'][i] * 40)\n",
    "        print(f\"E{i+1:2d}: {'█' * acc_bar:<40} {history['substance_acc'][i]:.4f}\")\n",
    "    \n",
    "    # F1 Score chart\n",
    "    print(\"\\n🎯 SYMPTOM F1 SCORE TRENDS:\")\n",
    "    print(\"-\" * 50)\n",
    "    max_f1 = max(history['symptom_f1']) if max(history['symptom_f1']) > 0 else 1\n",
    "    for i in range(epochs):\n",
    "        f1_bar = int((history['symptom_f1'][i] / max_f1) * 40)\n",
    "        print(f\"E{i+1:2d}: {'█' * f1_bar:<40} {history['symptom_f1'][i]:.4f}\")\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n📈 TRAINING SUMMARY STATISTICS:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Best Substance Accuracy: {max(history['substance_acc']):.4f} (Epoch {history['substance_acc'].index(max(history['substance_acc'])) + 1})\")\n",
    "    print(f\"Best Symptom F1 Score: {max(history['symptom_f1']):.4f} (Epoch {history['symptom_f1'].index(max(history['symptom_f1'])) + 1})\")\n",
    "    print(f\"Final Train Loss: {history['train_loss'][-1]:.4f}\")\n",
    "    print(f\"Final Val Loss: {history['val_loss'][-1]:.4f}\")\n",
    "    print(f\"Loss Improvement: {((history['train_loss'][0] - history['train_loss'][-1]) / history['train_loss'][0] * 100):.1f}%\")\n",
    "\n",
    "# HTML-based visualization (alternative approach)\n",
    "def create_html_charts(history, filename='training_charts.html'):\n",
    "    \"\"\"\n",
    "    Create an HTML file with interactive charts using Chart.js\n",
    "    \"\"\"\n",
    "    html_content = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "        <title>Training History</title>\n",
    "        <script src=\"https://cdn.jsdelivr.net/npm/chart.js\"></script>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; margin: 20px; }}\n",
    "            .chart-container {{ width: 45%; display: inline-block; margin: 20px; }}\n",
    "            h1 {{ text-align: center; color: #333; }}\n",
    "            h2 {{ color: #666; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <h1>Training History Dashboard</h1>\n",
    "        \n",
    "        <div class=\"chart-container\">\n",
    "            <h2>Loss Over Time</h2>\n",
    "            <canvas id=\"lossChart\"></canvas>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"chart-container\">\n",
    "            <h2>Substance Accuracy</h2>\n",
    "            <canvas id=\"accuracyChart\"></canvas>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"chart-container\">\n",
    "            <h2>Symptom F1 Score</h2>\n",
    "            <canvas id=\"f1Chart\"></canvas>\n",
    "        </div>\n",
    "        \n",
    "        <div class=\"chart-container\">\n",
    "            <h2>Symptom Precision & Recall</h2>\n",
    "            <canvas id=\"precisionRecallChart\"></canvas>\n",
    "        </div>\n",
    "        \n",
    "        <script>\n",
    "            const epochs = {list(range(1, len(history['train_loss']) + 1))};\n",
    "            \n",
    "            // Loss Chart\n",
    "            new Chart(document.getElementById('lossChart'), {{\n",
    "                type: 'line',\n",
    "                data: {{\n",
    "                    labels: epochs,\n",
    "                    datasets: [{{\n",
    "                        label: 'Training Loss',\n",
    "                        data: {history['train_loss']},\n",
    "                        borderColor: 'rgb(255, 99, 132)',\n",
    "                        backgroundColor: 'rgba(255, 99, 132, 0.2)',\n",
    "                    }}, {{\n",
    "                        label: 'Validation Loss',\n",
    "                        data: {history['val_loss']},\n",
    "                        borderColor: 'rgb(54, 162, 235)',\n",
    "                        backgroundColor: 'rgba(54, 162, 235, 0.2)',\n",
    "                    }}]\n",
    "                }},\n",
    "                options: {{\n",
    "                    responsive: true,\n",
    "                    scales: {{\n",
    "                        y: {{ beginAtZero: true }}\n",
    "                    }}\n",
    "                }}\n",
    "            }});\n",
    "            \n",
    "            // Accuracy Chart\n",
    "            new Chart(document.getElementById('accuracyChart'), {{\n",
    "                type: 'line',\n",
    "                data: {{\n",
    "                    labels: epochs,\n",
    "                    datasets: [{{\n",
    "                        label: 'Substance Accuracy',\n",
    "                        data: {history['substance_acc']},\n",
    "                        borderColor: 'rgb(75, 192, 192)',\n",
    "                        backgroundColor: 'rgba(75, 192, 192, 0.2)',\n",
    "                    }}]\n",
    "                }},\n",
    "                options: {{\n",
    "                    responsive: true,\n",
    "                    scales: {{\n",
    "                        y: {{ beginAtZero: true, max: 1 }}\n",
    "                    }}\n",
    "                }}\n",
    "            }});\n",
    "            \n",
    "            // F1 Chart\n",
    "            new Chart(document.getElementById('f1Chart'), {{\n",
    "                type: 'line',\n",
    "                data: {{\n",
    "                    labels: epochs,\n",
    "                    datasets: [{{\n",
    "                        label: 'Symptom F1 Score',\n",
    "                        data: {history['symptom_f1']},\n",
    "                        borderColor: 'rgb(255, 206, 86)',\n",
    "                        backgroundColor: 'rgba(255, 206, 86, 0.2)',\n",
    "                    }}]\n",
    "                }},\n",
    "                options: {{\n",
    "                    responsive: true,\n",
    "                    scales: {{\n",
    "                        y: {{ beginAtZero: true, max: 1 }}\n",
    "                    }}\n",
    "                }}\n",
    "            }});\n",
    "            \n",
    "            // Precision & Recall Chart\n",
    "            new Chart(document.getElementById('precisionRecallChart'), {{\n",
    "                type: 'line',\n",
    "                data: {{\n",
    "                    labels: epochs,\n",
    "                    datasets: [{{\n",
    "                        label: 'Precision',\n",
    "                        data: {history['symptom_precision']},\n",
    "                        borderColor: 'rgb(153, 102, 255)',\n",
    "                        backgroundColor: 'rgba(153, 102, 255, 0.2)',\n",
    "                    }}, {{\n",
    "                        label: 'Recall',\n",
    "                        data: {history['symptom_recall']},\n",
    "                        borderColor: 'rgb(255, 159, 64)',\n",
    "                        backgroundColor: 'rgba(255, 159, 64, 0.2)',\n",
    "                    }}]\n",
    "                }},\n",
    "                options: {{\n",
    "                    responsive: true,\n",
    "                    scales: {{\n",
    "                        y: {{ beginAtZero: true, max: 1 }}\n",
    "                    }}\n",
    "                }}\n",
    "            }});\n",
    "        </script>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(filename, 'w') as f:\n",
    "        f.write(html_content)\n",
    "    \n",
    "    print(f\"\\n📊 Interactive charts saved to {filename}\")\n",
    "    print(\"Open this file in your web browser to view the charts!\")\n",
    "\n",
    "# Use text-based visualization\n",
    "print_training_charts(training_history)\n",
    "\n",
    "# Create HTML charts (optional - creates a file you can open in browser)\n",
    "create_html_charts(training_history)\n",
    "\n",
    "# Enhanced CSV export with more details\n",
    "def save_detailed_results(history, eval_results, filename='detailed_results.csv'):\n",
    "    \"\"\"\n",
    "    Save comprehensive results including training history and final evaluation\n",
    "    \"\"\"\n",
    "    import csv\n",
    "    \n",
    "    # Training history\n",
    "    with open(f'training_{filename}', 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['epoch', 'train_loss', 'val_loss', 'substance_acc', 'symptom_f1', 'symptom_precision', 'symptom_recall'])\n",
    "        for i in range(len(history['train_loss'])):\n",
    "            writer.writerow([\n",
    "                i+1, history['train_loss'][i], history['val_loss'][i],\n",
    "                history['substance_acc'][i], history['symptom_f1'][i],\n",
    "                history['symptom_precision'][i], history['symptom_recall'][i]\n",
    "            ])\n",
    "    \n",
    "    # Final evaluation results\n",
    "    with open(f'evaluation_{filename}', 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(['metric', 'value'])\n",
    "        writer.writerow(['substance_accuracy', substance_accuracy])\n",
    "        writer.writerow(['symptom_f1', symptom_f1])\n",
    "        writer.writerow(['symptom_precision', symptom_precision])\n",
    "        writer.writerow(['symptom_recall', symptom_recall])\n",
    "    \n",
    "    print(f\"📁 Detailed results saved to training_{filename} and evaluation_{filename}\")\n",
    "\n",
    "# Save comprehensive results\n",
    "save_detailed_results(training_history, eval_results)\n",
    "\n",
    "# Optional: Save the trained model\n",
    "torch.save(trained_model.state_dict(), 'best_model.pth')\n",
    "print(\"💾 Model saved as 'best_model.pth'\")\n",
    "\n",
    "print(\"\\n🎉 Evaluation completed! Check the generated files for detailed results.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6afcaa5",
   "metadata": {},
   "source": [
    "7. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ecf079df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model, vectorizer, and results saved!\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import csv\n",
    "\n",
    "torch.save(model.state_dict(), './tfidf_drug_use_model.pt')\n",
    "with open('./tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(vectorizer, f)\n",
    "\n",
    "model_info = {\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'model_config': {\n",
    "        'input_size': 10000,\n",
    "        'num_substance_classes': len(substance_classes),\n",
    "        'num_symptom_labels': len(symptom_columns)\n",
    "    },\n",
    "    'substance_classes': substance_classes,\n",
    "    'symptom_columns': symptom_columns,\n",
    "    'training_history': training_history\n",
    "}\n",
    "torch.save(model_info, './complete_model_info.pt')\n",
    "\n",
    "with open('training_history.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['epoch', 'train_loss', 'val_loss', 'substance_acc', 'symptom_f1', 'symptom_precision', 'symptom_recall'])\n",
    "    for i in range(len(training_history['train_loss'])):\n",
    "        writer.writerow([\n",
    "            i+1, training_history['train_loss'][i], training_history['val_loss'][i],\n",
    "            training_history['substance_acc'][i], training_history['symptom_f1'][i],\n",
    "            training_history['symptom_precision'][i], training_history['symptom_recall'][i]\n",
    "        ])\n",
    "\n",
    "with open('evaluation_results.csv', 'w', newline='') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['metric', 'value'])\n",
    "    writer.writerow(['substance_accuracy', substance_accuracy])\n",
    "    writer.writerow(['symptom_f1', symptom_f1])\n",
    "    writer.writerow(['symptom_precision', symptom_precision])\n",
    "    writer.writerow(['symptom_recall', symptom_recall])\n",
    "\n",
    "print('Model, vectorizer, and results saved!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
